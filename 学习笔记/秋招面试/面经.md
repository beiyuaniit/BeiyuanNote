- 这些知识都要以理解为主

# ==未解决==

- 进程、java进程、线程、java线程间的通信方式。。
- mysql存储单位
- gc算法分类
- lsm tree
- 可重复读能实现串行化吗
- 可重复读一定能防止幻读吗？
- mysql如何才能触发undolog
- string为什么是immutable的
- hashmap容量初始化
- hashmap线程不安全的原因
- apache POI中也有解决内存溢出的方法，有去了解过吗
- spring循环依赖只用两级缓存可以吗?
- equal和hashcode()
- mybatis plus的执行过程
- mysql给一些离散度较低的字段建立索引会出现什么问题（其实就是不能快速 定位。。返回一大堆相同的字段
-  什么时候触发行锁什么时候触发表锁
-  BBR协议
- BBR判断APP LIMITED，即应用层不发包如何探测带宽
- BBR1.0的缺点以及2.0的改进
- BBR如何估计带宽
- BBR如何知道探测带宽成功增大了
- QUIC优点
- 集群架构下如何实现下次连接不同的服务器也能0RTT
- 介绍CUBIC
- TCP OPT字段
-  TCP发包受限因素
- 如果使用UDP设计一个可靠传输协议，具体怎么做（参考tcp。。
- 多级TCP级联
-  redis的底层IO模型
- 对称加密和非对称加密
- GPT对行业的影响
- SQL数据库的设计模式
- 通过一个jvm线程id，如何查找到操作系统的线程id
- CPU 线程切换时，分配的时间片一般是多久
- TnreadLocal 源码
-  两个相互独立线程的交互方式
- 管道和消息队列的具体实现方式
- AQS 中的Thread队列和一般存储对象的队列有什么不一样的
- aqs源码
- 为什么方法区移动到了堆外内存
-  线程之间产生依赖，A线程执行完再执行B线程，怎么做
-  为什么方法区移动到了堆外内存
-  元空间放到堆外内存是否可以被回收？
- 元空间放到堆外内存是否可以被回收？类什么情况下可能会回收
- 类加载器什么情况下会被回收？
- LSM tree、B+ tree 区别、场景、设计倾向
- AP 和 TP 使用场景，区别
- 协程

# ==小总结==

- 适当扩展深度和广度，但是要避免给自己挖坑

# ==不是很清楚==（未整理好）

## spring循环依赖

- 在Spring中，解决循环依赖（Circular Dependency）的方式取决于Bean的作用域。尽管Spring提供了解决循环依赖的机制，但建议尽可能避免循环依赖的设计，以保持代码清晰和可维护性
  - 单例作用域（Singleton Scope）：
    - 默认情况下，Spring容器允许解决单例Bean之间的循环依赖。
    - Spring使用"提前暴露引用"的机制来解决循环依赖问题。
    - 在创建Bean的过程中，当Spring发现循环依赖时，会将正在创建的Bean的引用提前暴露给Spring容器，即使它还没有完全初始化。
    - 这样，即使存在循环依赖，每个Bean可以访问对方的引用，但只有在完全初始化后才能使用。
  - 原型作用域（Prototype Scope）：
    - 在原型作用域下，Spring无法解决循环依赖。
    - 当Bean的作用域为原型时，Spring不会缓存Bean的实例，而是每次请求时创建新的实例。
    - 在循环依赖的情况下，每次请求都会创建新的实例，导致无法解决依赖关系，最终会抛出异常。
- 三级缓存处理机制
  - 前置处理阶段（Early-Stage）：
    - 在Bean的创建过程中，当Spring发现循环依赖时，它会创建一个空的早期引用（Early Reference）来代表正在创建的Bean。
    - 这个早期引用提供了对正在创建的Bean的句柄，但里面的实例仍然是空的。
  - 初始阶段（Early-Stage）：
    - 在初始阶段，Spring将早期引用添加到第一级缓存中。
    - 第一级缓存使用ConcurrentHashMap来存储早期引用，以确保线程安全。
  - 循环依赖解决阶段（Circular Dependency Resolution）：
    - 在循环依赖解决阶段，如果一个Bean的依赖关系已解析，Spring会尝试从第一级缓存中获取已经解析的实例。
    - 如果找不到已解析的实例，Spring会尝试通过创建一个ObjectFactory来解决循环依赖。
    - ObjectFactory是一个提供Bean工厂的接口，它可以延迟创建Bean的实例。
    - 当Bean的依赖关系解析完成后，Bean会被实例化并存储在第二级缓存中。
  - 后置处理阶段（Late-Stage）：
    - 在后置处理阶段，Spring将已解析的Bean实例添加到第三级缓存中。
    - 第三级缓存同样使用ConcurrentHashMap来存储已解析的Bean实例，以确保线程安全。
    - 当一个Bean的所有依赖关系都解决完毕后，它会从第三级缓存中取出，并进行属性注入和初始化。

## TCP作为代理转发时如何保证可靠

- 通过使用TCP可靠性机制、数据完整性校验、数据确认重传、合理的超时设定、拥塞控制、错误处理以及日志和监控，可以提高TCP代理转发的可靠性。
  - 建立可靠的连接：TCP本身提供了可靠的连接机制。代理服务器在建立与客户端和目标服务器的连接时，会使用TCP的握手过程（三次握手）来建立稳定的连接。
  - 数据完整性校验：在代理服务器接收到客户端发送的TCP数据包后，可以对数据进行校验，如使用校验和或散列算法来验证数据的完整性，检测是否有数据丢失或篡改。
  - 数据确认和重传：代理服务器作为中间节点接收客户端发送的数据后，会将数据转发给目标服务器。在数据转发过程中，代理服务器需要等待目标服务器的确认（ACK）消息，以确认目标服务器已成功接收数据。如果代理服务器在一定的超时时间内没有收到确认消息，代理服务器会自动进行数据重传，确保数据的可靠传输。
  - 合理的超时设定：代理服务器需要根据网络情况和延迟设置合理的超时时间。超时时间太短可能会导致过多的重传，影响性能；超时时间太长可能会导致延迟增加。通过合理设定超时时间，可以平衡可靠性和性能之间的关系。
  - 拥塞控制：代理服务器需要实施拥塞控制，避免网络拥塞。当代理服务器发现网络拥塞时，可以使用拥塞避免算法，如减少发送速率或发送窗口的大小，以保证网络的稳定性和可靠性。
  - 错误处理：代理服务器需要处理可能出现的错误情况，例如目标服务器不可达、连接中断等。在这些情况下，代理服务器需要相应地进行错误处理，例如关闭连接或发送适当的错误消息给客户端。
  - 日志和监控：代理服务器可以记录和监控传输过程中的事件和状态，以便及时发现和解决可能的问题。通过日志和监控，可以对代理转发的可靠性进行分析和改进。

## 代理服务器挂了怎么办

- 当代理服务器挂掉时，可以采取以下几个措施
  - 备用代理服务器：建立一个备用代理服务器，当主要的代理服务器出现故障时，可以切换到备用代理服务器进行服务。备用代理服务器可以配置为热备份，即时响应故障的发生，也可以配置为冷备份，需要手动触发切换。
  - 自动故障检测和切换：使用监控系统或脚本来检测代理服务器的健康状态。一旦监测到代理服务器故障或不可用的情况，可以自动触发切换到备用代理服务器。这可以通过负载均衡设备、DNS解析器设置、故障检测脚本等来实现。
  - 多个代理服务器：在代理服务器集群中部署多个实例，以增加可靠性。请求可以通过负载均衡设备或DNS解析器分发到可用的代理服务器实例上，当其中某个代理服务器出现故障时，剩余的代理服务器可以继续提供服务。
  - 监控和告警：设置监控和告警系统来实时监测代理服务器的状态。当代理服务器挂掉时，可以及时收到警报并采取相应的应对措施，例如重新启动代理服务器、调整负载均衡配置等。
  - 客户端重试机制：客户端可以采取一些策略来应对代理服务器挂掉的情况。例如，通过设置超时时间和重试次数，当连接代理服务器失败时，尝试连接备用代理服务器或自身进行故障转移。
  - 容灾和备份：定期备份代理服务器的配置和数据，以便在故障发生时快速恢复服务。也可以设置冗余的硬件设备，如磁盘阵列、冗余电源等，以减少硬件故障带来的影响。

# ==java==

## String的equal和==

- `equals()`方法：它是String类的方法，用于比较两个字符串的内容是否相等。`equals()`方法比较的是字符串的值，而不是对象的引用。当两个字符串的字符序列相同（即内容相同）时，`equals()`方法将返回`true`；否则，返回`false`。
- = = 运算符：它用于比较两个对象的引用是否相等。对于String对象，= = 比较的是字符串对象在内存中的地址。如果两个String对象指向同一块内存地址，则==运算符返回true；否则，返回false

## ArrayList.sort()

- Java 8及以上版本的ArrayList.sort()方法底层使用的是一个基于双轴快速排序（Dual-Pivot Quick Sort）算法的优化版本

- 双轴快速排序是对传统快速排序算法的改进。它通过选择两个轴（pivot）来将待排序集合划分成三个部分，分别是小于第一个轴的元素、介于两个轴之间的元素、大于第二个轴的元素。然后对这三个部分分别进行递归排序

- 过程

  - 选择两个轴元素，并根据它们的大小进行从小到大的排序。
  - 遍历集合，将元素根据和轴的比较结果放置在相应的位置上，以形成小于第一个轴的区域、介于两个轴之间的区域和大于第二个轴的区域。
  - 对这三部分进行递归调用
  - 如果两个轴元素相等，说明它们都在待排序集合中有多个重复的值，此时将这部分区域视为与轴元素匹配，不再进行排序。
  - 当待排序的区域大小小于一个阈值时，切换到插入排序（Insertion Sort）以提高性能。

- 和传统快排相比的优点

  - 可聚集重复元素：双轴快速排序通过选择两个轴元素，并将介于这两个轴之间的元素放在一个中间区域，可以有效地聚集重复元素。这样可以减少比较和交换的次数，从而提高排序的效率

  - 双轴快速排序可以减少比较和交换的次数：传统的单轴快速排序在每个划分步骤中需要将元素与轴元素进行比较，并将元素移动到正确的位置。而双轴快速排序通过选择两个轴元素，可以将数组中的元素更快地划分为三个区域，其中中间部分无需进行额外的比较和交换操作
  - 减少递归深度：在传统的快速排序中，如果划分不平衡，可能导致递归的深度增加，从而增加了额外的递归开销。而双轴快速排序通过选择两个轴元素，可以在一定程度上减少不平衡划分的情况，从而降低了额外的递归深度，提高了性能。

- 注意

  - ArrayList.sort()方法是一个原地排序算法，即会直接修改原始ArrayList的顺序，而不会创建新的集合。此外，对于小于插入排序阈值的待排序区域，ArrayList.sort()会切换到插入排序算法以减少递归的开销和提高性能。

## 破坏双亲委派机制

- 需要破坏双亲委派机制时，一般会涉及以下详细场景和技术
  - 自己去加载类？
  - 动态加载和版本管理：在某些场景下，应用程序需要根据运行时需求动态加载特定版本的类。为了实现这一点，可以创建自定义类加载器，并在该加载器中实现自定义的加载策略，绕过双亲委派机制。这样，可以根据特定的需求，从底层开始加载目标类，以确保加载所需的特定版本。
  - 模块化系统和类加载控制：在一些模块化系统中，可能需要根据模块之间的依赖关系自定义类的加载顺序。为了实现这一点，可以创建自定义的类加载器，并在加载过程中完全控制类的加载顺序，而不受双亲委派机制的限制。这样，可以根据模块之间的依赖关系，确保类加载的正确性和一致性。
  - 热部署和类替换：在部分应用场景中，可能需要实现热部署功能，即在应用程序运行时替换已加载的类。为了实现这一点，可以创建自定义的类加载器，并在加载新类时绕过双亲委派机制。通过自定义类加载器加载新的类，并在加载完成后使用新的类替换旧的类定义，从而实现热部署的效果。
  - 安全沙箱和类加载控制：在一些安全敏感的环境中，可能需要更严格地控制对类的加载和访问。为了实现这一点，可以创建自定义的安全类加载器，并实现自定义的加载策略，以限制对特定类的加载和访问。在自定义的类加载器中，可以通过特定的安全规则和策略，绕过双亲委派机制，并且只加载所需的受信任的类。
- 注意事项
  - 破坏双亲委派机制可能引入不确定性和潜在的问题，需要仔细评估和测试。
  - 自定义的类加载器需要继承ClassLoader类，并实现相关的加载逻辑。（自己加载某些类？）
  - 在破坏双亲委派机制时，需要注意类的命名规范和唯一性，以避免类冲突。
  - 需要确保自定义的类加载器适当地缓存已加载的类和管理类加载的生命周期。
  - 在进行类替换和热部署时，需要考虑到类定义的一致性和正确性
- 例子
  - Java应用服务器：一些Java应用服务器，如Tomcat和Jetty，使用自定义的类加载器来加载Web应用程序的类。这样做是为了实现热部署功能，可以在运行时动态替换Web应用程序的类，而不需要重新启动服务器。
  - OSGi框架：OSGi（开放服务网关倡议）是一种面向Java的动态模块化系统，可以在运行时加载、卸载和替换模块。在OSGi框架中，自定义的类加载器用于实现模块间的隔离和加载控制，绕过双亲委派机制。
  - Android插件化开发：在Android开发中，一些插件化框架使用自定义的类加载器来加载插件中的类。这种方式可以实现动态加载插件，将插件的类加载到特定的插件ClassLoader中，从而实现模块化和独立性。
  - Java桌面应用程序：有些Java桌面应用程序可能使用自定义的类加载器来加载外部插件或扩展。这样做的目的是实现动态扩展功能，允许用户在运行时添加新的功能模块，而不需要重新启动应用程序。

## 字节流和字符流

- 字符流和字节流都有对应的输入流和输出流类。例如，字符流的Reader和Writer类提供了读取和写入字符数据的方法，字节流的InputStream和OutputStream类提供了读取和写入字节数据的方法
  - 数据单位：
    - 字节流（Byte Stream）以字节（8位二进制数据）为单位进行读写。字节流适用于处理任意类型的数据，包括图像、音频和视频等二进制数据。
    - 字符流（Character Stream）以字符（16位Unicode字符）为单位进行读写。字符流主要用于处理文本数据，例如文本文件的读写。
  - 编码处理：
    - 字节流：字节流以字节的形式直接传输数据，不涉及字符编码和解码方式。它们主要基于InputStream和OutputStream类层次结构来读写原始的字节数据。
    - 字符流：字符流提供了对字符的编码（转换字符为字节序列）和解码（将字节序列转换为字符）的功能。字符流内部使用编码器（Encoder）和解码器（Decoder）来实现字符与字节之间的转换。它们主要基于Reader和Writer类层次结构来读写字符数据。
  - 应用场景：
    - 字节流：字节流适用于处理任意类型的数据，包括二进制文件、图像、音频和视频等。它们可以直接读写原始的字节数据，不需要进行字符编码和解码。
    - 字符流：字符流主要适用于处理文本数据，如文本文件的读写。字符流能够直接处理字符数据，并提供了对字符的编码和解码功能，能够正确地处理Unicode字符和特殊字符。
  - 性能：
    - 字节流：由于字节流直接操作底层的输入输出流，所以在处理大量二进制数据时通常比字符流更高效。
    - 字符流：尽管字符流处理文本数据时需要进行字符编码和解码，但由于字符流对字符进行了抽象和封装，使得字符流在处理纯文本时更加方便和易用。

## `try-catch-finally` 原理

- 指令
  - `try` 块的字节码指令：
    - `try` 块的起始位置会有一个 `try` 指令，标记着 `try` 块的起始位置。
    - `try` 块中的代码会被逐条编译成字节码指令，并按顺序执行。
  - `catch` 块的字节码指令：
    - 当在 `try` 块中发生异常时，JVM会根据异常类型查找匹配的 `catch` 块。
    - JVM会根据异常类型查找异常处理器表（Exception Handler Table）中对应的异常处理器。
    - 如果找到匹配的异常处理器，JVM会跳转到对应处理器的起始位置，并执行处理器中的逻辑。处理器的代码可用于处理异常、恢复状态等。
    - `catch` 块可能对应多个异常处理器，每个处理器负责处理特定类型的异常。
    - 处理完后回到finally？
  - `finally` 块的字节码指令：
    - `finally` 块的起始位置会有一个 `start_finally` 指令，标记着 `finally` 块的起始位置。
    - `finally` 块中的代码会被逐条编译成字节码指令，并按顺序执行。
    - `finally` 块结束后，会有一个 `end_finally` 指令。
  
- 返回值覆盖
  
  - try、catch中执行了return，会继续执行finally中的代码
  - 若finally中有return语句
    - 在执行 try或catch语句的`return` 语句之前，JVM会将 `return` 返回值保存在一个临时变量（并不马上返回
    - 然后执行finally块
    - `finally` 块中return的 返回值会覆盖掉 `try` 块和`catch`块中的返回值
  
- 返回值不修改

  - 因为返回值已经保存到一个临时变量了，所以在finnally中再对原有变量修改，则不会影响

    ```java
    // 走try返回21，走catch返回22。都不会被finally的33影响
    public static int doSomething() {
            int res = 21;
            try {
                throw new ArrayIndexOutOfBoundsException();
                // return res;
            } catch (Exception e) {
                return res+1;
            } finally {
                res = 33;
            }
    }
    ```

    

- 异常覆盖
  
  - `finally` 块中发生或抛出了新异常，不会执行finally异常后的代码
  - 若没有在finally中进行catch处理，那么会覆盖之前try和catch抛出中的异常，该异常会成为最终异常并抛出
  - 若在finally中catch处理则不影响
  
- try和finally其中之一有return ，整个异常处理语句块后就不能有代码，这部分代码访问不到，编译时直接报错。666

- 调用System.exit(int status)
  
  - try-catch-finally中
    - 不会执行后续代码
    - 直接退出并打印Process finished with exit code status
  - 注
    - 若只是在catch中编译了，但是运行时没有异常被catch到，则不影响。因为压根就没有执行System.exit(int status);
  
- 要特别注意处理 `finally` 块中可能发生的异常情况，以避免意外的行为和资源泄漏

## finalize（）

- `finalize()` 方法是Java中的一个特殊方法，它属于`java.lang.Object`类，因此所有的类都可以覆盖它。`finalize()` 方法在Java 9中已被标记为过时（Deprecated），因此不推荐使用它

- 定义

  ```java
  protected void finalize() throws Throwable {
      // 清理操作
  }
  ```

- 要点
  - `finalize()` 方法的可见性是`protected`，这意味着它只能在对象内部或子类中访问和覆盖。
  - `finalize()` 方法没有参数，返回类型为`void`。
  - `finalize()` 方法声明了`Throwable`异常，因为它可以抛出任何异常或错误。
  - `finalize()` 方法的执行时间是不确定的，无法预测具体的调用时间点以及调用次数。
  - 覆盖 `finalize()` 方法时，应注意避免长时间的执行或抛出异常，以免影响垃圾回收器的性能和行为。
  - 不能依赖`finalize()` 方法来释放关键资源，因为不可预测的调用时机有可能导致资源无法及时释放。
- 在对象被垃圾回收之前，垃圾回收器会在某个时间点调用 `finalize()` 方法，用于执行一些清理操作。它是垃圾回收器的最后一次机会来处理对象，通常用于释放占用的资源，如文件、网络连接、数据库连接等
- 不建议使用
  - 由于无法预测垃圾回收器何时调用 `finalize()` 方法，因此不能依赖于该方法来释放关键资源。此外，`finalize()` 方法的调用也会导致性能问题和不确定的行为。
  - 取而代之的是，推荐使用显式的资源管理和关键资源的 try-with-resources 语句块。这样可以确保在不再需要资源时，及时关闭或释放它们

- finalize()发生了异常
  - 不会导致程序crush
  - 如果没有其他地方捕获这个异常，异常将被传递给垃圾回收器，并被忽略。垃圾回收器会记录该异常，但不会抛出或处理它。因此，如果 `finalize()` 方法中抛出的异常没有被捕获，那么它将被默默地丢弃，没有任何异常处理逻辑会执行。
  - 如果在 `finalize()` 方法内部捕获了异常，可以选择在捕获块中处理异常，或者重新抛出异常。如果重新抛出异常并且没有被其他的异常处理机制捕获，那么同样的情况会再次发生。

## 重写equals

- why
  - 默认情况下，equals()方法比较的是两个对象的引用地址，即判断它们是否引用同一个内存地址的对象
  - 然而，有时候我们需要根据对象的内容（比如某些属性）来决定它们是否相等
  - 通过重写equals()方法，我们可以自定义对象相等的逻辑，比如基于对象的某个属性或多个属性进行比较，而不仅仅是比较引用地址。这在需要使用对象相等性判断的场景下会更加准确和灵活
  - 因为hashcode会重复，为了进一步判断hashcode
- 重写equals()方法的主要目的是定义对象相等的逻辑。重写后的equals()方法应该满足以下几个条件：
  1. 自反性：对于任何非null的引用值x，x.equals(x)应该返回true。
  2. 对称性：对于任何非null的引用值x和y，如果x.equals(y)返回true，则y.equals(x)也应该返回true。
  3. 传递性：对于任何非null的引用值x、y和z，如果x.equals(y)返回true，并且y.equals(z)返回true，则x.equals(z)也应该返回true。
  4. 一致性：对于任何非null的引用值x和y，如果参与比较的对象没有发生变化，则多次调用x.equals(y)应该返回相同的结果。
  5. 对于任何非null的引用值x，x.equals(null)应该返回false。

## 重写hashcode

- why

  - 默认的hashCode()方法实现是将对象的内存地址转换为整数
  - (比较的内容更加有意义)
    - 为了提高hashCode()方法的准确性和唯一性，我们通常需要根据对象的实际内容生成哈希值。这要求我们重写hashCode()方法，确保当两个对象相等（根据equals()方法判断）时，它们的hashCode()方法返回的哈希值也是相等的。这样可以保证在使用哈希码的数据结构（如哈希表）时，能够正确地处理相等性和查找操作。

- 怎么重写

  ```java
      @Override
      public boolean equals(Object o) {
          if (this == o) return true;
          if (o == null || getClass() != o.getClass()) return false;
          Order order = (Order) o;
          return Objects.equals(id, order.id) && Objects.equals(userId, order.userId) && Objects.equals(goodsId, order.goodsId) && Objects.equals(deliveryAddrId, order.deliveryAddrId) && Objects.equals(goodsName, order.goodsName) && Objects.equals(goodsCount, order.goodsCount) && Objects.equals(goodsPrice, order.goodsPrice) && Objects.equals(orderChannel, order.orderChannel) && Objects.equals(status, order.status) && Objects.equals(createDate, order.createDate) && Objects.equals(payDate, order.payDate);
      }
  
      @Override
      public int hashCode() {
          return Objects.hash(id, userId, goodsId, deliveryAddrId, goodsName, goodsCount, goodsPrice, orderChannel, status, createDate, payDate);
      }
  ```


## 没有静态外部类

- final修改的类是不可继承
- 不能用static修饰一个外部类

## 静态内部类

- 静态内部类是定义在另一个类中的类，并且使用static关键字进行修饰。静态内部类与外部类是相互独立的，它可以有自己的字段、方法和构造函数等成员。

  不用先创建外部类的对象就可以创建静态内部类

  静态内部类可以访问外部类的静态成员，这包括：

  1. 静态字段：静态内部类可以直接访问外部类中的静态字段，无需创建外部类的实例。它可以通过外部类名加上字段名的方式来访问。
  2. 静态方法：静态内部类可以直接调用外部类中的静态方法，无需创建外部类的实例。同样，它可以通过外部类名加上方法名的方式来调用。

  例如，假设有一个外部类OuterClass，它包含一个静态字段staticField和一个静态方法staticMethod。静态内部类InnerClass可以直接访问OuterClass的staticField和调用OuterClass的staticMethod。

  而静态内部类不能直接访问外部类的非静态成员，这包括：

  1. 实例字段：静态内部类没有外部类的实例引用，因此无法直接访问外部类的实例字段。
  2. 实例方法：同样，静态内部类也不能直接调用外部类的实例方法。

  如果静态内部类需要访问外部类的非静态成员，可以通过创建外部类的实例来实现。可以在静态内部类中创建外部类的实例，并通过该实例来访问外部类的非静态成员。

- 本身是比较独立

  - 能够有私有、静态、非静态的属性方法等
  - 创建实例时不依赖外部类的实例。OutClass.InnerClass inner=new OutClass.InnerClass ();

## 方法内的类

- 不能声明为静态

- 能够访问外部的私有、静态变量

  ```java
  public final class  Solution {
  
      private  static int b=1;
  
      public void get(){
          class  Inner{
              public void set(){
                  System.out.println(b);
              }
          }
          Inner inner=new Inner();
          inner.set();
      }
  
  }
  ```
  
- 外部类无法访问到方法内的类

## 外部类的权限

- 非静态内部类
  - 能够通过创建实例的方式访问其所有方法、属性（包括私有的）
  - 非静态内部类没有静态属性
- 静态内部类
  - 能够访问所有

## 非静态内部类的权限

- 非静态内部类（也称为成员内部类）可以访问外部类的所有成员，包括外部类的私有成员。这是因为非静态内部类与外部类之间存在着一个特殊的关系，即非静态内部类持有一个对外部类的引用
  - 外部类的所有成员变量（包括私有变量，静态变量）。
  - 外部类的所有方法（包括私有方法）。
  - 外部类的所有构造函数（包括私有构造函数）。
  - 外部类的所有内部类（无论是静态还是非静态内部类）

- 要通过实例来创建，不能包含静态属性、方法

  - 非静态内部类的创建需要先创建外部类的实例，然后使用外部类的实例来创建内部类的实例

    ```java
    public class SolTest {
        Solution sol=new Solution();
        @Test
        public void solTest() {
            Solution.InnerClass innerClass=sol.new InnerClass();
        }
    }
    
    ```

    

  - 非静态内部类依赖于外部类的实例，它的创建必须依赖于外部类的实例化过程。而静态属性是属于类的，不依赖于任何实例，可以在类加载的时候直接初始化

  - jdk14反正是不支持的

    ```java
    class Solution {
    
        public static void main(String[] args) {
            Scanner in = new Scanner(System.in);
    
        }
        class InnerClass{
            static final   int a;  // 编译时报错
            static  int b; // 编译时报错
        }
    }
    ```

    

- 非静态内部类中，如果外部类与内部类具有同名的成员变量或者方法，可以通过 `外部类.this` 来访问外部类的成员，以避免命名冲突

  ```java
  public class OuterClass {
      private String message = "Hello, World!";
      
      public void displayMessage() {
          InnerClass inner = new InnerClass();
          inner.displayMessage();
      }
      
      public class InnerClass {
          private String message = "Hello, Inner World!";
          
          public void displayMessage() {
              System.out.println("Outer Message: " + OuterClass.this.message);
              System.out.println("Inner Message: " + this.message);
          }
      }
  }
  ```

## 线程安全的集合

- 当在多线程环境中需要处理共享数据时，使用线程安全的集合类可以确保数据的一致性和线程安全性。下面是对几个常用的线程安全的集合类进行详细介绍：

  1. ConcurrentHashMap：
     - ConcurrentHashMap 是一个线程安全的哈希表实现，继承自 AbstractMap 类。
     - 它使用分段锁（Segment）来实现并发控制，不同的线程可以同时进行读取操作，而写操作会锁住对应的段，从而实现较高的并发性能。
     - ConcurrentHashMap 支持高并发读取，适合在读多写少的场景下使用。
     - ConcurrentHashMap 不允许键或值为空，否则会抛出 NullPointerException。
  2. CopyOnWriteArrayList：
     - CopyOnWriteArrayList 是一个线程安全的动态数组实现，继承自 AbstractList 类。
     - 它采用了"写时复制"（copy-on-write）的策略，在修改操作时会创建一个新的数组，从而避免了写操作的锁竞争。
     - 多个线程可以同时进行读取操作，而写操作会复制一份新的数组进行修改，适合在读多写少的场景下使用。
     - 由于每次写操作都需要复制整个数组，所以对于频繁的写操作，性能会比较低。
  3. ConcurrentLinkedQueue：
     - ConcurrentLinkedQueue 是一个线程安全的无界队列实现，实现了 Queue 接口。
     - 它使用非阻塞算法来实现高效的并发访问，多个线程可以同时进行入队和出队操作，无需加锁。
     - ConcurrentLinkedQueue 适用于生产者和消费者线程并发访问的场景，例如多线程异步任务的处理。
  4. BlockingQueue：
     - BlockingQueue 是一个阻塞队列接口，继承自 Queue 接口，提供了在队列为空或满时阻塞线程的功能。
     - 它可以用于实现生产者-消费者模式，其中生产者线程将数据放入队列，消费者线程从队列中取出数据进行处理。
     - 常用的实现类有 ArrayBlockingQueue、LinkedBlockingQueue、PriorityBlockingQueue 等。
     - ArrayBlockingQueue 是一个基于数组的有界队列，LinkedBlockingQueue 是一个基于链表的可选有界队列，而 PriorityBlockingQueue 是一个基于优先级的无界队列。

  除了上述介绍的几个线程安全集合类，还有其他一些类似的集合类，如 ConcurrentSkipListSet、ConcurrentSkipListMap 等。这些集合类提供了不同的特性和适用场景，根据具体需求选择合适的集合类可以提高多线程环境下的性能和安全性。

## CopyOnWrite

- 在 Java 中，Copy-on-write (COW) 通常用于优化对于共享集合的并发访问。常见的数据结构，如 ArrayList 和 HashMap，都有对应的线程安全版本，如 CopyOnWriteArrayList 和 CopyOnWriteHashMap。

  Java 中的 COW 原理如下：

  1. 数据复制：当创建一个 COW 集合时，它会初始化一个不可变的底层数组或哈希表来存储数据。这个底层数组或哈希表在创建后不会被修改。
  2. 写操作：当一个线程试图进行写操作（添加、删除或修改元素）时，它会首先创建一个当前数据的副本。这份副本是一个可变的数据结构，可以被线程修改，而不会影响底层的不可变数据。
  3. 修改副本：线程将修改应用到副本上，而不是底层的原始数据。这确保了其他线程在遍历原始数据时不会受到并发修改的影响。
  4. 更新引用：当修改完成后，COW 集合会将引用指向新的副本，这样其他线程访问集合时会得到最新的更新版本。

  COW 的主要优势在于读多写少的场景下，可以提供较好的并发性能。由于读取操作不会阻塞或复制数据，多个线程可以同时访问共享的底层数据结构，避免了竞争条件和锁带来的开销。只有在写操作时，COW 集合会进行复制和更新，这可能会带来一些开销，但不会影响其他并发读的线程。

  需要注意的是，由于 COW 集合的底层数据是不可变的，所以它适用于那些数据集合相对较小、写操作较少的场景。对于频繁写入或大型数据集合，COW 的复制操作可能会引起较高的开销。

  总结来说，Java 中的 COW 利用不可变的底层数据结构和副本的方式，实现了并发访问的安全性和性能优化。它适用于读多写少的场景，可以提供较好的并发性能和线程安全性。然而，在具体使用时，需要根据应用场景进行评估和选择。

## 守护线程



## 反射



## 源码

- ThreadLocal
- Integer  常量池在初始化时就放在了方法区

# ==jvm==

## java的数组放在哪个区

- 动态初始化

  ```java
  int []arr=new int [3];
  //
  int n=a*b;
  Object []objs=new Object[n];
  ```

  - 使用 `new` 关键字创建的数组对象实例会被放在堆区内存中。
    - 这是因为数组对象的大小在编译时不能确定，需要在运行时动态分配内存，而堆区内存具有动态分配和释放的特性，非常适合存储大小不固定的对象。除此之外，堆区内存还具有可共享、可回收等优点
  - 为什么数组对象的大小在编译时不能确定?
    - 因为数组是一种引用类型,程序必须在运行时先判断数组长度，然后才能根据数组长度动态分配内存空间
    - 数组的长度可以是一个变量或者表达式，且在运行过程中可能会根据条件动态调整数组长度

- 静态初始化

  ```java
  int []arr={3,1,2};
  ```

  - 也会被分配在堆区内存中
    - 不同的是，静态初始化方式创建的数组对象在编译时已经确定了大小和初始值，因此它们的内存地址是在编译期间就已经确定的，不需要在运行期间动态分配内存空间
    - 所以它们的数据存储在类文件常量池中，并已经在编译时为它们分配了内存空间，即常量池中的静态区域。所以，使用静态初始化方式创建的数组和类的静态成员变量放在同一个静态区域中
    - 在 Java 程序运行时，JVM 会把这些静态变量和静态数组对象从常量池中加载到内存中，供程序调用。

## 常量池

- 类常量存放在元空间，对象常量（如字符串）存在在堆
- 常量池是位于类文件中的一部分，存储着编译器生成的字面量（如字符串、数值、类名、方法名等）和符号引用（如类、方法、字段等引用）。在 Java 应用程序启动时，JVM 就会读取类文件中的常量池信息，并将其加载到内存中，以供程序运行时使用。
- 可以说常量池在一开始是存在于类文件中的，但它并没有被直接加载到内存中。只有当 Java 应用程序启动时，JVM 才会读取类文件中的常量池信息，并将其加载到内存中
- 存放字面量和符号引用
  - 字符串常量：即以双引号引起来的字符串值，例如：“Hello World”。
  - 数值常量：包括整数、浮点数和字符，例如：123、3.14、'A’等。
  - 类名、方法名和字段名：存储着类的全限定名、方法名和字段名。
  - 符号引用：包括类和接口的名称、方法和字段的名称和描述符，以及类中方法的调用点（即符号引用名称和描述符的结合）。符号引用用于在类加载期间解析为实际的直接引用。
  - 方法类型和方法句柄：这些都是在 Java 7 中引入的内容，用于支持 invokedynamic 指令的动态调用机制

## 符号引用和直接引用

- 符号引用和直接引用是一种逻辑上的分离，符号引用是表述引用的符号描述，而直接引用是符号引用最终所指的内存地址。这个分离的设计可以增加程序的灵活性，使得程序可以更加方便地进行编译和优化，同时也增强了安全性，使得Java程序更难受到攻击。
- 符号引用和直接引用的关系是由JVM在类加载和连接的过程中完成的。当一个类被加载到JVM中时，其中包含的符号引用会被解析成对应的直接引用，然后被存储在方法区中的运行时常量池中，成为一个类在内存中的运行时数据结构的一部分。这个过程叫做符号引用转化为直接引用。
- 符号引用
  - 用于描述引用另一个类或方法的代码。它包括了类或方法的全限定名、方法参数和返回值等信息。符号引用是在编译时生成的，相当于引用的标识符。这个引用使用了一些未决定下来的量，比如方法的参数类型和返回值类型，因此符号引用是一个不完整的引用。
- 直接引用
  - 指向具体的内存地址。它是在程序运行时根据符号引用解析生成的，比如一个指向方法的调用点直接引用就是该方法在内存中的实际地址。直接引用是解析出符号引用所代表的方法或类之后，可以直接使用的引用，它是一个完整的引用。
- 一个是符号，一个是值

## 对象的生命周期

- 总的来说，以下4个阶段是 Java 对象生命周期的基本流程
  - 创建阶段
    - 当创建一个对象时，JVM 会分配一块内存空间，并为其分配一个唯一的对象引用。通常使用 `new` 关键字来创建一个新的对象，以及调用类的构造方法进行初始化。在这个阶段，对象被创建，但其属性可能没有被设置，因此对象可能不可用。
  - 可达性阶段
    - 在 Java 中，通过引用来访问对象。只要对象能够被访问，就被认为是“可达”状态，包括将对象赋值给某个变量、将对象作为参数传递给某个方法、以及将对象作为成员变量存储在其他对象中等。只要对象处于可达状态，其并不会被销毁
  - 不可达性阶段
    - 当对象不再被任何引用所引用时，其进入不可达状态。当垃圾收集器运行时，将会扫描所有可达对象，并将不可达对象标记为垃圾对象，等待垃圾收集器进行回收。
  - 垃圾回收阶段
    - 在不可达性阶段，JVM会启动垃圾回收器回收对象。垃圾回收器会标记那些可被回收的对象，并回收占用的内存空间。垃圾回收会根据不同的垃圾收集器算法和参数来进行，包括标记清除算法、标记整理算法、复制算法等。在垃圾回收阶段结束之后，内存空间回收完毕，对象被销毁。

- Java 对象生命周期中可能会受到以下过程的影响（延长或缩短

  - 堆内存分配
    - 对象在创建时需要在堆内存中动态分配内存空间，如果堆内存不足，就会导致 OutOfMemoryError 等异常
  - 垃圾回收
    - 当对象不再被引用时，可能被收回垃圾回收器回收，回收的时机取决于垃圾回收器的算法和 JVM 的设置。垃圾回收可能会大幅度降低程序的性能，尤其是对于大型和高并发的应用程序

  - 反射
    - 通过反射 API，对象的生命周期可以被延长，对象可以在其原有生命周期结束后继续被使用
  - 引用
    - 通过将对象存储不同的引用（强引用、软引用、弱引用或虚引用）中，可以对其生命周期产生影响。强引用会使对象一直保持可达状态，直到引用被取消或被赋予其他对象。软引用和弱引用可以允许对象在某些条件下被回收，虚引用是一种只被用于跟踪对象回收的引用类型

  - finalize() 方法
    - 当变量引用的对象被垃圾回收器回收时，JVM 会执行对象的 finalize() 方法，该方法中可以对对象进行一些资源清理等操作。finalize() 方法的执行可能会延长对象的生命周期
    - 如果在 finalize() 方法中重新设置了对象的引用变量，并使其重新变成了可达状态，那么这个对象的生命周期就会被延长，直到再次被回

## finalize（）

-  Java 中 Object 类中定义的方法，是垃圾回收期间调用的一个可覆盖的方法，一般被用来在垃圾回收被执行前释放一些资源或执行一些清理操
- 如果在 finalize() 方法中发生了异常，那么这个异常会被忽略，并且 finalize() 方法只会被执行一次

- 执行时机
  - `finalize()` 方法的执行具有一定的不确定性,因为其是由垃圾回收器决定何时执行的，并且在执行过程中可能会受到互斥锁的等待、线程崩溃、JVM 关闭等一系列问题的影响
  - 执行时机：`finalize()` 方法的执行时机并不靠谱。Java 虚拟机并不保证 `finalize()` 方法总能被及时地执行，也并不保证执行时机的准确性和唯一性
- 所以需要遵循以下几个原则
  - 在 `finalize()` 方法中，应该主动释放对象已经占据的资源或者断开外部的连接等，以便这些资源被回收
  - 在 `finalize()` 方法的结尾处，要调用超类 `Object` 的 `finalize()` 方法，以保证 Object 类的通用清理操作能够得到执行
  - 避免在 `finalize()` 方法中执行长时间耗时的操作， 因为这可能会导致阻塞垃圾回收器，影响正常垃圾回收的时间，从而影响整个应用程序的性能

## jvm堆的结构

- Java堆的底层数据结构通常是基于一个连续的、固定大小的内存块来实现的。这个内存块被划分为不同的区域，用于存储不同种类的数据
- 它主要包括新生代（Young Generation）和老年代（Old Generation），以及一些相关的辅助数据结构
  - 新生代（Young Generation）：
    - Eden空间：首次分配对象的地方。Eden空间是一个大的内存区域，用于存放新创建的对象。
    - Survivor空间：存放经过一次垃圾回收后仍然存活的对象。通常，新生代有两个相同大小的Survivor空间，分别为From空间和To空间。
  - 老年代（Old Generation）：
    - 老年代是用来存储长时间存活的对象的内存区域。这些对象往往是在新生代经过多次垃圾回收后仍然存活的对象。
  - 持久代（Perm Generation）：
    - 在旧的Java虚拟机版本中，持久代用于存储类信息、方法信息和常量池等数据。但在Java 8及以后的版本中，持久代被元空间（Metaspace）取代。
  - 元空间（Metaspace）：
    - 元空间是在Java 8及以后的版本中取代了持久代的概念。它用来存储类的元数据信息，如类的结构、方法等。元空间不再位于堆内存中，而是直接使用本机内存。
  - 辅助数据结构：
    - 栈（Stack）：用于保存函数调用和局部变量等数据。
    - 方法区（Method Area）：用于保存类的元数据、静态变量等。
    - 引用计数表（Reference Counting Table）：用于记录对象的引用计数，以辅助垃圾回收的判断。

## 元空间位置

- 元空间（Metaspace）不属于传统的堆（Heap）或方法区（Method Area）。在 JDK 8 及以后的版本中，元空间是用来存储类的元数据信息的一块特殊内存区域。

  传统的 JVM 中，方法区用于存储类的结构、方法、字段、常量池等信息。但是随着永久代（PermGen）的存在，方法区的大小是有限制的，因此可能会导致 OutOfMemoryError 错误。为了解决这个问题，JDK 8 引入了元空间来替代永久代。

  元空间使用的是直接内存（Direct Memory），它不受 JVM 堆的限制，可以动态地分配和释放内存，避免了传统方法区的一些限制。元空间用于存储类的元数据信息，包括类的结构、方法、字段、注解等。

  总结起来，元空间是一种替代永久代的特殊内存区域，用于存储类的元数据信息。它使用的是直接内存，而不是传统的堆或方法区。

## 元空间取代持久代？

- Java在之前的版本中使用持久代（Perm Generation）来存储类信息、方法信息和常量池等数据。然而，随着Java应用程序的复杂性和内存需求的增加，持久代存在一些限制和问题，因此在Java 8版本中引入了元空间（Metaspace）来取代持久代。

- 元空间（Metaspace）实际上是作为 JVM 的一部分存在的，用于存储类的元数据。与传统的永久代（PermGen）相比，元空间在内存中的分配方式有所不同，并且可以在需要时进行动态扩展。

  元空间在 JVM 内部使用的是直接内存（Direct Memory）来存储类的元数据，而不是使用 Java 堆内存。直接内存是由 JVM 使用操作系统的原生堆外内存来分配和管理的。因此，尽管元空间使用了直接内存，但它仍然属于 JVM 的一部分，因为 JVM 是负责管理并提供元空间的功能。

- 原因
  - 避免OutOfMemoryError：在持久代中，由于类和方法信息一直存在，如果有大量的类和方法信息加载到持久代中，有可能导致持久代的内存溢出（OutOfMemoryError）。使用元空间可以充分利用本机内存，减少了持久代内存限制和相关的内存溢出问题。
  - 自动内存管理：持久代需要手动设置初始大小和最大大小。而元空间根据需要自动分配和扩展。元空间的大小不再受限于固定的内存区域，可以使用主机操作系统提供的本机内存。
  - 类的卸载：持久代中的类一般难以卸载，即使不再被使用。这可能导致内存占用增加，无法及时释放资源。而元空间中的类可以更容易地被垃圾回收并卸载，从而有效释放内存。
  - 性能改进：元空间利用了本机内存，减少了对Java堆的依赖。这样可以减少垃圾回收的开销，提高应用程序的性能。
  
- 好处
  - 动态内存分配：永久代的大小是固定的，并且无法动态调整。这可能导致在特定场景下出现内存不足或浪费的问题。而元空间使用本地内存（Native Memory）来存储类的元数据，可以根据需要进行动态分配，避免了永久代固定大小的限制。
  - 元数据回收：在永久代中，类的元数据只能在Full GC（全局垃圾回收）期间才能进行回收。Full GC通常较少发生，并且需要较长时间，因此可能导致永久代的内存占用一直增加，最终导致内存溢出。而在元空间中，由于采用本地内存存储，一般的垃圾回收机制对元数据也一并进行回收，使得内存管理更加灵活，避免了永久代垃圾回收的问题。也可以手动调用类加载器的`unloadClass()`方法来卸载该类
  - GC效率改进：永久代的垃圾回收是基于标记-清除（Mark-Sweep）算法，垃圾回收效率较低，并且容易导致长时间的停顿。而元空间采用一般的垃圾回收机制，如新生代和老年代的回收算法，可以充分利用现代垃圾回收器的优势，提高垃圾回收效率，并减少垃圾回收导致的停顿时间。
  - 适应大规模应用：随着Java应用变得越来越复杂和大规模化，类的元数据占用的内存也越来越大。永久代在内存限制方面具有固定大小的限制，无法满足大型应用的需求。元空间使用本地内存，其大小可以根据需求进行动态调整，能够更好地适应大规模应用的需求。
  
- 总结
  
  - 使用元空间取代持久代的目的是解决持久代存在的一些限制和问题，提供更灵活、更高效的类和元数据存储方式，从而改善Java应用程序的内存管理和性能

## 元空间垃圾回收

- 元空间（Metaspace）不在堆（Heap）中，它使用的是直接内存（Direct Memory），而不受堆的垃圾回收机制的影响。

  元空间的垃圾回收主要通过内部的元数据清理和释放来实现。当类被加载到元空间时，会生成对应的元数据信息并保存在其中。当这些类不再被使用或引用时，元空间会自动进行垃圾回收。

  元空间的垃圾回收并不像堆那样有明确的垃圾回收算法（如标记-清除、复制、标记-整理等）。它的回收是基于元数据的引用计数和元空间内存的动态分配来实现的。当某个类或元数据不再被引用时，它们的内存会被自动回收并释放。

  需要注意的是，元空间相对于永久代（PermGen）而言是更加灵活和可扩展的。它使用的是直接内存，可以根据应用程序的需要进行动态分配和释放，不再受限于堆的大小。这也使得元空间不容易出现内存溢出的问题，相比之下，永久代在过去会因为存储类的元数据而导致内存溢出错误的发生。

## 垃圾回收算法

- 垃圾回收算法是用于自动管理和释放不再使用的内存资源的技术。在现代的垃圾回收实现中，常见的垃圾回收算法包括以下几种：

  1. 标记-清除（Mark and Sweep）算法：该算法分为两个阶段。首先，从根对象（如栈和静态变量）开始，标记出所有可达的对象。然后，清除（回收）所有未被标记的对象。这种算法简单直接，但可能会导致内存碎片问题。

  2. 标记-复制（Mark and Compact）算法：该算法也分为两个阶段，首先标记出所有可达的对象，然后将存活的对象复制到一个新的内存空间中，然后清理旧的内存空间。该算法解决了内存碎片的问题，但需要额外的复制操作。

  3. 标记-整理（Mark and Compact）算法：标记-整理算法与标记-复制算法类似，但在复制对象之前，会先进行一次对象的整理操作，将存活对象向一端移动，然后将剩余空间清除。这样可以解决内存碎片问题，而不需要每次都进行对象的复制。

  4. 分代（Generational）算法：该算法基于现实观察，将对象根据其存活时间分为不同的代。通常将新创建的对象放在新生代，稍长时间存活的对象放在老年代。对于新生代的对象，可以使用复制算法或者标记-整理算法，因为新生代对象的生命周期较短，产生的垃圾也较频繁。而对于老年代的对象，可以使用标记-清除或者标记-整理算法。  （标记复制比标记整理有更快的效率，但是占用的空间更大？）

     copy算法是在新生代执行的，因为新生代对象大多朝生夕灭，存活时间短，占内存小，所以一次gc后剩下的对象少而且小，直接从from survivor和eden区copy到to survivor就好了，这叫minor gc。标记整理是老年代常用的，因为老年代对象大而且生存时间长，不适合用copy算法，这叫major gc？

  5. 增量式（Incremental）算法：增量式算法将垃圾回收操作分解为多个阶段，在每个阶段之间允许程序继续执行。这样可以减少垃圾回收操作对程序的影响，但会增加算法的复杂性。

  这只是一些常见的垃圾回收算法，实际上还有其他的变体和组合算法。垃圾回收算法的选择取决于特定的应用场景和需求，需要平衡内存使用效率、吞吐量和延迟等因素。现代的垃圾回收器常常使用多种算法的组合，以便在不同场景下获得最佳的性能和效果。

## 本地内存和直接内存

- 概念
  - 本地内存（Native Memory）：本地内存是指操作系统分配给应用程序使用的内存区域。它是由操作系统直接管理和分配的，不受Java虚拟机的限制。本地内存包括了Java堆内存以外的内存区域，如堆外内存、栈内存、元空间等。
  - 直接内存（Direct Memory）：直接内存是Java NIO库提供的一种内存分配方式下的内存。它允许Java程序直接使用本地内存，而不通过Java堆内存来访问，以提高I/O操作的性能。直接内存使用ByteBuffer类的allocateDirect()方法进行分配，这些分配的内存并不受Java堆内存大小的限制，而是由操作系统分配和回收。
- 关系
  - 直接内存只是本地内存的一部分
- 本地内存是指整个操作系统分配给应用程序的内存区域，元空间使用的是本地内存中的一部分，因为它不受Java堆内存的限制
- 直接内存则是一种特定的内存分配方式下的内存，能够绕过Java堆内存进行直接访问，用于优化高性能的I/O操作

## cms

- CMS（Concurrent Mark Sweep）是一种并发标记和清除的垃圾收集算法，设计用于减少应用程序的停顿时间和提供更好的响应性能。

- 过程
  - 并发标记（Concurrent Marking）：
    - 初始标记（Initial Mark）：并发标记的第一个阶段是初始标记。在此阶段，垃圾收集器会检查从根节点开始的直接可达对象，并标记它们。为了准确标记这些对象，垃圾收集器会暂停应用程序的线程。初始标记通常是一个很短暂的停顿。
    - 并发标记（Concurrent Mark）：在初始标记后，垃圾收集器与应用程序线程并发运行。它标记所有从根节点可达的对象以及通过被标记对象引用的对象。并发标记阶段与应用程序并行执行，减少了对应用程序的停顿时间。
    - 重新标记（Remark）：在并发清除的过程中，可能会有新的对象产生，这些新对象需要被重新标记为活动对象。为了完成这个阶段，垃圾收集器会短暂地暂停应用程序的执行，并标记这些新生成的对象。重新标记阶段的停顿时间通常比初始标记稍长，但仍然要比传统的垃圾收集算法的全局标记阶段短。
  - 并发清除（Concurrent Sweep）：在并发清除阶段，垃圾收集器与应用程序并发工作，回收被标记为垃圾的对象。对象的内存空间被回收并重新标记为空闲，为新的对象分配做好准备。
  - 并发重置（Concurrent Reset）：最后一个阶段是并发重置阶段，用于清理并重置CMS算法的内部数据结构，为下一次垃圾收集周期做准备。
  
- 总结
  - 优点：
  
    - 减少停顿时间：CMS的主要优势是减少垃圾回收期间的停顿时间，使得应用程序能够以更快的速度响应用户的请求。
    - 并发执行：CMS与应用程序的主线程并发执行，充分利用多核处理器的优势，提高垃圾回收的效率。
  
    缺点：
  
    - 占用处理器资源：由于CMS在与应用程序并发执行，它会占用一定的处理器资源，可能会导致应用程序的吞吐量降低。
    - 内存碎片化：CMS算法不会进行整理操作，因此可能导致内存空间的碎片化，这可能会影响到后续对象的分配。
  
- 在CMS的特定阶段，比如初始化标记（Initial Mark）和重新标记（Remark）阶段，为了准确地标记存活对象，需要停顿所有的应用程序线程。这些停顿通常是短暂的，因为它们只是为了在标记过程中获取堆的一致快照。在这些阶段期间，应用程序的线程暂停执行，称为短暂停顿。一旦这些阶段完成，应用程序的线程就会被唤醒，恢复执行。
  
- cms可能触发full gc的情况
  
  - 并发失败（Concurrent Mode Failure）：当CMS在进行并发标记和并发清除的过程中，发现堆空间不足以容纳新的对象时，会触发并发失败。CMS会尝试进行一次Full GC来回收垃圾对象和整理内存，并重新执行垃圾收集。
  - 收集器过载（Collector Overhead Limit Exceeded）：由于CMS使用多线程并发进行垃圾收集，它会有一定的系统开销。如果这些开销超过了预设的阈值（即收集器过载限制），那么就会触发Full GC，以减少开销并恢复正常执行。
  - 显式触发（Explicit Invocation）：应用程序可以通过调用`System.gc()`方法显式触发Full GC。然而，强烈建议不要过度依赖显式触发Full GC，因为这违背了垃圾收集器自动管理内存的原则，并可能造成性能下降
  
- Full GC是一种较为昂贵的操作，通常会引起较长的停顿时间。由于CMS的设计初衷是尽量减少停顿时间，因此会尽量避免触发Full GC。但在某些情况下，如并发失败或收集器过载，触发Full GC可以清理垃圾对象并整理内存，以保持应用程序的正常执行。（补救

## 其他GC算法

- 除了CMS（Concurrent Mark-Sweep）之外，还有几种常见的垃圾回收算法可以用于处理Java应用程序的内存管理：

  1. 标记-清除算法（Mark and Sweep）：标记-清除算法是一种基本的垃圾回收算法。它分为两个阶段：标记阶段和清除阶段。首先，它会遍历对象图，标记出所有存活的对象。然后，在清除阶段，它会释放未标记的对象，即被判定为垃圾的对象。这种算法可能会导致内存碎片化问题。
  2. 标记-整理算法（Mark and Compact）：标记-整理算法是在标记阶段将存活对象标记后，对存活对象进行整理。它会将存活对象移动到内存的一端，以便为新的对象腾出连续的内存空间。这种算法解决了标记-清除算法可能导致的内存碎片化问题。
  3. 复制算法（Copying）：复制算法将堆内存划分为两个区域，一半作为活动区域，一半作为闲置区域。当活动区域满了时，它会将存活对象复制到闲置区域，并对活动区域进行清除。这种算法消耗了一部分内存空间，但它避免了内存碎片化，并且具有高效的回收速度。常见的复制算法有半区复制算法和伊甸园复制算法。
  4. 分代算法（Generational）：分代算法基于“新生代”和“老年代”两个假设：大部分对象在短时间内会被垃圾回收器回收，只有很少的对象存活较长时间。根据这个假设，分代算法将堆内存划分为多个区域，根据对象的生命周期将其分配在不同的区域中。一般来说，新生代使用复制算法，老年代使用标记-清除算法或标记-整理算法。

  根据应用程序的需求和场景，可以选择适当的垃圾回收算法。例如，CMS适用于对停顿时间敏感的大型应用程序，复制算法适用于小型临时对象较多的场景，分代算法适用于兼顾效率和内存利用率的场景等。 这些算法的选择通常由Java虚拟机的实现和配置参数来决定。

## new 对象会在元空间开辟物理空间？

- 创建对象不会直接在元空间开辟物理空间。元空间用于存储类的元数据信息，而对象实例数据是存储在堆中的
- 元空间主要用于存储类的元数据信息，如类定义、方法、字段等。它是属于JVM的一部分，而不是用于存储对象实例的物理空间。
- 当创建一个新的类时，JVM会在元空间中为该类分配空间，并存储该类的相关信息。这些信息包括类的结构、字段、方法、注解等。元空间的大小可以通过JVM的启动参数来配置。

## system.gc()

- `System.gc()`是Java中的一个方法，用于请求调用垃圾回收器进行垃圾回收。它是一个建议性的方法，即它只是向Java虚拟机发送了一条建议，告诉它可以执行垃圾回收操作(包括Young GC 和Full GC)
- 虚拟机可以选择忽略这个请求，或者在适当的时机执行垃圾回收。因此，`System.gc()`方法并不能保证立即触发垃圾回收。
- 过于频繁或滥用地调用`System.gc()`方法可能会对应用程序的性能产生负面影响。垃圾回收过程本身会占用一定的计算资源，频繁执行垃圾回收可能会导致额外的延迟和CPU消耗。因此，在大多数情况下，推荐让Java虚拟机自行管理垃圾回收过程。只在特定情况下（如进行性能分析或进行内存优化），才应该考虑手动调用`System.gc()`方法

## 各区内存溢出

- 当Java程序发生内存溢出时，主要涉及到以下几个区域和原因：
  - 堆内存溢出（Heap Overflow）：堆内存用于存储对象实例，当创建的对象过多或者对象过大，堆内存可能会耗尽并导致溢出。常见的原因包括内存泄漏、错误的对象生命周期管理、持续分配大对象等。
  - 栈内存溢出（Stack Overflow）：栈内存用于存储方法调用的信息、局部变量等。递归调用层次过深、局部变量占用的空间过大等情况会导致栈内存溢出。
  - 方法区溢出（Method Area Overflow）：方法区用于存储类的信息、静态变量、常量池等。当加载的类、静态变量或常量池过多，而方法区空间不足时，可能会导致方法区溢出。
  - 本地方法栈溢出（Native Stack Overflow）：本地方法栈用于调用本地方法（Native Method）。如果本地方法栈的空间不足以支持方法调用，就会出现本地方法栈溢出。
- 引起内存溢出的具体原因和解决方案因程序而异，可以通过以下方法来详细分析和解决内存溢出问题：
  - 错误日志和堆栈信息：当内存溢出发生时，Java虚拟机会生成错误日志和堆栈信息。查看错误日志和堆栈信息，定位导致溢出的具体位置，分析可能的原因。
  - Heap Dump分析：使用工具生成Heap Dump（堆转储）文件，将当前堆内存中的所有对象和数据保存下来。通过分析Heap Dump文件，可以确定哪些对象占用过多的内存、是否存在内存泄漏等问题。
  - 内存分析工具：使用专业的内存分析工具（如Eclipse Memory Analyzer、VisualVM等），通过导入Heap Dump文件或实时监测程序运行时的内存信息，进行更细致、深入的内存分析。
  - 代码审查：检查代码中是否存在资源未正确释放、循环引用、持续创建大对象等不合理的内存使用方式，进行代码审查和优化。
  - 调整虚拟机参数：根据具体情况，可以适当调整Java虚拟机的启动参数，如增加堆内存大小、方法区大小等。

## jdk1.7和1.8堆内存差异

- JDK 1.7和JDK 1.8在内存区域的管理方面存在一些区别，主要包括以下几个方面
  - 永久代（JDK 1.7） vs 元空间（JDK 1.8）：
    - JDK 1.7中使用永久代来存储类的元数据、静态变量等。永久代的大小是固定的，通常需要手动调整。
    - JDK 1.8中永久代被废弃，采用元空间来存储类的元数据。元空间使用本地内存来存储，大小可以根据需要进行动态调整，不再受到固定大小的限制。
  - 元数据的存储位置：
    - JDK 1.7中，类的元数据存储在永久代中。
    - JDK 1.8中，类的元数据存储在元空间中。元空间使用本地内存（Native Memory）来存储元数据，不再依赖于Java堆内存。
  - 字符串常量池的位置：
    - **JDK 1.7中，字符串常量池位于永久代中。（堆中的特殊区域）**
    - **JDK 1.8中，字符串常量池被移至（正常的）堆内存中**
  - 字符串垃圾回收机制：
    - JDK 1.7中，字符串常量池位于永久代，永久代的垃圾回收（Full GC）时才会回收其中的垃圾对象。
    - JDK 1.8中，字符串常量池位于堆内存中，一般的垃圾回收机制即可对其进行回收，避免了专门的Full GC。
  - 指针碰撞（JDK 1.7） vs TLAB（JDK 1.8）：
    - JDK 1.7中，堆内存中的对象分配使用指针碰撞算法，即对象在堆内存中紧凑排列，通过指针直接分配。
    - JDK 1.8中，引入了Thread-Local Allocation Buffer（TLAB）机制，将堆内存划分为多个线程本地的内存缓冲区，线程在自己的TLAB中分配对象，减少了锁竞争和线程同步开销。
- 需要注意的是，JDK 1.7和JDK 1.8的内存管理还有其他一些细微的差异，例如CMS垃圾回收器的行为变化、G1垃圾回收器在JDK 1.8中的加强等。选择合适的JDK版本和相应的配置调优，可以提高应用程序的性能和稳定性，并充分利用内存资源。

## 内存问题排查

- 内存问题是常见的软件开发和部署过程中遇到的挑战之一。以下是一些常用的方法和工具来解决内存问题
  - 查看错误日志和异常堆栈跟踪：
    - 内存溢出通常会导致Java虚拟机抛出OutOfMemoryError异常，错误日志中会包含异常的堆栈跟踪信息。观察堆栈跟踪，找到引发内存溢出的代码位置。
  - 分析内存使用情况：
    - 使用Java虚拟机自带的工具，如jmap、jcmd、jconsole等，来获取内存使用情况的统计数据。
    - 使用第三方的内存分析工具，如VisualVM、Eclipse Memory Analyzer等，来分析内存快照和堆转储文件，找出内存泄漏或大对象占用内存等问题。
  - 日志记录和分析：
    - 在应用程序中添加适当的日志记录，以便追踪内存使用情况和异常。使用日志分析工具（如ELK Stack，即Elasticsearch、Logstash和Kibana）可以帮助你分析和监控日志，从而找出内存相关的问题。
  - 内存泄漏检测和分析：
    - 使用工具来检测潜在的内存泄漏问题，例如通过垃圾收集器日志分析工具、内存泄漏检测器（如LeakCanary）、HeapDump工具等。
    - 使用内存分析工具来分析内存快照，查找无法释放的对象引用、长生命周期对象等。
  - 进行性能测试和压力测试：
    - 使用性能测试工具，如JMeter、ApacheBench等，来模拟并监测高负载场景下的内存使用情况。
    - 在真实环境中进行压力测试，以观察内存使用率和系统稳定性，并定位是否存在内存问题。
  - 代码审查和重构：
    - 定期进行代码审查，发现和修复潜在的内存问题。重构可能会改进内存使用效率，并减少内存相关的问题。
- 优化
  - 优化内存使用：
    - 使用更高效的数据结构和算法，减少内存的占用。
    - 避免创建过多的临时对象，如使用池化技术或重用对象来减少内存分配和垃圾回收的开销。
    - 合理管理和释放资源，及时关闭文件、数据库连接、网络连接等。
  - JVM调优：
    - 调整JVM的内存相关参数，如堆大小（-Xmx、-Xms）、新生代和老年代的比例（-XX:NewRatio）、垃圾回收器的选择等。
    - 开启合适的JVM调优选项，如做性能调优时可以通过-XX:+PrintGCDetails来查看GC日志。

## jvm运行在用户态

- 操作系统将Java程序编译为字节码，并由JVM解释和执行这些字节码。JVM是一个独立于操作系统的虚拟机，通过将字节码转换为操作系统可以理解的机器代码来执行程序
- 在JVM内部，它负责处理Java程序的内存管理、线程调度、垃圾回收、异常处理等任务。JVM在用户态运行，意味着它运行在应用程序的用户空间，在独立的进程中执行，并没有直接访问底层操作系统的权限。

- 当Java程序需要底层操作系统资源（如文件操作、网络通信等）时，JVM会通过Java Native Interface（JNI）进行交互，从而调用底层操作系统提供的相关功能。在这种情况下，涉及到与操作系统的交互，涉及到的代码会运行在内核态

## 怎么排查oom

- 当遇到 OutOfMemoryError（OOM）错误时，可以按照以下步骤进行排查：

  1. 确认错误类型：OutofMemoryError属于Java虚拟机的错误。根据错误的消息，可以了解到是哪个部分出现了内存不足的问题，例如Java堆内存、方法区、本地内存等。

  2. 查看堆栈跟踪：从错误消息中获取堆栈跟踪信息，找到引发OOM错误的代码位置。这将为问题的排查提供一个起点。

  3. 分析Dump文件：如果应用程序终止时产生了Dump（内存快照）文件，可以使用Java Heap Dump工具（如MAT、VisualVM等）打开该文件，以获取更多详细的内存使用信息。可以查看对象的分布情况、对象引用关系、内存泄漏等。

  4. 内存泄漏分析：根据堆栈跟踪信息和Heap Dump文件，分析是否存在内存泄漏。内存泄漏通常由无用的对象持续占据内存，导致无法回收的情况。

  5. 检查代码：仔细检查代码，看是否存在可能导致内存泄漏的问题，比如不正确的对象生命周期管理、资源未关闭等。查看是否有过度消耗内存的数据结构、循环引用等。

  6. 调整内存设置：根据应用程序的需求和可用内存资源，调整Java虚拟机的内存设置。可以通过调整-Xmx（最大堆内存）和-Xms（初始堆内存）等参数，以增加可用内存。

  7. 优化算法和数据结构：检查应用程序的算法和数据结构，确保它们是高效的，并合理使用内存资源。

  8. 使用内存监控工具：使用内存监控工具，如VisualVM、JConsole等，实时监控应用程序的内存使用情况，以便更早地发现和解决内存问题。

  9. 优化资源使用：确保使用完资源后及时释放，如关闭数据库连接、IO流等。还可以使用缓存、对象池等技术，减少资源的创建和销毁频率。

  排查OOM问题需要进行一系列的分析和调试，它可能涉及代码、内存设置和资源管理等多个方面。通过仔细检查和分析，可以找到导致内存不足的原因，并做出相应的优化和调整，以解决该问题。

## dump文件

- Dump文件是指在计算机系统或应用程序崩溃时生成的一种二进制文件。它保存了在崩溃发生前的内存状态和程序执行的相关信息，可以用于诊断和分析导致崩溃的原因。

  当计算机系统或应用程序遇到严重的错误或异常情况时，它们可能无法继续正常运行，这时系统会生成一个dump文件。这个文件中包含了进程的内存快照，也就是进程在发生崩溃之前所占用的内存空间的内容。这包括了程序的代码、数据、堆栈信息以及系统的调用栈和寄存器状态等。

  生成dump文件对于故障排除和调试非常有用，因为它提供了崩溃发生时的上下文信息，可以帮助开发人员或技术支持人员定位问题。通过分析dump文件，可以了解程序在崩溃时的状态，例如哪些线程正在运行、导致崩溃的代码位置、变量的值等。这有助于确定导致崩溃的异常、错误或逻辑问题，并进一步采取措施来修复这些问题。

  不同操作系统和开发平台可能使用不同的方式生成和处理dump文件。例如，在Windows操作系统中，当应用程序崩溃时，可以生成一个命名为"crash dump"（崩溃转储）的文件。在Linux/Unix系统中，生成的文件通常称为"core dump"（核心转储）。

  分析dump文件通常需要使用专门的调试工具或分析器。这些工具可以打开dump文件，并提供了丰富的功能，如检查线程状态、查看内存内容、分析函数调用栈等。这些工具的使用可以帮助定位问题的根本原因，并指导修复过程。

  总而言之，dump文件是在计算机系统或应用程序崩溃时生成的一种二进制文件，保存了崩溃发生前的内存状态和相关信息，用于故障排除和调试。通过分析dump文件，可以定位问题并解决程序崩溃的原因

## 垃圾回收器

**如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。**

虽然我们对各个收集器进行比较，但并非要挑选出一个最好的收集器。因为直到现在为止还没有最好的垃圾收集器出现，更加没有万能的垃圾收集器，**我们能做的就是根据具体应用场景选择适合自己的垃圾收集器**。试想一下：如果有一种四海之内、任何场景下都适用的完美收集器存在，那么我们的 HotSpot 虚拟机就不会实现那么多不同的垃圾收集器了。

JDK 默认垃圾收集器（使用 `java -XX:+PrintCommandLineFlags -version` 命令查看）：

- JDK 8：Parallel Scavenge（新生代）+ Parallel Old（老年代）
- JDK 9 ~ JDK20: G1

### [#](#serial-收集器)Serial 收集器

Serial（串行）收集器是最基本、历史最悠久的垃圾收集器了。大家看名字就知道这个收集器是一个单线程收集器了。它的 **“单线程”** 

 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ **"Stop The World"** ），直到它收集结束。

**新生代采用标记-复制算法，老年代采用标记-整理算法。**

![Serial 收集器](https://oss.javaguide.cn/github/javaguide/java/jvm/serial-garbage-collector.png)Serial 收集器

虚拟机的设计者们当然知道 Stop The World 带来的不良用户体验，所以在后续的垃圾收集器设计中停顿时间在不断缩短（仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续）。

但是 Serial 收集器有没有优于其他垃圾收集器的地方呢？当然有，它**简单而高效（与其他收集器的单线程相比）**。Serial 收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率。Serial 收集器对于运行在 Client 模式下的虚拟机来说是个不错的选择。

### [#](#parnew-收集器) ParNew 收集器

ParNew 收集器其实就是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和 Serial 收集器完全一样。

**新生代采用标记-复制算法，老年代采用标记-整理算法。**

![ParNew 收集器 ](https://oss.javaguide.cn/github/javaguide/java/jvm/parnew-garbage-collector.png)ParNew 收集器 

它是许多运行在 Server 模式下的虚拟机的首要选择，除了 Serial 收集器外，只有它能与 CMS 收集器（真正意义上的并发收集器，后面会介绍到）配合工作。

**并行和并发概念补充：**

- **并行（Parallel）**：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。
- **并发（Concurrent）**：指用户线程与垃圾收集线程同时执行（但不一定是并行，可能会交替执行），用户程序在继续运行，而垃圾收集器运行在另一个 CPU 上。

### [#](#parallel-scavenge-收集器) Parallel Scavenge 收集器

Parallel Scavenge 收集器也是使用标记-复制算法的多线程收集器，它看上去几乎和 ParNew 都一样。 **那么它有什么特别之处呢？**

- ```java
  -XX:+UseParallelGC
  
      使用 Parallel 收集器+ 老年代串行
  
  -XX:+UseParallelOldGC
  
      使用 Parallel 收集器+ 老年代并行
  
  ```

parallel Scavenge 收集器关注点是吞吐量（高效率的利用 CPU）。CMS 等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是 CPU 中用于运行用户代码的时间与 CPU 总消耗时间的比值。 Parallel Scavenge 收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解，手工优化存在困难的时候，使用 Parallel Scavenge 收集器配合自适应调节策略，把内存管理优化交给虚拟机去完成也是一个不错的选择。

**新生代采用标记-复制算法，老年代采用标记-整理算法。**

![Parallel Old收集器运行示意图](https://oss.javaguide.cn/github/javaguide/java/jvm/parallel-scavenge-garbage-collector.png)Parallel Old收集器运行示意图

**这是 JDK1.8 默认收集器**

使用 `java -XX:+PrintCommandLineFlags -version` 命令查看

- ```java
  -XX:InitialHeapSize=262921408 -XX:MaxHeapSize=4206742528 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGC
  java version "1.8.0_211"
  Java(TM) SE Runtime Environment (build 1.8.0_211-b12)
  Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode)
  
  ```

JDK1.8 默认使用的是 Parallel Scavenge + Parallel Old，如果指定了-XX:+UseParallelGC 参数，则默认指定了-XX:+UseParallelOldGC，可以使用-XX:-UseParallelOldGC 来禁用该功能

### [#](#serial-old-收集器) Serial Old 收集器

**Serial 收集器的老年代版本**，它同样是一个单线程收集器。它主要有两大用途：一种用途是在 JDK1.5 以及以前的版本中与 Parallel Scavenge 收集器搭配使用，另一种用途是作为 CMS 收集器的后备方案。

![Serial 收集器](https://oss.javaguide.cn/github/javaguide/java/jvm/serial-garbage-collector.png)Serial 收集器

### [#](#parallel-old-收集器) Parallel Old 收集器

**Parallel Scavenge 收集器的老年代版本**。使用多线程和“标记-整理”算法。在注重吞吐量以及 CPU 资源的场合，都可以优先考虑 Parallel Scavenge 收集器和 Parallel Old 收集器

集器。

![Parallel Old收集器运行示意图](https://oss.javaguide.cn/github/javaguide/java/jvm/parallel-scavenge-garbage-collector.png)Parallel Old收集器运行示意图

### [#](#cms-收集器) CMS 收集器

**CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常符合在注重用户体验的应用上使用。**

**CMS（Concurrent Mark Sweep）收集器是 HotSpot 虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。**

从名字中的**Mark Sweep**这两个词可以看出，CMS 收集器是一种 **“标记-清除”算法**实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤：

- **初始标记：** 暂停所有的其他线程，并记录下直接与 root 相连的对象，速度很快 ；
- **并发标记：** 同时开启 GC 和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以 GC 线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。
- **重新标记：** 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短
- **并发清除：** 开启用户线程，同时 GC 线程开始对未标记的区域做清扫。

![CMS 收集器](https://oss.javaguide.cn/github/javaguide/java/jvm/cms-garbage-collector.png)CMS 收集器

从它的名字就可以看出它是一款优秀的垃圾收集器，主要优点：**并发收集、低停顿**。但是它有下面三个明显的缺点：

- **对 CPU 资源敏感；**
- **无法处理浮动垃圾；**
- **它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。**

### G1 收集器

**G1 (Garbage-First) 是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC 停顿时间要求的同时,还具备高吞吐量性能特征.**

被视为 JDK1.7 中 HotSpot 虚拟机的一个重要进化特征。它具备以下特点：

- **并行与并发**：G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。
- **分代收集**：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。
- **空间整合**：与 CMS 的“标记-清除”算法不同，G1 从整体来看是基于“标记-整理”算法实现的收集器；从局部上来看是基于“标记-复制”算法实现的。
- **可预测的停顿**：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在垃圾收集上的时间不得超过 N 毫秒。

G1 收集器的运作大致分为以下几个步骤：

步骤：

- **初始标记**
- **并发标记**
- **最终标记**
- **筛选回收**

![G1 收集器](https://oss.javaguide.cn/github/javaguide/java/jvm/g1-garbage-collector.png)G1 收集器

**G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来)** 。这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 G1 收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。

**从 JDK9 开始，G1 垃圾收集器成为了默认的垃圾收集器**

### ZGC 收集器

与 CMS 中的 ParNew 和 G1 类似，ZGC 也采用标记-复制算法，不过 ZGC 对该算法做了重大改进。

在 ZGC 中出现 Stop The World 的情况会更少！

Java11 的时候 ，ZGC 还在试验阶段。经过多个版本的迭代，不断的完善和修复问题，ZGC 在 Java 15 已经可以正式使用了！

不过，默认的垃圾回收器依然是 G1。你可以通过下面的参数启动 ZGC：

```java
java -XX:+UseZGC className
```



# ==juc==

## ThreadLocal

- 源码

  - 部分

  ```java
  public class ThreadLocal<T> {
      static class ThreadLocalMap {
          static class Entry extends WeakReference<ThreadLocal<?>> {
              Object value;
  
              Entry(ThreadLocal<?> k, Object v) {
                  super(k);
                  value = v;
              }
          }
  
          private static final int INITIAL_CAPACITY = 16;
          private Entry[] table;
          private int size;
  
          ThreadLocalMap(ThreadLocal<?> firstKey, Object firstValue) {
              table = new Entry[INITIAL_CAPACITY];
              int i = firstKey.threadLocalHashCode & (INITIAL_CAPACITY - 1);
              table[i] = new Entry(firstKey, firstValue);
              size = 1;
          }
  
          private void set(ThreadLocal<?> key, Object value) {
              Entry[] tab = table;
              int len = tab.length;
              int i = key.threadLocalHashCode & (len-1);
  
              for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) {
                  ThreadLocal<?> k = e.get();
                  if (k == key) {
                      e.value = value;
                      return;
                  }
                  if (k == null) {
                      replaceStaleEntry(key, value, i);
                      return;
                  }
              }
              tab[i] = new Entry(key, value);
              int sz = ++size;
              if (!cleanSomeSlots(i, sz) && sz >= threshold)
                  rehash();
          }
      }
  
      private static final int HASH_INCREMENT = 0x61c88647;
      private final int threadLocalHashCode = nextHashCode();
  
      private static AtomicInteger nextHashCode = new AtomicInteger();
      private static int nextHashCode() {
          return nextHashCode.getAndAdd(HASH_INCREMENT);
      }
  
      protected T initialValue() {
          return null;
      }
  
      public T get() {
          Thread thread = Thread.currentThread();
          ThreadLocalMap map = getMap(thread);
          if (map != null) {
              ThreadLocalMap.Entry e = map.getEntry(this);
              if (e != null) {
                  return (T) e.value;
              }
          }
          return setInitialValue();
      }
  
      private T setInitialValue() {
          T value = initialValue();
          Thread thread = Thread.currentThread();
          ThreadLocalMap map = getMap(thread);
          if (map != null)
              map.set(this, value);
          else
              createMap(thread, value);
          return value;
      }
  
      private ThreadLocalMap getMap(Thread thread) {
          return thread.threadLocals;
      }
  
      private void createMap(Thread thread, T firstValue) {
          thread.threadLocals = new ThreadLocalMap(this, firstValue);
      }
  }
  ```

- 重点
  - ThreadLocal（当前线程创建，通过它去访问ThreadLocalMap），ThreadLocalMap是静态内部类
  - Thread中有ThreadLocal.ThreadLocalMap threadLocals = null;这个属性。所以每个线程都有自己的ThreadLocalMap
  - 访问时都是访问当前线程自己的ThreadLocalMap
  
- 原理
  - ThreadLocal 变量进行 get() 或 set() 操作时，首先获取当前线程的 ThreadLocalMap 对象（如果还未初始化，则进行初始化），然后将 ThreadLocal 对象作为 key，从中获取对应的 value，如果不存在，则通过 createMap() 方法获取初始值
  
    ```java
        public void set(T value) {
            Thread t = Thread.currentThread();
            ThreadLocalMap map = getMap(t);
            if (map != null) {
                map.set(this, value);
            } else {
                createMap(t, value);
            }
        }
    ```
  
    
  
  - ThreadLocalMap 绑定在当前线程，使用 ThreadLocal 对象作为 key，可以确同一线程每个 ThreadLocal 变量值与其他 ThreadLocal 变量值互相独立
  
  - ThreadLocalMap 的使用是在单一线程内部进行的，因此不需要考虑并发问题

## aqs

- 简介
  - AQS（AbstractQueuedSynchronizer）是Java中一种用于构建同步器的框架，它提供了一种基于FIFO等待队列的机制，用于实现各种同步器，如ReentrantLock、Semaphore、CountDownLatch等。
  - AQS的核心思想是状态管理和线程队列。它的内部维护了一个状态变量，通过CAS（Compare and Swap）操作来实现对状态的原子更新。在AQS中，线程会根据当前状态来判断是否可以继续执行，如果不能执行，则将当前线程加入等待队列，并进行自旋或者阻塞，直到满足执行的条件。
  - AQS是一个抽象类，在实现具体同步器时，需要继承AQS并实现相应的模板方法。这些模板方法会在具体的同步器中根据特定的需求来实现，从而实现特定的同步机制。
- AQS通过以下几个关键的方法实现了同步器的构建
  - acquire方法：当线程需要获取同步资源时，通过该方法实现等待和阻塞。如果资源被其他线程占用，当前线程就会被加入等待队列，并进入阻塞状态，直到资源释放。
  - tryAcquire方法：该方法用于非阻塞地尝试获取同步资源，如果成功获取资源，则返回true，否则返回false。
  - release方法：当线程释放同步资源时，通过该方法来释放。它会更新状态，并唤醒等待队列中的某个线程。
  - tryRelease方法：该方法用于非阻塞地尝试释放同步资源，如果成功释放，则返回true，否则返回false。
  - 其他辅助方法：AQS还提供了一些其他辅助方法，如Condition的相关方法，用于实现条件等待和通知。

- 结构
  - CLH队列（双向链表）：AQS中的等待队列使用CLH（Craig, Landin, and Hagersten）队列的变种实现，它是一种基于双向链表的队列。每个等待线程都会封装成一个Node对象，并按照FIFO的顺序加入到队列中。CLH队列支持高效的线程入队和出队操作，并且在多处理器环境下具有较好的扩展性。
  - State（同步状态）：AQS维护一个整型的状态变量，用于表示同步资源的状态。状态可以是共享的（如锁的可重入次数）或独占的（如锁的持有状态）。通过状态变量的高位和低位分别表示共享模式和独占模式的状态。
  - Condition队列：AQS提供了Condition接口，用于支持条件的等待和唤醒操作。它使用与CLH队列类似的方式实现等待队列，每个等待线程也会封装成一个Node对象，并加入到Condition队列中。
  - CAS（Compare and Swap）操作：AQS使用CAS操作（通过Unsafe类提供）来实现对状态变量的原子更新。CAS操作可以保证原子性，避免竞态条件，确保并发访问的正确性。
  - Node 类：等待队列中的每个节点对应一个等待线程。Node 类包含了线程等待状态、前驱节点和后继节点等信息。
- 资源的获取和释放
  - 当一个线程需要获取同步资源时，它会通过acquire方法尝试获取资源。如果资源已被占用，线程会被加入到等待队列中，并进入阻塞状态（或自旋等待）。
  - 当线程成功获取到资源时，AQS会更新状态变量，标识资源的拥有者，同时线程可以继续执行后续操作。
  - 当线程持有资源并需要释放时，它会调用release方法进行释放。释放操作可能会导致等待队列中的其他线程被唤醒，进而竞争资源。
- 同步状态的管理
  - AQS使用CAS操作来保证对状态变量的原子更新，避免竞态条件。
  - acquire方法和release方法会根据状态变量的值来判断是否满足获取或释放资源的条件。
  - 通过维护状态变量和等待队列，AQS实现了多线程的协调与同步，确保资源的互斥访问和正确的执行顺序

## aqs线程编排

- AQS 使用一个内部的FIFO（先进先出）双向队列，来维护等待获取同步状态的线程。当一个线程需要获取同步状态（比如锁）时，如果同步状态已被其他线程占用，该线程会被加入队列，并被阻塞等待
- 流程
  - 当线程需要获取同步状态时，会调用 `acquire()` 方法，并将自身加入到等待队列中的尾部。
  - 如果当前同步状态可用，即没有其他线程占用，该线程将成功获取同步状态，继续执行后续的操作。
  - 如果同步状态不可用，则当前线程会被阻塞，被设置为等待状态（即被park）。
  - 当其他线程释放了同步状态（比如释放了锁），AQS 会尝试唤醒等待队列中的下一个线程。
  - 被唤醒的线程将从阻塞状态变为可运行状态，并再次尝试获取同步状态。
  - 如果线程在等待过程中被中断或者超时，则会从等待队列中移除，并抛出对应的中断或超时异常。
- AQS 使用先进先出的队列模型来实现线程的排列。先到达的线程会先被加入到队列的尾部，并在同步状态可用时按顺序获取。这确保了在共享资源（比如锁）上的有序访问，并避免了线程饥饿的问题。

## 多线程下单例模式的问题

- 线程安全性问题：如果多个线程同时调用获取单例对象的方法，可能会导致多个实例的创建，违背了单例模式的初衷。例如，在懒汉式的单例模式中，当多个线程同时判断实例为空时，都会创建一个新的实例。
- 初始化问题：在多线程环境下，如果在使用单例对象之前进行一些复杂的初始化操作，可能导致多个线程同时进行初始化，引发错误或不确定的行为。
- 资源竞争问题：如果单例对象持有某些共享资源，多个线程同时对这些资源进行操作，可能引发竞争条件和数据不一致性问题。

## ConcurrentHashMap扩容

- 过程
  - 扩容触发：ConcurrentHashMap使用分段锁的方式来实现高效的并发操作。每当某个Segment的容量超过了阈值（一般情况下为Segment容量的3/4），即达到了负载因子时，就会触发扩容操作。
  - 分段锁更新：ConcurrentHashMap内部使用分段锁（Segment）机制，每个Segment都可以独立进行读写操作。在扩容过程中，仅有被扩容的Segment会被锁住，其他未被扩容的Segment仍然可以被其他线程并发访问和修改。这样可以减小锁的粒度，提高并发性能，并保证正在扩容的Segment的数据一致性。
  - 迁移标记：扩容操作会在正在被迁移的Segment中进行标记。在扩容时，先标记该Segment为迁移中（migrating）状态，然后进行数据的迁移和合并操作。其他线程在访问Segment时，会先检查标记，如果发现Segment正在扩容中，则需要额外的处理逻辑，如等待迁移结束或在旧表和新表之间进行数据双重校验等。这样可以保证在数据迁移期间，其他线程不会发生脏读或不一致的问题。
  - 数据迁移和复制：在扩容期间，被迁移的Segment中的数据会被复制到新的扩容Segment中。数据的复制会使用原子性的操作，如CAS（compare and swap）来确保多线程并发的正确性。只有当数据复制完成后，才会解除对被迁移Segment的访问锁。
  - 读写操作的兼容性：在扩容过程中，ConcurrentHashMap允许并发的读操作和写操作继续进行。读操作可以在新旧表之间选择，而写操作可以同时对旧表和新表进行，但写操作对于正在迁移的Segment会使用额外的迁移标记和CAS等机制来保证数据的正确性。
    - 如果读操作发生在扩容开始的阶段，此时新表已经创建但还未开始转移数据，那么读操作仅需要在旧表中进行。因为在此时旧表仍然包含了所有的有效数据，而新表还没有开始接收数据。
    - 如果读操作发生在扩容中或接近扩容完成的阶段，此时旧表和新表都可能包含有效数据。选择新表还是旧表取决于读操作发生的时间。如果读操作发生在数据迁移已经完成的语境下，那么可以直接在新表中进行查询，因为新表已经包含了所有的数据。如果读操作发生在数据迁移过程中，那么需要先在新表中进行查询，如果未找到，则需要在旧表中进行查询。
  - 延迟删除：为了避免在扩容期间发生并发冲突，ConcurrentHashMap会延迟删除旧表中的键
- 总结
  - 通过以上的扩容机制，ConcurrentHashMap可以在保持线程安全的同时实现高效的并发扩容。它通过分段锁和数据迁移的方式，充分利用了多核处理器的并行计算能力（读写兼容），同时控制了并发访问的冲突，提高了并发性能。这使得ConcurrentHashMap成为了处理多线程并发访问的哈希表实现之一

## CountDownLatch

- CountDownLatch（倒计时门闩shuan）是一种多线程同步工具，用于等待其他线程完成一组操作后再继续执行。它在Java中是通过java.util.concurrent包提供的一个类。

- 两个主要方法：

  - await()`：调用该方法的线程进入等待状态，直到计数器达到零或等待线程被中断。`
  - `countDown()`：减少计数器的计数值。当一个线程完成了需要等待的操作后，调用`countDown()`方法将计数器递减。

- CountDownLatch在创建时需要指定一个初始的计数值，这个值表示需要等待的操作的数量。每次调用`countDown()`方法都会减少计数器的值，直到计数器达到零为止。当计数器为零时，所有在`await()`方法上等待的线程将被释放，可以继续执行后续的操作。

- 应用场景

  - 主线程需要等待所有的子线程都完成某个任务后再进行下一步操作。
  - 多个线程同时进行一组独立的操作，主线程需要等待它们全部完成后再处理结果。

- 注意

  - 值得注意的是，CountDownLatch的计数器值一旦设置就无法修改。一旦计数器达到零，后续的`countDown()`方法调用将不起作用。如果需要重复使用CountDownLatch，可以考虑使用CyclicBarrier。

- 代码示例

  - 在示例中，创建了5个WorkerThread线程，并传入同一个CountDownLatch实例。每个线程执行一些模拟的任务，完成后调用`countDown()`方法减少计数器的值。最后，主线程调用`await()`方法等待所有线程完成任务，并在计数器达到零后继续执行。

  ```java
  import java.util.concurrent.CountDownLatch;
  
  public class Example {
      public static void main(String[] args) throws InterruptedException {
          int threadCount = 5;
          CountDownLatch latch = new CountDownLatch(threadCount);
  
          for (int i = 0; i < threadCount; i++) {
              Thread thread = new WorkerThread(latch);
              thread.start();
          }
  
          // 等待所有线程完成任务
          latch.await();
  
          System.out.println("All threads have completed their tasks");
      }
  
      static class WorkerThread extends Thread {
          private CountDownLatch latch;
  
          WorkerThread(CountDownLatch latch) {
              this.latch = latch;
          }
  
          @Override
          public void run() {
              // 模拟线程执行一些任务
              try {
                  Thread.sleep(1000);
              } catch (InterruptedException e) {
                  e.printStackTrace();
              }
  
              // 完成任务后减少计数器的值
              latch.countDown();
          }
      }
  }
  ```


## 优化（避免）死锁

- 方案
  - 避免多重锁的循环依赖：如果多个线程需要获取多个资源的锁，确保线程获取锁的顺序是固定的。例如，如果线程A需要先获取锁1再获取锁2，而线程B需要先获取锁2再获取锁1，那么就有潜在的死锁风险。为了避免死锁，可以要求所有线程按照相同的顺序获取锁，从而避免互相等待对方持有的锁。
  - 使用定时锁：有些情况下，获取锁的操作可能会被阻塞，并且无法确定阻塞的时间。为了避免线程无限等待锁而产生死锁，可以使用定时锁。定时锁提供了`tryLock()`方法，可以在一定的等待时间后返回结果，无需无限等待。如果在等待一段时间后未能获取到锁，可以采取其他处理方式，而不是等待导致死锁。
  - 尽量降低锁的粒度：过多的线程竞争共享资源的锁可能导致死锁。因此，在设计时尽量降低锁的粒度，减少锁的冲突，使得多个线程可以并发执行而不相互阻塞。例如，将大锁拆分为多个小锁，使得并发性提高，减少死锁的发生。
  - 使用线程安全的数据结构：线程安全的数据结构本身就提供了内部的同步机制，避免了手动加锁的错误操作。使用这些线程安全的数据结构（如ConcurrentHashMap、ConcurrentLinkedQueue等）可以减少自行加锁时的死锁风险。
  - 合理规划线程执行顺序：线程的执行顺序可能会影响死锁的发生。通过合理规划线程的执行顺序，可以避免因线程之间存在循环的依赖关系而导致死锁。例如，通过破坏循环等待条件，确保线程的执行顺序不会引发死锁。
  - 使用死锁检测和避免策略：一些编程语言或框架提供了死锁检测和避免的机制。例如，在Java中，可以使用`java.util.concurrent`包中的`Lock`接口和`Condition`接口，以及死锁检测工具（如ReentrantLock和ReentrantReadWriteLock），在设计时遵循相关的编程规范，预防和避免死锁的发生。
  - 优化并发设计：通过合理的并发设计，可以降低线程之间的竞争和冲突，从而减少死锁的发生。例如，可以使用无锁算法、CAS操作、异步编程等技术来减少对共享资源的争用，从而降低死锁的风险。
- 要的是要深入理解并发编程的原理、使用正确的同步机制、合理规划资源的获取顺序，并且在设计多线程应用时遵循最佳实践和经验教训。同时，进行充分的测试和性能分析，以确保多线程程序的安全性、可靠性和性能

## 线程池调优

- 线程池的调优是为了提高多线程应用程序的性能和效率。下面是一些常见的线程池调优策略
  - 合理设置线程池的大小：
    - 可以根据CPU的核心数量来确定线程池的大小，一般来说，线程池的最大线程数不要超过CPU核心数量的1-2倍。
    - 考虑任务的性质和计算资源的可用性。如果任务是CPU密集型的，线程数不要超过CPU核心数量太多（n+1），以充分利用CPU资源。如果任务是IO密集型的，可以适当增加线程池的大小（2n），以充分利用等待IO的时间。
  - 使用合适的线程池类型：
    - FixedThreadPool适用于负载比较稳定的场景，可以预先创建固定数量的线程，提供较好的线程复用和任务处理速度。
    - CachedThreadPool适用于负载波动较大的场景，可以动态调整线程池的大小，根据任务的数量自动增减线程。
    - ScheduledThreadPool适用于需要定时执行任务的场景，可以预先创建固定数量的线程，用于定时执行任务。
  - 设置合适的任务队列：
    - ArrayBlockingQueue对于任务数量比较固定的场景比较适用，可以设置较大的队列容量，避免任务因为队列已满而被拒绝。
    - LinkedBlockingQueue适用于任务数量可能会动态增长或者任务执行时间不确定的场景。可以根据需要设置合适的队列大小。
    - PriorityBlockingQueue适用于需要根据优先级有序执行任务的场景，可以根据任务的优先级来排序队列。
  - 设置适当的拒绝策略：
    - AbortPolicy是线程池默认的拒绝策略，会直接抛出RejectedExecutionException异常。可以在任务无法执行时，抛出异常并记录日志。
    - CallerRunsPolicy会使用提交任务的线程来执行任务。适用于对任务执行顺序有要求的场景，但可能会影响任务提交线程的性能。
    - DiscardPolicy会直接丢弃无法执行的任务，不做任何处理。
    - DiscardOldestPolicy会丢弃等待队列中最旧的任务，然后将当前任务加入队列。
  - 监控和调优线程池：
    - 使用Java自带的监控工具，如JConsole和VisualVM，来监视线程池的状态和执行情况，包括线程数、活动线程数、任务队列状态等。
    - 根据监控结果，可以根据应用程序的需求和性能瓶颈，调整线程池的配置参数，如最大线程数、任务队列大小等。
  - 优化任务的执行时间：
    - 可以优化任务的算法和实现，减少计算量和IO等待时间，以提高任务的执行效率。
    - 使用缓存来避免重复计算或者读取
  - 预热线程池：
    - 在任务开始前，通过提前执行一些初始化任务，来创建好核心线程，达到预热线程池目的。这样可以确保线程池中的线程已经创建并启动，避免任务提交时的线程创建和启动延迟。

## java线程和os线程映射

- JVM线程和操作系统线程之间的对应关系可以有两种方式实现：
  - 一对一关系：每个JVM线程都会直接映射到一个操作系统线程。这种模型可以在多核处理器上实现真正的并行执行，并充分利用多核资源。但是，在大规模并发的情况下，创建大量的操作系统线程会带来较大的开销，并且可能会导致操作系统线程资源耗尽。
  - 多对一关系：多个JVM线程共享一个操作系统线程。虚拟机会自己实现线程调度器，通过时间片轮转等调度算法，在不同的JVM线程之间切换执行。这种模型在资源消耗方面更加高效，减少了创建和维护操作系统线程的开销。但是，因为多个JVM线程共享一个操作系统线程，所以在某些情况下可能出现阻塞或等待的线程导致整个操作系统线程失效的情况。
- 具体采用哪种线程模型取决于Java虚拟机的实现和操作系统的支持。不同的虚拟机和操作系统可能使用不同的线程模型来实现JVM线程和操作系统线程的对应关系

## 内存屏障

- 内存屏障是一种处理器指令，用于控制指令执行的顺序和内存访问的可见性。它能够影响指令和数据的重排序、缓存的刷新和加载等行为。
- 在深入了解内存屏障之前，我们需要了解一些与内存屏障相关的概念：
  - 指令重排序：为了提高处理器的性能，处理器在执行指令时可能会对指令进行重排序，以隐藏指令延迟或提高指令的并行度。然而，多线程场景下，指令重排序可能会破坏程序的语义，导致不正确的结果。
  - 内存可见性：在多线程环境下，一个线程对共享变量的修改可能对其他线程是不可见的，这是由于处理器和编译器的优化以及缓存的存在。
- 三种类型
  - 写屏障（Store Barrier / Store Fence）：写屏障确保在写屏障之前的存储操作完成后，再执行写屏障之后的存储操作。它可以保证存储的顺序性。写屏障通常用于确保某些变量的存储操作对其他线程可见。
  - 读屏障（Load Barrier / Load Fence）：读屏障确保在读屏障之前的加载操作完成后，再执行读屏障之后的加载操作。它可以保证加载操作的顺序性。读屏障通常用于确保加载操作能够看到先前的写操作。
  - 全屏障（Full Barrier / Full Fence）：全屏障是同时具有写屏障和读屏障特性的屏障，它既保证存储的顺序性，也保证加载的顺序性。全屏障会禁止所有指令重排，并且强制所有未完成的操作立即完成。
- 应用场景
  - 线程同步：内存屏障可以与锁、条件变量等机制结合使用，实现线程之间的同步。通过在指定的位置插入内存屏障，在一些操作之前或之后，控制线程的访问顺序和数据的可见性。
  - volatile变量语义：在Java中，volatile关键字具有内存屏障的特性。对volatile变量的写操作将会在写操作之后插入写屏障，对volatile变量的读操作将会在读操作之前插入读屏障。这样可以确保volatile变量的写操作与读操作之间具有顺序性。
  - 优化和内存模型：内存屏障的使用还与底层的处理器架构、编译器和操作系统的内存模型有关。它可以用于优化并发程序的性能，控制内存访问的可见性，保证程序的正确性。
- 注意
  - 内存屏障的粒度和效果会根据不同的处理器架构、编译器和操作系统的实现而有所不同。在编写多线程程序时，正确使用内存屏障可以确保线程之间的同步和数据的一致性。然而，过度使用或不正确使用内存屏障可能会导致性能下降或产生其他问题，所以需要谨慎使用。正确理解和使用内存屏障需要深入了解底层的处理器架构和内存模型

## 线程a执行完后b再执行

- 方案

  - 使用Thread的join()方法：在A线程中，可以通过调用B线程的join()方法来等待B线程的完成。这会导致当前线程（A线程）暂停执行，直到B线程执行完成后再继续执行。

  ```java
  Thread threadB = new Thread(new BRunnable());
  threadB.start();
  
  // 在A线程中调用B线程的join()方法
  try {
      threadB.join();
  } catch (InterruptedException e) {
      e.printStackTrace();
  }
  
  // B线程执行完毕后，A线程继续执行下面的代码
  ```

  - 使用CountDownLatch：CountDownLatch是一个线程间的计数器，可以用来控制线程的执行顺序。在A线程中，可以创建一个CountDownLatch，并在B线程中调用countDown()方法来减少计数器，当计数器减少到0时，A线程就可以继续执行。

  ```java
  CountDownLatch latch = new CountDownLatch(1);
  
  // 在B线程中调用countDown()
  Thread threadB = new Thread(new BRunnable(latch));
  threadB.start();
  
  // 在A线程中调用await()，等待计数器减少到0
  try {
      latch.await();
  } catch (InterruptedException e) {
      e.printStackTrace();
  }
  
  // B线程执行完毕后，A线程继续执行下面的代码
  ```

  - 使用线程池的submit()方法：如果使用线程池，可以使用submit()方法提交任务，并返回一个Future对象。在A线程中，可以调用Future对象的get()方法来等待B线程的执行完成。

  ```java
  ExecutorService executor = Executors.newFixedThreadPool(2);
  
  Future<?> futureB = executor.submit(new BRunnable());
  
  // 在A线程中调用get()方法，等待B线程的执行完成
  try {
      futureB.get();
  } catch (InterruptedException | ExecutionException e) {
      e.printStackTrace();
  }
  
  // B线程执行完毕后，A线程继续执行下面的代码
  ```

## Java中线程通信的方式

- 使用共享变量：多个线程可以共享同一个变量，在操作共享变量时使用synchronized关键字来实现互斥访问，确保线程安全。
- 使用wait()和notify()方法：通过Object类中的wait()和notify()方法，可以实现线程的等待和唤醒操作，等待的线程会进入阻塞状态，直到被唤醒。
- 使用Lock和Condition：Lock接口和Condition接口提供了与synchronized关键字相似的功能，可以通过Lock接口的lock()和unlock()方法实现互斥访问，通过Condition接口的await()和signal()方法实现线程的等待和唤醒。
- 使用阻塞队列：Java提供了一些并发集合类，如ArrayBlockingQueue、LinkedBlockingQueue等，这些类可以用作线程间通信的容器，一个线程可以将数据放入队列，另一个线程可以从队列中取出数据，实现线程间的同步和通信。

## t1、t2交替打印1-100

- 无锁

  ```java
  //两个线程，一个打印奇数，一个打印偶数
  public class OneToHundred{
     static volatile int flag = 0;
     public static void main(String[] args){
        new Thread(new Task1(),"A").start();
        new Thread(new Task2(),"B").start();
     }
  }
  
  class Task1 implements Runnable{
     @Override
     public void run(){
       int i = -2;
       while(i<=99){
         if(OneToHundred.flag == 0){
            i+=2;
            System.out.println("a:" + i);
            OneToHundred.flag = 1;
         }
       }
     }
  }
  
  class Task2 implements Runnable{
     @Override
     public void run(){
       int i = -1;
       while(i<=98){
         if(OneToHundred.flag == 1){
            i+=2;
            System.out.println("b:" + i);
            OneToHundred.flag = 0;
         }
       }
     }
  }
  
  ```

  

- 有锁

  - 加锁

    ```java
    public class AlternatePrinting {
        private static final Object lock = new Object();
        private static int count = 1;
        private static final int MAX_COUNT = 100;
    
        public static void main(String[] args) {
            Thread t1 = new Thread(new Runnable() {
                // 里面不用加重写注解
                public void run() {
                    while (count <= MAX_COUNT) {
                        synchronized (lock) {
                            if (count % 2 == 0) {   // 这里不用判断也可以，判断下是为了更好控制输出
                                // 不加判断好像主线程结束不了
                                System.out.println("Thread 1: " + count);
                                count++;
                                lock.notify(); // 唤醒等待的线程     三个线程的话要notifyAll(),不然死锁
                            } else {
                                try {
                                    // t1释放锁并阻塞
                                    // 自己进入阻塞状态
                                    lock.wait(); 
                                } catch (InterruptedException e) {
                                    e.printStackTrace();
                                }
                            }
                        }
                    }
                }
            });
    
            Thread t2 = new Thread(new Runnable() {
                public void run() {
                    while (count <= MAX_COUNT) {
                        synchronized (lock) {
                            if (count % 2 == 1) { 
                                System.out.println("Thread 2: " + count);
                                count++;
                                lock.notify(); // 唤醒等待的线程
                            } else {
                                try {
                                    lock.wait(); // t2释放锁并等待被唤醒
                                } catch (InterruptedException e) {
                                    e.printStackTrace();
                                }
                            }
                        }
                    }
                }
            });
    
            t1.start();
            t2.start();
            
            // 主线程不用加也可以，应该是非主线程等待某个线程结束？
            //t1.join();
            //t2.join();
        }
    }
    ```

- Lock

  ```java
  import java.util.concurrent.locks.Lock;
  import java.util.concurrent.locks.ReentrantLock;
  
  public class Main {
      private static final int MAX_COUNT = 99;
      private  static int count = 1;
      private static final Lock lock = new ReentrantLock();
  
      public static void main(String[] args) throws InterruptedException {
          Thread t1 = new Thread(new PrintNumberTask(0));
          Thread t2 = new Thread(new PrintNumberTask(1));
  
          t1.start();
          t2.start();
  
          t1.join();
          t2.join();
      }
  
      static class PrintNumberTask implements Runnable {
          private int threadId;
  
          public PrintNumberTask(int threadId) {
              this.threadId = threadId;
          }
  
          @Override
          public void run() {
              while (count <= MAX_COUNT) {
                  lock.lock();
                  try {
                      if (count % 2 == threadId) {
                          System.out.println("Thread " + threadId + ": " + count);
                          count++;
                      }
                  } finally {
                      lock.unlock();
                  }
              }
          }
      }
  }
  ```

  

# ==cn==

## ARP协议

- Address Resolution Protocol，用于在IP网络中解析IPv4地址与MAC地址之间映射关系的协议
- 流程
  - 当一个主机在 IP 网络中想要发送数据到目标主机时，它首先会检查自己的 ARP 缓存表中是否已经有目标主机的 IP 地址和对应的 MAC 地址的映射关系。如果有，就直接使用缓存中的 MAC 地址进行通信。
  - 如果在 ARP 缓存表中找不到目标主机的 MAC 地址映射，发送方主机就会发送一个 ARP 请求广播，其中包含目标主机的 IP 地址，但是目标主机的 MAC 地址还未知。
  - 网络中的所有主机都会接收到这个 ARP 请求广播包，但只有与目标主机相应 IP 地址匹配的主机会响应。其他主机会忽略这个广播包。
  - 目标主机收到 ARP 请求后，会向发送方主机发送一个 ARP 响应包，其中包含目标主机的 MAC 地址。
  - 发送方主机将目标主机的 IP 地址和响应中的 MAC 地址存储在它的 ARP 缓存表中，以便今后的通信中直接使用。
  - 发送方主机在收到响应后，就可以使用目标主机的 MAC 地址进行后续的数据传输。
- 注意
  - ARP 是一种无连接协议，它在数据链路层（如以太网）上进行操作，只负责映射 IP 地址与 MAC 地址之间的关系。ARP 请求和响应是通过广播方式发送，所有主机都能接收并处理，因此需要避免 ARP 欺骗等安全问题
  - IPv6 网络中使用邻居发现协议（Neighbor Discovery Protocol，NDP）来取代 ARP 协议

# ==tcp、http==

## http状态码

- http1.1

- 1xx（信息类状态码）- 表示请求已被接收，服务器需进一步处理。
  - 100 Continue：表示服务器已经成功接收到请求的初始部分，客户端应该继续发送剩余的请求。
- 2xx（成功类状态码）：表示请求已成功被服务器接收、理解和处理。
  - 200 OK：表示请求已成功，服务器返回了请求的内容作为响应
  - 201 Created：表示请求已成功，并且服务器已经创建了新的资源。
  - 204 No Content：表示服务请求已成功处理，但响应报文中不包含实体的主体部分。
  - 206 Partial Content：表示服务器已成功处理了部分GET请求，响应报文中包含了请求范围内的实体内容 

- 3xx（重定向类状态码）：表示需要进一步操作才能完成请求。
  - 301 Moved Permanently：表示请求的资源已永久移动到新的URL。客户端应该使用新的URL进行后续请求
  - 302 Found：表示请求的资源暂时从不同的URL获取，临时重定向。客户端应该使用新的URL进行后续请求
  - 304 Not Modified：表示客户端发送的条件请求未满足，因此可以使用缓存的版本。服务器未返回实体内容
- 4xx（客户端错误类状态码）：表示客户端发送的请求有错误。
  - 400 Bad Request：请求无效，服务器无法解析。
  - 401 Unauthorized：表示请求需要进行用户验证。
  - 403 Forbidden：表示服务器拒绝请求，没有权限访问。
  - 404 Not Found：表示请求的资源不存在。
  - 405 Method Not Allowed：表示请求方法不被服务器允许。
  - 408 Request Timeout：表示客户端请求超时。
- 5xx（服务器错误类状态码）：表示服务器在处理请求时出现错误
  - 500 Internal Server Error：表示服务器遇到了不可预期的错误。
  - 502 Bad Gateway：表示作为网关或代理服务器的服务器从上游服务器接收到无效响应。
  - 503 Service Unavailable：表示服务器暂时不可用，通常是由于过载或维护。
  - 504 Gateway Timeout：表示作为网关或代理服务器的服务器在等待上游服务器响应时超时。
  - 505 HTTP Version Not Supported：表示服务器不支持请求中所使用的HTTP协议版本。

## http和rpc

- 
  HTTP（Hypertext Transfer Protocol）和RPC（Remote Procedure Call）是两个不同的概念和协议。

  HTTP是一种用于客户端和服务器之间传输超文本的通信协议。它基于请求-响应模型，客户端发送HTTP请求到服务器，服务器返回HTTP响应给客户端。HTTP通常用于Web应用程序，并且是基于文本的协议。

  RPC是一种用于远程调用的协议。它允许一个应用程序在其他应用程序上调用一个过程（或函数）而不必关心具体的网络细节。RPC可以使用不同的传输协议进行通信，包括HTTP、TCP、UDP等。RPC通常用于分布式系统和服务间的通信。

  尽管HTTP可以被用作RPC的底层传输协议之一，但并不意味着HTTP就是RPC。RPC比HTTP更为灵活，可以使用多种协议进行通信，而HTTP是其中一种可能的选择。

## rpc协议所在的层

- RPC（Remote Procedure Call）协议并没有一个固定的层级，它通常被认为是一个跨越多层的概念。
- 多层
  - 在网络协议栈中，RPC协议通常被用于实现应用层之间的远程通信。因此，RPC协议可以在应用层中运行，与其他应用层协议（如HTTP、SOAP等）并行工作。
  - RPC协议也可以在传输层的基础上构建，比如在TCP或UDP的基础上实现RPC通信。这种情况下，RPC协议会作为传输层协议的一部分来工作。
  - 一些RPC框架还会涉及到中间件层，例如使用消息队列或分布式系统来实现RPC通信。在这种情况下，RPC协议可能会作为中间件层的一部分来协调远程调用。
- 总结
  - RPC协议的具体位置会根据实现方式、框架和使用环境的不同而有所变化。它可以在应用层、传输层或中间件层中操作，以实现不同的远程过程调用需求。 

## tcp粘包

- CP粘包是指在TCP传输中，发送方发送的数据被接收方接收时出现粘连在一起的情况，接收方无法正确切分出发送方原本分开的数据包的现象

- 原因

  - Nagle算法：发送端为了提高传输效率，该算法可能会将多个较小的数据块合并成一个大的数据块进行发送。这种情况下接收端会接收到多个数据块合并在一起的情况，导致粘包现象。

  - 接收端不及时读取数据：如果接收端的应用程序没有及时读取数据，TCP缓冲区中的多个数据包可能会一起被读取，形成粘包。
  - 操作系统缓冲区：操作系统在接收数据时也可能会进行缓冲，当缓冲区满了才将数据交给应用程序处理，这可能造成多个数据包一起被交给应用程序，导致粘包。

- TCP本身并没有专门的策略来解决粘包问题，因为TCP只负责提供可靠的字节流传输，不关心应用层的数据拆分与组装

- 应用层处理

  - 定长数据包：发送方在发送数据之前将数据包按照固定长度进行拆分，在接收方按照相同的固定长度进行接收和处理。这种方式确保每个数据包长度统一，易于解析。
  - 消息长度字段：发送方在发送数据之前，在数据包头部添加一个固定长度的字段，记录当前数据包的长度。接收方在接收到数据后先读取消息长度字段，然后根据长度字段的值读取相应长度的数据，避免粘包问题的发生。
  - 分隔符：发送方在数据包之间添加一个特定的分隔符，接收方根据分隔符进行数据的拆分和处理。这种方式需要特定的分隔符，应注意在数据中不要出现与分隔符相同的字符，以免错误解析数据。
  - 应用层协议：设计一个自定义的应用层协议，包含数据包的头部和数据部分，接收方根据协议结构解析数据包。协议可以定义消息长度、消息类型、校验和等字段，以保证数据包的正确拆分和解析

## rpc&http

- rpc
  - RPC是一种用于进行远程方法调用的协议，可以实现不同计算机或不同进程之间的通信。它旨在使远程调用像本地调用一样简单，隐藏了底层通信细节。RPC通常基于底层的网络传输协议（如TCP/IP）来实现通信。
  - 在某种程度上，RPC确实需要定义自己的协议和通信规范。许多RPC框架都提供了自己的协议和通信规范，以简化开发过程并提供更高层次的抽象。这些RPC框架通常负责处理底层的网络通信、序列化、远程方法调用以及错误处理等细节，使开发人员能够更专注于业务逻辑
    - gRPC：由Google开发的高性能、开源的RPC框架，使用Protocol Buffers作为默认的数据序列化格式，并提供多种编程语言的支持。
    - Apache Thrift：由Apache基金会开源的跨语言RPC框架，提供了自己的IDL（Interface Definition Language），支持多种传输协议和数据序列化格式。
    - Apache Dubbo：由Apache基金会开源的Java RPC框架，专注于高性能和易用性，支持服务注册、负载均衡和容错等特性。
- http
  - HTTP是一种用于在客户端和服务器之间传输数据的协议。它是Web应用程序的核心协议，用于在客户端浏览器和Web服务器之间进行请求和响应。HTTP是一种无状态的、面向文本的协议，它使用URI（Uniform Resource Identifier）定位资源，并使用请求方法（如GET、POST等）进行操作。
- 对比
  - 设计目标和语义:
    - RPC的设计目标是在不同的系统之间提供一种机制，使得远程过程调用（函数或方法调用）能够像本地调用一样方便。RPC中的方法调用通常被认为是同步的，并且在远程系统上执行。RPC的语法是面向服务的，它应该尽可能地隐藏网络通信的复杂性，使得用户感觉不到在远程调用中的细节。
    - HTTP是一种用于传输和交互信息的协议，主要用于Web应用程序。它是无状态的，每个请求都是独立的，并使用请求和响应模型。HTTP的方法（如GET、POST、PUT等）指定了请求在服务器上的操作，而响应状态码则表示请求的结果。HTTP的语法是面向文本的，其消息格式遵循特定的规范，例如请求行、头部和消息体。
  - 数据格式和传输方式:
    - RPC通常可以支持多种数据格式，包括二进制和文本格式。例如，Google的Protocol Buffers和Apache Thrift采用二进制格式，以提供更高效的序列化和传输性能。RPC框架还可以提供对称加密、压缩和其他优化技术来提高数据传输的效率和安全性。
    - HTTP主要使用基于文本的数据格式，如JSON、XML和HTML。这些数据格式易于理解和解析，适用于Web应用程序和API之间的通信。尽管HTTP也可以使用二进制数据，例如使用HTTP/2协议的二进制传输，但可读性更好的文本数据格式仍然被广泛使用。
  - 应用场景和支持:
    - RPC通常用于构建分布式系统中的组件之间的高性能通信，例如微服务架构中的服务之间的调用。RPC可以更紧密地集成到应用程序的内部，因为它们可以提供更底层的控制和优化。
    - HTTP广泛应用于Web开发中，包括浏览器和服务器之间的通信以及Web API的实现和调用。由于HTTP是一个通用且成熟的协议，几乎所有的编程语言和平台都提供了对HTTP的良好支持。
- 状态
  - RPC本身并没有规定其是否是无状态协议，这是取决于具体的实现方式和所采用的协议。
  - http是无状态，可根据需求提供状态管理
  - 有状态
    - 会话管理：服务器可以为每个客户端维护一个会话，并将会话标识符与客户端进行关联。客户端在每个RPC请求中提供会话标识符，服务器使用该标识符来查找和恢复相关的会话信息。服务器可以在会话中存储特定的状态数据，并在后续请求中使用它们。
    - 上下文传递：客户端在每个RPC请求中提供额外的上下文信息，这些信息可以包含环境变量、用户凭证、会话令牌等。服务器可以使用这些上下文信息来区分和识别每个请求，然后根据上下文信息来决定请求的处理方式和返回结果。
    - 连接保持：在底层的网络通信中，客户端和服务器之间可以维持长连接，而不是在每个请求之后立即断开连接。这样可以让服务器保持客户端的状态信息，而不必每次请求都重新初始化。服务器可以在连接级别保存状态，并在后续的请求中重新使用它们。

## tcp面向字节，udp面向报文

- TCP 
  - 一种面向字节的传输协议。它将数据拆分成一个个的字节流，并根据一定的规则进行有序传输。TCP 会将数据分割为更小的数据段（segment），然后将这些数据段按顺序发送给接收方，接收方再将这些数据段重新组合成完整的数据。这种方式确保了数据的可靠性和完整性，因为数据段的发送和接收都有确认机制。
  - 还是看成数据段吧，毕竟对每个字节进行编号 有点太离谱
- UDP 
  - 一种面向报文的传输协议。它以报文为单位进行数据的传输。对于 UDP 来说，数据没有被划分为连续的字节流，而是作为完整的报文被发送和接收。UDP 提供了一种简单的数据传输方式，它的优势是传输速度快且延迟低，适用于对实时性要求较高的应用，如视频和音频流传输。
- udp报文太大怎么办
  - 数据分片: 将大的 UDP 报文分成较小的片段进行传输。发送端将数据分割为适当大小的片段，并在每个片段中标记顺序和片段号。接收端接收到这些片段后，按照片段号重新组装成完整的数据。这种方式可以解决 UDP 报文太大导致无法完整传输的问题。
  - MTU（Maximum Transmission Unit）调整: MTU 是指在网络通信中一次能够传输的最大数据量。如果 UDP 报文超过了网络的 MTU 大小，它将被分片或丢弃。因此，可以尝试调整发送端和接收端所在的网络设备的 MTU 设置，将其增大以容纳更大的 UDP 报文。不过，这需要在网络环境允许的情况下进行调整。
  - 压缩数据: 如果 UDP 报文中的数据可以被压缩，可以考虑对数据进行压缩以减小报文大小。压缩后的数据可以节省网络带宽，并且可以减少 UDP 报文的大小，从而避免报文过大的问题。
  - 使用其他协议: 如果 UDP 报文真的太大无法满足需求，可以考虑使用其他协议，如 TCP，因为 TCP 允许传输更大的数据量。不过要注意的是，TCP 在可靠性和延迟方面与 UDP 存在差异。

## tcp有限状态机

- 有限状态机（Finite State Machine，FSM）是一种数学模型，用于描述系统或算法的行为，它由一组有限的状态和一组在不同状态之间的转换规则组成

- 当涉及TCP协议的有限状态机时，具体的状态和状态转换如下

  - CLOSED（关闭状态）：初始状态，表示连接未建立或已经关闭。
    - 转换条件：可以通过应用程序发起连接请求。

  - LISTEN（监听状态）：等待连接请求状态，表示服务器正在监听指定的端口号，准备接受传入的连接请求。
    - 转换条件：收到一个连接请求。

  - SYN-SENT（发起连接状态）：表示客户端已经发送了连接请求（SYN段），等待服务端的确认。
    - 转换条件：收到服务端的确认（SYN-ACK段）。

  - SYN-RECEIVED（接受连接状态）：表示服务器已收到客户端的连接请求，并发送了确认（SYN-ACK段），等待客户端的确认。
    - 转换条件：收到客户端的确认（ACK段）。

  - ESTABLISHED（连接已建立状态）：表示双方的连接请求已互相确认，可以开始传输数据。
    - 转换条件：可以通过正常的数据传输或收到关闭连接请求（FIN段）。

  - FIN-WAIT-1（主动关闭状态1）：表示主动关闭连接的一方发送了关闭连接请求（FIN段），等待对方的确认。
    - 转换条件：收到对方的确认（ACK段）。

  - FIN-WAIT-2（主动关闭状态2）：表示已发送关闭连接请求并收到对方的确认，等待对方发送自己的关闭请求。
    - 转换条件：收到对方的关闭连接请求（FIN段）。

  - TIME-WAIT（等待关闭状态）：等待关闭状态，表示双方都发送了关闭连接请求，并确认对方的关闭请求，等待一段时间以确保在该时间段内的延迟数据包都被处理完毕。
    - 转换条件：经过适当的时间，等待时间结束后，可以安全地关闭连接。

  - CLOSE-WAIT（被动关闭状态）：表示接收端已收到关闭连接请求，并发送确认，等待发送端关闭连接。
    - 转换条件：可以通过应用程序发起关闭连接请求。

  - LAST-ACK（最后确认状态）：表示发送端已收到接收端的关闭连接请求，并发送确认，等待接收端的确认。
    - 转换条件：收到接收端的确认（ACK段）。

  - CLOSING（关闭连接状态）：表示关闭连接的双方同时发送了关闭连接请求。
    - 转换条件：收到对方的关闭连接请求（FIN段）。

  - TIME-WAIT2（最终等待关闭状态）：表示关闭连接的双方都已发送了关闭连接请求，并确认对方的关闭请求，等待一段时间以确保在该时间段内的延迟数据包都被处理完毕后彻底关闭连接。
    - 转换条件：经过适当的时间，等待时间结束后，可以安全地关闭连接。

## TCP上层协议怎么提高链路利用率

- 要综合考虑网络环境、应用需求和具体的实现情况，合理选择适用的策略
  - 减少延迟：通过优化算法和机制，减少数据传输的延迟。例如，采用延迟感知的拥塞控制算法，及时调整发送速率和窗口大小，避免不必要的等待和传输延迟。
  - 增大窗口大小：TCP使用滑动窗口机制进行流量控制，发送端的窗口大小决定了发送端能够在没有接收端确认的情况下发送的数据量。通过增大窗口大小，可以提高发送端的发送能力，更充分地利用链路带宽。
  - 使用拥塞控制算法：TCP拥塞控制机制可以避免网络拥塞，确保链路的稳定性。选择合适的拥塞控制算法，并根据网络状况进行调整，可以提高网络利用率。例如，采用基于拥塞窗口的拥塞控制算法，根据网络拥塞程度调整发送速率。
  - 优化应用层协议：根据具体需求，优化上层应用协议的设计。例如，在数据的打包和传输上采用更高效的方式，减少协议头部的开销，避免不必要的重传或冗余数据的传输等。
  - 并行连接：如果一个连接无法充分利用链路带宽，可以考虑使用多个并行连接来同时进行数据传输。这样可以提高链路利用率，尤其在提供多个服务器IP地址时，可以利用多个连接同时传输数据。
  - 压缩和加速：利用数据压缩和加速技术，减小数据包的大小，从而减少传输延迟和占用带宽，提高链路利用率。例如，使用压缩算法对数据进行压缩，或者使用加速技术如HTTP加速、CDN等。

## RESTful

- RESTful（Representational State Transfer）是一种针对网络应用程序设计的软件架构风格。它强调以资源为中心，通过使用HTTP协议的不同方法（如GET、POST、PUT、DELETE等）来实现对资源的操作。
- 以下是一些常见的RESTful代码规范：
  - 使用合适的HTTP方法：RESTful API使用HTTP方法来表示对资源的不同操作。常见的HTTP方法有GET、POST、PUT、DELETE等。按照规范，GET用于获取资源，POST用于创建资源，PUT用于更新资源，DELETE用于删除资源。
  - 使用良好的URI设计：URI是表示资源的唯一标识符。在RESTful API中，URI应该明确、语义清晰，具备可读性。采用复数形式表示集合资源，使用名词表示特定资源。例如，`/users`表示用户集合资源，`/users/{id}`表示特定用户资源。
  - 使用合适的HTTP状态码：RESTful API使用HTTP状态码来表示请求结果的状态。常见的状态码有：
    - 2xx（成功）：表示成功处理请求，如200（OK）表示成功获取资源，201（Created）表示成功创建资源。
    - 4xx（客户端错误）：表示客户端发生错误，如400（Bad Request）表示请求无效，404（Not Found）表示资源不存在。
    - 5xx（服务器错误）：表示服务器发生错误，如500（Internal Server Error）表示服务器内部错误。
  - 使用内容协商：RESTful API 支持使用不同的数据格式来表示资源的表示（Representation）。常用的数据格式包括JSON、XML、HTML等。客户端可以通过在请求头中指定 Accept 字段来指定其期望的响应格式，而服务器可以在响应头中指定 Content-Type 来表示返回的数据格式。
  - 异常处理和错误消息：当发生错误时，RESTful API应该提供有意义的错误消息，以帮助开发者和客户端理解问题。错误消息应该具有一致的格式，包含错误码、错误描述和可能的解决方案。
  - 使用版本控制：为了兼容不同版本的API，应该对API进行版本控制。可以在URI中包含版本号，如`/v1/users`。版本控制可以通过请求头或URI参数进行指定，以确保兼容性和稳定性。
  - 使用安全机制：RESTful API 应该使用适当的安全机制来保护资源和用户的信息。常见的安全机制包括用户认证、授权等。可以使用标准的安全协议（如OAuth）来实现安全性。

## POST和PUT的区别

- PUT和POST都是HTTP协议中常见的请求方法，它们在语义和用途上有以下区别：

  1. 请求语义：
     - POST：表示创建资源或提交数据请求到指定的URL。通常用于在服务器上创建新的资源，或者执行不幂等的操作，比如提交表单、发布评论等。
     - PUT：表示更新或替换指定URL位置的资源。通常用于完整替换整个资源或创建新的资源，使用PUT方法时需要传递完整的资源表示。
  2. 幂等性：
     - POST：由于POST方法通常用于不幂等的操作，即多次调用同一个POST请求可能会在服务器上创建多个相同的资源或产生多个副作用，因此POST方法不具备幂等性。
     - PUT：PUT方法被设计为幂等的，即多次调用同一个PUT请求会产生相同的结果。使用PUT方法更新资源时，如果相同的请求多次执行，服务器的状态和资源内容应该保持一致。不幂等会返回执行失败
  3. 数据传递：
     - POST：POST请求通常将数据作为请求体中的参数传递，可以包含更丰富的数据类型。POST请求也可以使用URL中的查询参数来传递数据。
     - PUT：PUT请求通常将数据作为请求体中的整个资源表示传递，而不是作为参数。PUT方法要求客户端传递完整的资源表示，以便服务器进行替换或更新操作。
  4. 资源的创建与更新：
     - POST：POST方法常用于创建新的资源，即在服务器上生成一个新的资源。每次调用POST请求可能会产生不同的资源标识（比如数据库自动生成的ID）。
     - PUT：PUT方法常用于更新已存在的资源或替换整个资源。每次调用PUT请求应该更新或替换指定URL位置的资源，不会创建新的资源标识。

  根据以上区别，在设计和使用API时，需要根据具体的业务需求和语义要求来选择合适的请求方法（POST还是PUT）。要注意确保请求方法的语义和行为符合REST原则和业务逻辑的需要。

## https通信过程

- 基于tcp
  - HTTPS在TCP之上添加了加密层（SSL或TLS），它负责对传输的数据进行加密和解密，以确保数据的机密性和完整性。
  - 握手过程中的握手数据也会进行加密传输，使得HTTPS的通信更加安全可靠       ？
- 验证公钥的有效性，密文传输
  - 客户端（例如浏览器）向服务器发送HTTPS请求。请求的URL以https://开始，而不是常见的http://。
  - 服务器收到请求后，会生成一对密钥，包括公钥和私钥。公钥用于加密数据，而私钥用于解密数据。
  - 服务器将其公钥的副本发送给客户端。
  - 客户端收到服务器的公钥后，会验证其有效性。这一步确保客户端正在与预期的服务器通信，而不是中间人攻击者。验证通常涉及到与已知的可信证书颁发机构（CA）比对。
  - 在验证服务器的公钥有效性后，客户端会生成一个称为"会话密钥"的随机密钥。此会话密钥将用于在HTTPS通信过程中加密和解密数据。
  - 客户端使用服务器的公钥将会话密钥加密，并将其发送回服务器。
  - 服务器收到客户端加密的会话密钥后，会使用私钥进行解密，得到会话密钥。
  - 服务器和客户端现在都拥有相同的会话密钥，可以使用该密钥进行加密和解密通信中的数据。
  - HTTPS通信过程中的数据将使用会话密钥进行对称加密，并在传输过程中进行保护。
- 不同于传统的三次握手
  - HTTPS建立连接的过程包括了一个TLS/SSL协议的握手过程，该握手过程用于客户端和服务器之间的身份验证、协商加密算法和建立加密通道
  - 过程
    - 客户端向服务器发送一个加密连接请求，也称为ClientHello消息。该消息包含了客户端加密算法的列表、随机数等信息。
    - 服务器接收到ClientHello消息后，回复一个ServerHello消息。该消息包含了服务器选择的加密算法、服务器证书（包含公钥）、随机数等信息。
    - 客户端收到ServerHello消息后，验证服务器证书的有效性。这一步是为了确保客户端与预期的服务器通信，而不是中间人攻击。客户端会检查证书的签名、有效期等信息，并与已知的可信证书颁发机构（CA）进行比对。
    - 如果服务器证书验证通过，客户端会生成一个称为"会话密钥"的随机密钥，并使用服务器的公钥进行加密。
    - 客户端将加密后的会话密钥发送给服务器。
    - 服务器使用自己的私钥进行解密，得到会话密钥。
    - 客户端和服务器现在都拥有相同的会话密钥，并可以使用该密钥进行加密和解密通信中的数据。
  - 握手过程完成后，HTTPS的通信将使用会话密钥进行对称加密，确保数据的机密性和完整性。

## SSRF漏洞

- SSRF（Server-Side Request Forgery  服务端请求伪造）是一种安全漏洞，指的是攻击者可以利用目标服务器发起伪造的请求，从而使服务器访问攻击者指定的资源。这种漏洞可能导致一系列安全问题，包括远程代码执行、信息泄露和内部服务扫描等。

  具体来说，SSRF漏洞通常发生在Web应用程序中，攻击者可以通过操纵输入参数，使服务器去请求本应该被限制访问的内部资源或其他外部服务器上的敏感数据。攻击者可以利用这个漏洞来绕过防火墙、访问本地资源、扫描内部网络等。

- 当存在SSRF漏洞时，攻击者可以利用它来执行以下操作：

  1. 访问内部资源：攻击者可以构造恶意请求，使服务器去访问本应该是内部私有的资源，例如数据库、配置文件等。这可能导致敏感信息的泄露。
  2. 访问其他外部服务器：攻击者可以构造请求，使服务器去请求攻击者指定的外部服务器，从而导致攻击者能够在服务器上执行任意操作，例如访问敏感数据、发起攻击、传播恶意代码等。
  3. 通过反射型XSS攻击实现钓鱼攻击：攻击者可以利用SSRF漏洞操纵目标服务器发起伪造的请求，并将恶意脚本注入到返回的内容中。当用户访问包含注入脚本的页面时，恶意脚本会被执行，从而进行钓鱼攻击、窃取用户的敏感信息等。

- 为了防止SSRF漏洞，开发人员需要采取以下措施：

  1. 输入验证和过滤：对用户输入的URL参数进行有效的验证和过滤，确保只能访问可信任的资源。可以使用白名单机制、正则表达式、URL解析等方法来防止恶意URL的传递。
  2. 限制访问权限：服务器配置应该限制应用程序访问外部资源的权限，尽量避免直接访问受信任的服务器或内部资源。
  3. 使用安全库和框架：在开发过程中使用可信的安全库和框架，这些库和框架提供了处理HTTP请求的安全机制，例如设置安全的默认配置、防止重定向到内部IP地址等。
  4. 监控和日志记录：实施安全监控和日志记录机制，及时检测和响应SSRF攻击，并记录相关的信息以进行调查和纠正措施。

## https证书

- HTTPS证书是用于建立安全的HTTPS连接的数字证书，HTTPS（Hypertext Transfer Protocol Secure）是HTTP的安全版本。HTTPS使用加密机制来保护数据在客户端和服务器之间的传输过程，防止数据被窃听、篡改或伪装。

  HTTPS证书包含了一个实体（如网站、企业等）的身份信息，它由权威的证书颁发机构（Certificate Authority，CA）签发。证书中包括了以下信息：

  1. 域名：证书中包含了证书所属的域名，即被保护网站的域名。
  2. 公钥：证书中包含了加密通信所需的公钥，用于加密和解密数据。
  3. 数字签名：证书中包含了由证书颁发机构签发的数字签名，用于验证证书的真实性和完整性。通过验证签名，可以确保证书的合法性。

  HTTPS证书的工作原理如下：

  1. 客户端（浏览器）向服务器发起HTTPS连接请求。
  2. 服务器返回自己的数字证书，包含了公钥和其他信息。
  3. 客户端使用预装好的权威证书颁发机构的公钥来验证服务器证书的有效性。如果验证通过，客户端信任该证书，并生成一个用于加密通信的随机对称密钥。
  4. 客户端使用服务器的公钥对随机对称密钥进行加密，发送给服务器。
  5. 服务器使用自己的私钥对加密后的随机对称密钥进行解密，得到真正用于加密通信的对称密钥。
  6. 双方使用对称密钥进行加密和解密通信数据，确保数据的机密性、完整性和身份验证。

  HTTPS证书可以防止中间人攻击和数据窃听，为用户和网站提供了安全的通信环境。网站通常需要为其域名购买有效的HTTPS证书，并定期更新，以确保安全性和可靠性。

## FTP端口

- FTP协议使用两个端口进行数据传输：

  1. 控制连接端口：FTP的控制连接端口负责建立和管理FTP会话，包括传输指令、身份验证和控制命令。控制连接默认使用TCP协议的端口号21。

  2. 数据连接端口：FTP的数据连接端口用于实际的文件传输。数据连接有两种传输模式：主动模式（Active Mode）和被动模式（Passive Mode）。

     - 主动模式下，FTP服务器（也称为FTP服务端）向客户端发起数据连接，使用TCP协议的端口20进行数据传输。
     
     - 被动模式下，FTP服务器开启一个临时的高端口（通常大于1023），等待客户端发起数据连接。

  这两个端口在FTP协议中起到不同的作用，控制连接端口用于建立和维护FTP会话，而数据连接端口用于实际的文件传输。需要注意的是，FTP协议的数据传输部分不是加密的，因此在网络传输过程中可能存在安全风险。

  如果需要使用安全的文件传输协议，可以考虑使用基于SSL/TLS的FTPS（FTP Secure）或SFTP（SSH File Transfer Protocol），它们通常使用不同的端口进行数据传输，并提供加密和身份验证等安全功能。

# ==os==

## 操作系统对内存的管理方式

- 单一连续分区管理方式：
  - 工作原理：操作系统将整个内存空间划分为两个区域，一个区域用于存放操作系统和用户程序，另一个区域用于存放用户数据。
  - 优点：实现简单，适用于较小的系统。
  - 缺点：容易造成内存碎片，导致无法分配足够的连续内存空间来满足进程的需求。
- 固定分区管理方式：
  - 工作原理：操作系统将内存划分成多个固定大小的分区，每个分区只能用于运行一个进程。
  - 优点：能够同时运行多个进程，提高了系统的并发性。
  - 缺点：内存利用率相对较低，分区尺寸固定，无法充分利用不同进程的内存需求，存在外部碎片问题。
- 可变分区管理方式：
  - 工作原理：操作系统允许动态地分配和回收内存分区，根据进程的需要分配合适大小的分区。
  - 优点：提高了内存利用率和系统的灵活性，减少了外部碎片。
  - 缺点：仍然存在外部碎片问题，分配和回收分区可能导致内存空间的不连续。
- 页式存储管理方式：
  - 工作原理：操作系统将内存划分为固定大小的页框（物理页），进程的地址空间也划分为相同大小的页（逻辑页），通过页表将逻辑页映射到物理页。
  - 优点：消除了外部碎片问题，实现了非连续的内存分配，能够更高效地利用内存空间。
  - 缺点：可能存在内部碎片，一页可能未被完全用满。
- 段式存储管理方式：
  - 工作原理：操作系统将进程的地址空间划分为逻辑段，每个逻辑段有不同的大小和属性。物理内存被分成段框，通过段表将逻辑段映射到物理段框。
  - 优点：提供了更灵活的内存管理，每个段的大小根据实际需求进行分配，减少了浪费。
  - 缺点：同样存在外部碎片问题，可能导致内存利用率下降。
- 段页式存储管理方式：
  - 工作原理：将进程的地址空间划分为逻辑段，每个逻辑段再被分成固定大小的页。通过段表将逻辑段映射到物理内存的页框，通过页表将逻辑页映射到物理页。
  - 优点：兼具段式存储和页式存储的优点，提供了更灵活、高效的内存管理方式。
  - 缺点：需要同时管理段表和页表，增加了管理的复杂性和存储开销。
- 内部碎片（Internal Fragmentation）
  - 指已分配给进程的内存空间中存在未被使用的部分
  - 它发生在静态分区（如固定分区）或者某些需要按固定大小分配内存的管理方式（如页式存储管理）中。
  - 内部碎片是由于要求分配给进程的内存大小不是分配单位的整数倍而造成的。虽然内存已被分配出去，但其中的一部分是未被进程真正利用的，从而浪费了内存资源。
  - 优化
    - 优化内存分配算法，例如使用最优适应算法或首次适应算法来选择合适大小的内存块。
- 外部碎片（External Fragmentation）
  - 指分配给进程的内存空闲区域呈现不连续的状态，导致无法满足大于任何单个可用空闲区域的内存请求
  - 外部碎片通常发生在动态分区管理方式，如可变分区、段式存储管理和段页式存储管理中
  - 随着进程的加载和卸载，内存中的空闲区域可能变得零散分布，即使总的空闲内存足够，但无法分配给某个进程，造成内存资源无法最大化利用。
  - 优化
    - 动态内存管理技术，如内存紧缩（Memory Compaction）、内存合并（Memory Coalescing）等，通过移动进程或合并空闲内存块来减少或消除碎片。
    - 虚拟内存技术，如分页和分段，来降低外部碎片问题。？？？



## 什么是虚拟内存

- 虚拟内存（Virtual Memory）是计算机系统中的一种技术，它可以将计算机的物理内存和磁盘存储结合起来，为每个进程提供一个抽象的、相对较大的地址空间。  运行的放在内存中，还没运行的放在磁盘

  虚拟内存通过将进程的地址空间划分为多个固定大小的虚拟页面（Virtual Page）来实现。这些页面被映射到物理内存或磁盘上。当进程访问一个虚拟页面时，虚拟内存管理器会根据需要，将相应的虚拟页面加载到物理内存中，或者从物理内存交换出去。

  虚拟内存的主要目的是提供以下几个方面的好处：

  1. 扩展地址空间：每个进程可以拥有更大的虚拟地址空间，远远超过实际可用的物理内存大小。
  2. 内存隔离：每个进程的虚拟内存空间是相互隔离的，使得进程之间不会直接干扰彼此的内存。
  3. 资源管理：虚拟内存允许系统根据需求，动态地管理物理内存和磁盘空间，使得多个进程可以共享有限的物理内存并且能够合理利用磁盘空间。
  4. 共享内存：虚拟内存允许多个进程共享同一物理内存页，从而提供更高的内存利用率和更高效的数据共享。（物理上吧，不算虚拟内存共享吧）

  当一个进程需要访问虚拟内存中的某个页面时，虚拟内存管理器会根据页面表（Page Table）将虚拟地址翻译为物理地址。如果页面已经在物理内存中，那么访问就会直接在物理内存中进行。如果页面不在物理内存中，就会产生一个缺页中断（Page Fault），虚拟内存管理器会将相应的页面从磁盘加载到物理内存，并重新执行引起缺页中断的指令。

  总之，虚拟内存技术通过在物理内存和磁盘之间建立抽象的、可扩展的地址空间，提供了更好的资源管理和内存使用效率，提高了系统的可用性和性能。

- 虚拟内存的实现

  - 分页和分段是计算机系统中用于实现虚拟内存的两种主要技术。

    分页（Paging）是将进程的地址空间划分为固定大小的页（Page）的过程。每个页的大小通常为2的幂次方，如4KB或8KB。物理内存也被划分为与页大小相同的页框（Page Frame）。

    分页机制通过建立一个页表（Page Table）来实现虚拟地址到物理地址的映射。页表记录了虚拟页号和对应的物理页框号之间的映射关系。当进程访问某个虚拟地址时，分页系统会根据页表将其转换为对应的物理地址，并进行访问。

    分段（Segmentation）是将进程的地址空间划分为不同大小的段（Segment）的过程。每个段代表了进程中的一个逻辑单元（如代码段、数据段等），每个段可以具有不同的大小。

    分段机制通过建立一个段表（Segment Table）来实现虚拟地址到物理地址的转换。段表记录了虚拟段号和对应的物理地址之间的映射关系。当进程访问某个虚拟地址时，分段系统会根据段表将其转换为对应的物理地址，并进行访问。

    分页和分段的目的都是为了支持虚拟内存，扩展进程的地址空间，并提供了地址映射和保护机制。分页主要关注地址空间的划分和页面的调度，而分段主要关注逻辑单元的划分和段的管理。

    通常，分页与分段可以结合使用，形成分页分段系统，以充分发挥它们各自的优势。分页分段系统可以同时提供更大的地址空间、更灵活的逻辑组织和更高效的内存管理。

## 虚拟内存管理

- 虚拟内存技术是一种操作系统使用的高级内存管理技术，它允许进程访问比实际物理内存更大的地址空间。虚拟内存技术的基本思想是将进程的内存分为固定大小的块，称为页面（page），并将页面映射到物理内存或辅助存储器（如硬盘）上。
- 流程
  - 分页（Paging）：操作系统将进程的地址空间划分为固定大小的页，同样大小的物理内存也划分为页框。通过页表将逻辑页映射到物理页框，实现逻辑页和物理页之间的地址映射。
  - 页面置换（Page Replacement）：当进程请求的数据不在物理内存中时，操作系统会选择一个页进行置换，将其从内存中移出，并将新需要的页加载到内存中。页面置换算法（如最优（Optimal）、最近最久未使用（LRU）等）有助于决定选择哪个页面进行置换。
  - 页面调度（Page Fault Handling）：当进程访问一个尚未分配给它的页面时，会发生页面错误（Page Fault）。操作系统会根据需要从辅助存储器中加载页面，并更新页表进行映射。这个过程对于进程来说是透明的
- 优点
  - 扩大了地址空间：虚拟内存技术允许进程访问比实际物理内存更大的地址空间，从而允许同时运行更多的进程或者处理更大的数据。
  - 内存隔离与保护：每个进程都有自己的虚拟内存空间，相互之间彼此隔离，不会相互干扰。此外，操作系统可以使用页面访问权限位来保护进程的内存，防止程序越界访问或修改非法内存。
  - 提高内存利用率：虚拟内存技术可以根据进程的需要动态加载和卸载页面，根据物理内存的优先级和需求进行管理，减少外部碎片，提高内存的利用率。
  - 简化编程：使用虚拟内存技术，程序员可以将关注点放在逻辑地址空间上，而无需担心物理内存的细节。操作系统会处理映射和页面置换等任务。

## 虚拟地址

- 虚拟地址是在虚拟内存管理下，由每个进程所使用的一种地址空间。它是由操作系统为每个进程分配的一组逻辑地址
- 概述
  - 在计算机系统中，每个进程都有自己独立的虚拟地址空间，进程中的程序可以使用这些虚拟地址来访问内存。虚拟地址是相对于进程而言的，它不直接对应于物理内存中的实际地址，而是经过地址映射转换为物理地址才能访问实际的存储单元。
  - 虚拟地址空间通常以连续的方式划分为多个块，每个块对应着不同的用途，例如代码段、数据段、堆区、栈区等。进程使用虚拟地址来引用这些不同的块，而操作系统负责将虚拟地址转换为对应的物理地址，使进程可以访问实际的内存存储。这种地址转换是通过页表（在分页内存管理中）或段表（在段式内存管理中）等数据结构实现的。
- 优点
  - 内存隔离：每个进程使用自己的虚拟地址空间，使得进程之间的内存彼此分离，不会相互干扰或读取对方的数据。
  - 内存扩展：虚拟地址空间可以远大于实际的物理内存大小，允许进程访问更大的地址空间，避免了实际内存的限制。
  - 虚拟内存管理：虚拟地址的使用使得操作系统可以实现虚拟内存管理，包括页面置换、主存与辅存的交换等功能，提高了内存的利用效率和系统的性能。

## 虚拟内存空间结构

- 当涉及到虚拟地址空间的详细结构时，下面是一种常见的示例：
  - 代码段（Text Segment）：存放可执行程序的机器指令。通常是只读的，因为代码在运行时不应该被修改。
  - 数据段（Data Segment）：存放静态变量和全局变量，包括初始化的和未初始化的数据。这个段通常是可读写的。
  - 堆（Heap）：用于动态分配的内存空间。堆的大小可以根据需要进行动态调整。在C/C++中，通过调用`malloc`、`new`等函数来分配内存。
  - 栈（Stack）：存放函数调用期间产生的局部变量和函数调用的上下文。栈是一块后进先出（LIFO）的内存空间。每次函数调用时，会将该函数的局部变量和返回地址等信息压入栈中，然后在函数返回时再弹出。
  - 共享库（Shared Library）：存放被多个进程共享的代码和数据。共享库可以在物理内存中只有一份，不同的进程共享使用。这种共享节省了内存的使用量，提高了系统的性能和效率。
  - 内核空间（Kernel Space）：为操作系统内核保留的内存区域。用户程序无法直接访问内核空间，只能通过系统调用进入内核空间执行内核提供的服务。     （进程共享，但是进程都不能互相访问）
- 虚拟地址空间的划分使得每个进程都有了自己独立的内存空间，从而提供了进程间的隔离和保护。操作系统通过页表（或段表）来管理虚拟地址空间和物理内存之间的映射关系。页表存储了虚拟内存页与物理内存页之间的对应关系，以及访问权限等相关信息。通过页表，操作系统可以实现虚拟地址到物理地址的转换，并且在需要时进行内存的分配和回收。

## 一个进程的虚拟内存空间能够被其他进程访问吗

- 在现代操作系统中，一个进程的虚拟内存空间通常是被其他进程无法直接访问的。每个进程都有自己独立的虚拟地址空间，其它进程无法直接读取或修改另一个进程的虚拟内存。

  操作系统通过使用内存管理单元（Memory Management Unit，MMU）来控制和映射进程的虚拟地址到物理内存地址。每个进程拥有独立的页表，根据页面表中的映射关系，虚拟地址被转化为相应的物理地址。这种映射仅在进程内部有效。

  但也存在一些特殊的情况或机制，使得进程之间可以共享或访问彼此的虚拟内存空间，例如：

  1. 进程间通信（Inter-Process Communication，IPC）：通过特定的IPC机制，如共享内存、管道、消息队列等，进程可以在一定程度上共享数据，使得它们可以访问彼此的虚拟内存空间。
  2. 跨进程调试：调试器可以在特殊的条件下访问和修改其他进程的虚拟内存，以便进行调试和分析。

  需要注意的是，这些操作通常是通过操作系统提供的特殊接口或机制实现的，且需要相应的权限。在普通情况下，一个进程的虚拟内存空间对于其他进程是不可见的，也无法直接访问或修改。这种隔离的设计有助于保护进程的数据和确保系统的安全性。

- 一个进程拿一个地址能否访问另一个进程相同位置的区域

  - 我的想法是：不一定，比如都用相同的地址来当做共享内存区，那么是可以的。否则就不行了。。

- 有哪些跨进程调试

  - 跨进程调试是指在调试一个进程时，可以访问和操作其他进程的内存、寄存器和线程上下文等信息。下面列出了一些常见的跨进程调试技术：

    1. 操作系统提供的调试接口：操作系统通常为调试器提供了特定的接口，使其能够附加到其他进程并获取调试信息。例如，Windows提供了Debug API，Linux提供了ptrace系统调用。
    2. 调试代理程序：调试代理程序是一种中间层，它允许调试器与目标进程进行通信。调试器通过与调试代理程序进行通信，以间接地访问和控制目标进程。例如，GDB使用GDB服务器作为调试代理。
    3. 动态链接库注入（DLL注入）：通过将一个动态链接库注入到目标进程的地址空间中，调试器可以在目标进程中执行自己的代码，以实现调试功能。
    4. 反调试技术：某些恶意软件或加壳程序会使用反调试技术来防止被调试。反调试技术可能会检测调试器的存在并尝试干扰调试器的操作，如修改寄存器或隐藏关键信息。对于这种情况，调试器需要采取相应的反反调试技术来绕过这些保护措施。

    需要注意的是，跨进程调试涉及到对其他进程的访问和操作，因此需要相应的权限和合法的用途。在正常情况下，操作系统和安全设置通常会限制或阻止未经授权的跨进程访问。跨进程调试的使用需要谨慎，并遵循适用的法律和规定。

## 分页式内存管理

- 虚拟内存技术的一种实现方式
- 一种内存管理技术，将进程的虚拟地址空间划分为固定大小的页面（page），并将物理内存划分为相同大小的页面框（page frame）。通过页表将进程的虚拟页面映射到物理页面框，实现地址的转换和内存的分配
- 流程
  - 页面划分：将进程的虚拟地址空间划分为大小相同的页面（通常为4KB或者2MB大小），每个页面都有一个唯一的页号。如进程的虚拟地址空间为32位，则可以划分为2^32个页面。
  - 物理页面框划分：将物理内存同样划分为相同大小的页面框，与页面大小保持一致。物理页面框的数量与进程虚拟地址空间大小相匹配。
  - 页表：每个进程维护一个页表，页表是一种数据结构，记录了虚拟页面到物理页面框的映射关系。页表的每一个表项对应一个虚拟页面，其中包含了该虚拟页面对应的物理页面框的地址。
  - 地址转换：当进程引用一个虚拟地址时，通过页表进行地址转换。首先，从虚拟地址中提取出页号部分，然后在页表中查找对应的表项。表项中记录了该虚拟页面对应的物理页面框的地址，将其与虚拟地址中的页内偏移相结合，得到真实的物理地址。
  - 页面调度和置换：如果进程所需的页面不在物理内存中（即发生页面错误），操作系统会选择一个页面进行置换，将其从内存中移出，并将新需求的页面加载到内存中。页面调度算法（例如最优（OPT）、最近未使用（LRU）、先进先出（FIFO）等）用于决定选择哪个页面进行置换。
- 优点
  - 内存利用率提高：通过分页，系统可以更灵活地使用内存，将进程的虚拟地址空间映射到物理内存中的任意空闲页面框，从而提高内存的利用率。
  - 共享和保护：通过页表的权限位设置，可以实现对页面的保护，防止非法访问，同时可以实现页面的共享，多个进程可以共享同一物理页面框。
  - 多道程序同时运行：分页内存管理允许多个进程同时运行，每个进程都有自己的页表，彼此独立，实现了进程间的隔离性。
- 总结
  - 分页内存管理通过将进程的虚拟地址空间划分为页面，并将页面映射到物理内存页面框中，实现了地址的转换和内存的分配。它提高了内存的利用率、实现了共享和保护，同时也支持多道程序同时运行。

## 分段式内存管理

- 分段式内存管理是一种内存管理技术，将进程的地址空间按照逻辑分段的方式进行管理，每个段对应着一个逻辑单位，例如代码段、数据段、堆区、栈区等。每个段都有固定的大小，并具有起始地址和长度信息。
- 流程
  - 地址空间划分：进程的地址空间按照不同的逻辑功能进行划分，每个逻辑段对应一个功能模块，如代码段、数据段等。每个段具有独立的起始地址和长度。
  - 段表：操作系统为每个进程维护一个段表，段表是一种数据结构，记录了每个段的起始地址和长度等信息。段表的每一个表项对应一个逻辑段，其中包含了该段的起始地址、长度和权限等属性。
  - 地址转换：当进程引用一个逻辑地址时，操作系统通过访问段表来进行地址转换。进程将逻辑地址划分为段号和段内偏移两部分，通过段号在段表中查找对应的表项，获得该段的起始地址和长度。然后将段的起始地址与段内偏移相结合，得到真实的物理地址。
  - 段保护和共享：通过段表中的权限位，可以对段进行保护，限制对该段的访问。不同段可以有不同的权限设置，可以实现对数据和代码的保护。此外，段表还可以实现段的共享，多个进程可以共享同一逻辑段，减少内存占用和数据复制。
  - 内存分配：在分段式内存管理中，内存的分配是以段为单位进行的。当进程需要申请新的段或释放已有的段时，操作系统将根据需要为进程分配或回收相应的内存空间，更新段表中的信息。
- 优点
  - 灵活性：分段式内存管理可以将进程的地址空间按照逻辑功能进行划分，更加灵活地组织内存空间，使得程序员可以更方便地管理和编写代码。
  - 内存保护：通过段表中的权限位设置，可以实现对不同段的访问权限限制，保护敏感数据和代码，防止非法访问和破坏。
  - 共享机制：不同进程可以共享同一逻辑段，减少内存占用和数据复制，提高系统的效率
- 总结
  - 分段式内存管理通过将进程的地址空间按照逻辑功能进行划分，每个段具有独立的起始地址和长度，通过段表进行地址转换和保护，实现了灵活性、内存保护和共享机制。

## 段页式内存管理

- 段页式管理则是虚拟内存技术的一种实现方式

- 一种将段式管理和页式管理结合起来的内存管理技术。它将进程的虚拟地址空间划分为不同大小的逻辑段，并将每个逻辑段进一步划分为相同大小的页。通过段表和页表的组合，实现了从段到页、从页到物理页面框的多级地址映射。
- 流程
  - 地址空间划分：进程的虚拟地址空间被划分为逻辑段，每个逻辑段对应一个段表项。每个逻辑段的大小可以根据需要而定，如代码段、数据段、堆栈段等。
  - 段表和页表：每个进程维护一个段表，其中的每个段表项指向对应逻辑段的页表。而每个逻辑段也维护自己的页表，用于将逻辑页映射到物理页框。
  - 地址转换：当进程引用一个虚拟地址时，首先通过段表进行地址转换，将虚拟地址的段部分定位到对应的逻辑段。然后再通过页表将虚拟地址的页部分转换为物理地址的页框部分。
  - 多级映射：由于段的大小和页的大小通常不同，所以段表项和页表项之间的映射是多级的。即首先通过段表找到逻辑段的页表，再通过页表找到对应的物理页面框。
  - 页面置换和调度：如果进程引用的虚拟页面不在物理内存中，会发生缺页中断。操作系统根据页面置换算法选择需要置换的页面，并将新需要的页面从辅助存储器（如磁盘）加载到物理内存中。
- 优点
  - 段页式管理结合了段式管理和页式管理的优点
  - 段的优点在于逻辑上的分段，便于代码的模块化和数据的隔离。
  - 页的优点在于页面大小固定，便于内存的分配和页面置换。

- 总结
  - 段页式管理通过使用段表和页表进行地址映射和转换，实现了从逻辑段到页到物理页面框的多级地址映射。这种管理方式既可以提高内存的利用率和地址转换的效率，也保留了段的灵活性和页的简洁性

## 页表分级

- 虚拟地址空间的一个页表过大，一次加载到到内存很大
- 将页表分为多级，只把有需要的页表项加载到内存？
- 参考路由表分级

## 线程同步方式

- 什么是？
  - 线程同步是指在多线程编程中，为了避免对共享资源的并发访问而需要协调和控制线程的执行顺序和访问权限。当多个线程并发地访问共享资源时，如果没有合适的同步机制，可能会导致数据不一致、竞态条件或死锁等问题。
- 目的
  - 线程同步的目的是保护共享资源的一致性和可靠性，确保多个线程可以安全地访问和修改共享数据。同步机制通过提供互斥访问和同步操作，来保证在任意时刻只有一个线程可以访问共享资源，或在满足特定条件时允许线程进行访问。
- 方式
  - 互斥锁（Mutex）：互斥锁是最常见的线程同步机制之一。只有获得锁的线程可以执行关键代码段，其他线程需要等待锁被释放才能执行。互斥锁保证同一时刻只有一个线程可以访问共享资源，从而避免多个线程同时修改数据导致的竞态条件。
  - 读写锁（ReadWrite Lock）：读写锁也是一种线程同步机制，它区分了对共享资源的读取和写入操作。多个线程可以同时持有读锁，以实现并发读取操作，但只有一个线程能够持有写锁，以确保数据的一致性。
  - 条件变量（Condition Variable）：条件变量用于线程之间的通信和协作。线程可以通过条件变量等待某个条件的满足，当条件满足时，其他线程可以通过条件变量发出信号来唤醒等待的线程。
  - 信号量（Semaphore）：信号量是一种用于线程同步的计数器。它可以控制同时访问某个资源的线程数量，通常用于限制并发资源的访问量。
  - 屏障（Barrier）：屏障用于实现线程的同步点，它允许多个线程等待，直到所有线程都到达某个点才能继续执行。
  - 原子操作（Atomic Operation）：原子操作是一种不可分割的操作，能够保证操作的完整性和一致性。原子操作通常用于修改共享资源的情况，避免竞态条件导致的数据不一致。
  - 事件（Event）：事件是一种线程同步机制，用于线程之间的通信和协作。线程可以等待某个事件的发生，当事件发生时，其他线程可以触发事件来通知等待的线程。
  - 互斥量锁（Spinlock）：互斥量锁是一种自旋锁，它使用忙等待的方式进行线程同步。线程尝试获取锁时，如果发现锁已经被其他线程持有，就会一直自旋等待锁的释放，直到成功获取锁才会继续执行。
  - 信号（Signal）：信号是一种异步通信机制，在进程或线程间发送信号来通知事件的发生。线程可以通过处理信号来进行同步操作。
  - 管程（Monitor）：管程是一种高级线程同步机制，提供了一套更为抽象和易用的同步原语。它包含了共享资源、条件变量和同步操作等，通过内部实现来保证线程间的同步和互斥。
  - 读写锁（Reader-Writer Lock）的变种：除了传统的读写锁，还有一些变种，如偏好读写锁（Readers-Writer Fairness Lock）和递归读写锁（Recursive Read-Write Lock），它们在特定场景下提供了更灵活的同步方式。
  - 信号量（Semaphore）的变种：除了二进制信号量和计数信号量，还有一些其他的信号量类型，如读者写者信号量（Reader-Writer Semaphore）和前递信号量（Semaphore with Advance）等，它们在不同场景下提供了更灵活的同步机制。
  - 屏障（Barrier）的变种：除了普通的屏障，还有一些其他的屏障类型，如循环屏障（Cyclic Barrier）和分段屏障（Phase Barrier）。这些屏障在多线程协作中提供了更多的控制和灵活性。
  - 交换（Exchange）：交换是一种简单的同步操作，它允许两个线程交换数据或信号。线程A将自己的数据交给线程B，而线程B则将自己的数据交给线程A，从而实现线程之间的同步。
  - 计数器（Counter）：计数器是一种基本的同步原语，它可以用于控制线程的执行顺序和并发度。线程可以等待计数器的值达到某个特定值，或者通过递减计数器的方式来实现线程同步。
  - 条件变量（Condition Variable）的变种：除了传统的条件变量，还有一些其他的条件变量类型，如优先级条件变量（Priority Condition Variable）和超时条件变量（Timeout Condition Variable），它们在不同的线程协作场景下提供了更多的控制和灵活性。
  - 读拷贝写拷贝（Copy-on-Write）：读拷贝写拷贝是一种优化的同步机制，用于共享数据的读取和写入。当多个线程读取共享数据时，使用共享资源的拷贝，而当其中一个线程需要修改共享数据时，会进行复制以避免冲突，从而实现了读写的并发性。
  - 无锁（Lock-Free）：无锁算法是一种避免使用传统锁机制实现线程同步的方式。它通过使用原子操作、无锁数据结构或其他特定技术来实现线程间的同步，从而提高并发性和性能。
  - 消息传递（Message Passing）：消息传递是一种基于消息的线程通信模型，线程通过发送和接收消息来实现同步和协作。消息传递可以通过队列、邮箱、管道等方式进行，保证线程之间的同步和通信。
  - 扇入扇出（Fan-in/Fan-out）模式：扇入扇出模式是一种通过将任务划分为多个子任务并行执行，然后再合并结果的方式来实现线程同步和并发。通过合理划分工作，可以提高系统的吞吐量和响应性。

## 线程竟态

- 竞态（Race condition）是指在多线程或并发编程中，多个线程同时访问共享资源或共享数据时，由于缺乏合适的同步机制导致的不确定的行为。
- 竞态条件可能会导致数据的不一致性，破坏程序的正确性和可靠性。具体而言，当多个线程在同一时刻对共享资源进行读取、写入或修改时，由于线程的执行顺序和时间片分配等因素的不确定性，可能会导致以下问题：
  - 丢失更新（Lost update）: 多个线程同时对同一数据进行写操作，导致部分线程的更新操作被覆盖，最终导致数据更新不完整或丢失关键信息。
  - 数据竞争（Data race）: 多个线程同时读取和写入共享数据，导致数据的读写顺序不确定，可能会产生未定义的结果。
  - 硬件并发问题（Hardware concurrency problems）: 处理器的乱序执行或多级缓存的读写延迟等因素，可能会导致后续的操作在不正确的数据上进行。

## CPU多级缓存

- 简介
  - CPU缓存是一种硬件级别的缓存。它是由CPU芯片内部的高速存储器组成的，用于存储CPU频繁访问的数据和指令。CPU缓存位于CPU内部，与其他计算机组件（如主存）相比，具有更快的访问速度和更低的延迟。
  - CPU缓存并不直接包含主存（RAM）的内容，而是存储了一部分主存中的数据和指令的副本。CPU缓存是位于CPU内部的一种高速存储器，用于加速CPU对数据和指令的访问
  - 现代的CPU通常包含多级缓存，具体常见的缓存级别包括L1、L2、L3，有时还可能有L4缓存
  - 多级缓存之间存在层次结构，即L1缓存是最接近CPU核心的，所以速度最快；L3缓存是相对较慢但容量较大的，它可以由多个CPU核心共享；而主内存则是最大容量但速度最慢的存储器
- 各级缓存
  - L1缓存（一级缓存）：位于CPU核心内部的L1缓存分为指令缓存（L1i）和数据缓存（L1d）。它们通常之间相对独立地处理指令和数据，缓存的大小较小（通常为几十KB级别），但速度非常快，接近于CPU核心的执行速度。
  - L2缓存（二级缓存）：L2缓存位于CPU核心和主存之间，大小一般为几百KB到几兆字节。它的速度比L1缓存略慢，但相对于主存来说仍然非常快速。
  - L3缓存（三级缓存）：L3缓存是所有CPU核心共享的缓存，位于核心和主存之间。它的大小通常比L2缓存大，速度相对较慢，但仍然比主存要快。
  - L4缓存（四级缓存）：L4缓存是一些处理器架构中的可选级别，存在于L3缓存之上。它的容量通常更大，并提供更高的容错能力
- 原理
  - 缓存块（Cache Line）：缓存将数据分成固定大小的块，每个块称为缓存块或缓存行。当CPU访问特定的数据时，它会将相邻的数据块也加载到缓存中，以提高局部性原理的效果。这样，在访问相邻数据时，CPU可以直接从缓存中获取，而不必再次访问主存。
  - 缓存命中（Cache Hit）和缓存未命中（Cache Miss）：当CPU需要访问数据时，首先会检查缓存中是否存在该数据。如果数据在缓存中（缓存命中），CPU可以直接从缓存中读取，这称为缓存命中。如果数据不在缓存中（缓存未命中），CPU需要从主存中读取数据，并将其加载到缓存中进行处理。
  - 缓存一致性：由于多核CPU中各个核心都有自己的缓存，当一个核心修改了缓存中的数据时，其他核心的缓存中相同数据的副本可能会变得不一致。为了保持缓存的一致性，CPU采用了共享缓存一致性协议（如MESI协议）。该协议通过使用状态标记和通信机制，确保多个核心对同一块数据的访问是一致的。
  - 缓存替换算法：当缓存满了而需要将新数据加载到缓存中时，需要选择替换出去的缓存块。常见的算法有最近最少使用（LRU）、先进先出（FIFO）、最不经常使用（LFU）、随机替换（Random）
    - 各有优缺点
      - LRU算法通常能够较好地利用局部性原则，因为最近被访问的数据有更高的概率在近期内再次被访问。然而，LRU算法需要维护一个较大的访问历史记录，成本较高。
      - FIFO算法操作简单，但它可能无法反映出数据的访问频率，因为最先进入缓存的数据未必是最常被访问的。
      - LFU算法适用于具有长期使用模式的数据集，但如果访问频率突然改变，它可能无法及时适应。
      - 随机替换算法简单快速，但是缺乏对访问模式的适应性。
- 总结
  - CPU多级缓存的设计是为了提高CPU和主内存之间的数据传输速度。当CPU访问数据时，会先检查最近使用的数据是否在缓存中，如果命中缓存，那么CPU可以直接从缓存中获取数据，避免了从主内存中读取数据的延迟。这种缓存的结构可以大大减少内存访问的频率，从而提高CPU的性能。

## 伪缓存

- 简介
  - 伪缓存是一种在主存存储器上实现的软件层次缓存机制。它在计算机系统中使用，旨在提高数据访问的效率，通过利用局部性原理和较小的快速存储器。
  - 伪缓存通常在主存中创建一个数据缓存区域，该区域的大小相对较小，可以根据特定的应用需求和硬件条件进行调整。伪缓存的大小通常比硬件缓存小得多，因此无法存储大量的数据。它使用的是相对较小而快速的存储来保留最常用的数据，以便更快地访问。
  - 伪缓存通常使用行地址的直接映射或者基于散列的映射技术来将数据和主存中的地址进行映射。它将缓存行与主存地址相关联，当通过伪缓存访问数据时，系统会首先检查数据是否在缓存行中。如果在缓存行中，则直接返回数据给CPU；如果不在缓存行中，则需要从主存储器中读取相应的数据，并将其存入缓存行，以便未来的访问。
- 优点
  - 伪缓存的一个主要优点是相对于硬件缓存，伪缓存的实现更为简单且消耗的资源较少。伪缓存不需要专门的硬件支持，而仅需在软件层面进行实现和管理。这使得伪缓存可以灵活地适应不同的系统架构和内存配置。
    - 主存（主内存）是计算机系统中的一种硬件级别的存储器，但是伪缓存可在主存上进行软件级别的实现
- 限制
  - 伪缓存也存在一些限制和挑战。首先，伪缓存的性能受到主存储器的延迟和带宽的影响，因为它是在主存储器上进行访问的。这意味着伪缓存的访问速度可能不如硬件缓存。其次，伪缓存可能会对系统的一致性和并发性产生影响。在使用伪缓存时，需要采取适当的同步和管理策略，以确保数据的一致性和正确

## 同步和异步

- 同步和异步是用来描述操作之间相互关联和执行顺序的概念。

  同步操作是指程序按照定义的顺序依次执行操作，每个操作完成后才能进行下一个操作。在同步操作中，调用者需要等待被调用的操作完成后才能继续执行后续的代码。同步操作通常以阻塞的方式进行。

  异步操作是指程序不按照定义的顺序依次执行操作，而是通过回调、事件或者其他机制来通知操作的完成。在异步操作中，调用者不需要等待被调用的操作完成，而是可以继续执行后续的代码。异步操作通常以非阻塞的方式进行。

  通常情况下，同步操作更容易理解和编写，但可能会造成执行效率较低，因为调用者需要等待操作完成。而异步操作则可以提高执行效率，允许并发执行多个操作，但对程序的编写和理解可能会更加复杂。选择同步还是异步操作要根据具体情况和需求来决定。

## 进程切换和线程切换

- 进程的切换相对于线程来说更加耗费资源的主要原因有几点：
  1. 上下文切换开销：当操作系统在不同的进程之间切换时，需要保存和恢复每个进程的上下文信息，包括寄存器状态、内存映射、打开的文件等。这些上下文切换操作需要花费较多的时间和计算资源。
  2. 内存切换开销：不同的进程拥有独立的虚拟地址空间，切换进程时，操作系统需要切换内存映射，即将新进程的虚拟地址空间加载到物理内存中。这涉及到页表的刷新和TLB（Translation Lookaside Buffer）的切换，会引起内存访问的额外开销。
  3. 外部资源切换开销：进程切换可能涉及对外部资源的重新分配和释放，例如文件描述符、网络连接、打开的设备等。这些资源的切换可能需要额外的系统调用和操作，增加了开销和延迟。
- 相比之下，线程的切换相对较为轻量级：
  1. 上下文切换开销较小：线程之间共享同一进程的上下文信息，所以线程的切换不需要保存和恢复整个进程的上下文，只需要切换线程私有的部分即可，比如寄存器值的保存和恢复。
  2. 内存切换开销较小：线程共享进程的虚拟地址空间，所以不需要进行内存映射的切换。
  3. 外部资源切换开销较小：线程共享进程的外部资源，所以不需要对外部资源进行重新分配和释放。

## 有名管道和共享内存区别

- 有名管道（Named Pipe）和共享内存（Shared Memory）是用于进程间通信的两种不同机制。

- 有名管道

  - 一种半双工（单向）通信方式，通常用于同一台计算机上的进程之间进行通信。它通过文件系统来传输数据，进程可以在文件系统中的一个管道文件中写入数据，而另一个进程则可以从同一管道文件中读取数据。名管道提供了简单的读取和写入操作，但数据传输依赖于文件系统的读写操作。

- 共享内存

  - 一种完全不同的机制，它允许多个进程访问同一块内存区域，从而实现高效的数据共享。共享内存使得进程可以直接读取和写入内存，而无需复制数据或通过中间介质（如文件系统）进行通信。这使得共享内存比名管道更快速和高效。
  
- 总的来说，有名管道适用于需要在不同进程之间进行简单数据交换的场景，而共享内存适用于需要高效共享大量数据的场景。有名管道的优点是易于使用和实现，而共享内存的优点是速度快和灵活性高。选择使用哪种机制取决于具体的应用需求和设计考虑。

- 无名管道也依赖于文件系统的读写

  - 在创建无名管道时，操作系统会返回两个文件描述符，一个用于读取数据，一个用于写入数据。这两个文件描述符可以用于父进程和子进程之间的通信。父进程关闭写入文件描述符，子进程关闭读取文件描述符，这样就能在两个进程之间进行数据的传输。

    尽管无名管道没有名字和对应的文件系统文件，但是它依然通过文件描述符来实现进程之间的通信，因此仍然会依赖文件系统。不同的是，无名管道并不需要显式地创建一个文件，操作系统会为其分配一个特殊的文件描述符来进行数据传输。

## 什么是进程

- 进程是计算机中正在执行的程序的实例。它是操作系统分配资源和执行任务的基本单位。

  一个进程包含了程序代码、数据和执行环境。每个进程都有自己独立的内存空间和系统资源，可以独立运行。进程可以是应用程序、服务或系统任务。

  进程由操作系统进行创建、启动、暂停、恢复和终止。操作系统通过给每个进程分配资源，如内存、CPU 时间和文件句柄，来确保它们能够正常运行。

  进程之间相互独立，彼此不可见也不可干扰。它们通过进程间通信（IPC）来进行必要的数据交换和协调工作。

  每个进程都有一个唯一的数字标识符（PID），用于在操作系统中进行识别和管理。

  进程的状态包括运行、就绪、阻塞等，这取决于它们是否能够获取所需的资源以及是否有其他进程在占用资源。

  总之，进程是计算机中正在运行的程序的实例，它提供了多任务处理和资源管理的能力，使得系统能够同时执行多个任务。

## 什么是线程

- 线程是进程中的一个执行单元，是操作系统调度的最小单位。一个进程可以包含多个线程，它们共享进程的地址空间和系统资源。

  与进程不同，线程没有自己独立的内存空间，它们共享相同的进程内存。这意味着在同一个进程中的线程可以访问相同的数据和变量。线程之间的通信和数据共享更加方便快捷。

  线程的创建、启动、暂停和恢复工作由操作系统负责。与进程不同，线程的创建和切换开销相对较小，因为它们共享进程的资源。

  线程可以分为多个执行路径，每个线程都可以独立运行不同的代码块。这使得多线程编程能够实现并发性和并行性，提高程序的执行效率和响应性。

  线程可以有自己的状态，如运行、就绪、阻塞等，取决于是否能够获取所需的资源和是否有其他线程占用资源。

  线程之间可以通过线程间通信（Thread Communication）来进行必要的数据交换和协调工作。常见的线程间通信方式包括共享内存、消息传递、信号量等。

  需要注意的是，多线程编程也带来了一些挑战，如同步问题（如竞态条件、死锁）、资源竞争等。因此，在编写多线程程序时，需要谨慎处理线程间的同步和共享资源。

  总之，线程是进程中执行的最小单位，它们共享进程的内存和资源。多线程编程可以实现并发性和并行性，提高程序的性能和响应能力。

## 什么是协程

- 当我们在Java中创建线程时，通常会依赖操作系统的线程实现。每个线程都需要在操作系统中分配资源，包括内存和处理器时间。这意味着我们需要为每个线程维护一个独立的堆栈和上下文信息。这在大规模并发应用程序中可能会导致内存消耗和上下文切换开销的问题。

  虚拟线程的概念就是为了解决这些问题而引入的。它是一种用户级的轻量级线程模型，完全由Java代码实现。在虚拟线程模型中，多个虚拟线程可以在一个操作系统线程上运行，共享同一个堆栈和上下文信息，减少了对操作系统线程的依赖和资源消耗。

  虚拟线程采用的是事件驱动的方式，使用协作式调度。在传统的线程模型中，调度器会抢占式地切换线程的执行，但在虚拟线程中，线程的切换由应用程序显式地控制。当一个虚拟线程执行到一个阻塞操作时，它会主动释放资源，并将控制权还给调度器，然后可以切换到其他虚拟线程继续执行。这种协作式的调度方式可以减少上下文切换的开销，并提高应用程序的并发性能。

  虚拟线程的引入将使得开发者可以更加方便和高效地进行并发编程。相比传统的线程模型，虚拟线程具有更低的内存消耗、更快的启动和销毁速度，以及更好的资源利用率。它将使得开发者能够更好地应对高并发场景下的挑战，并提高应用程序的性能和可扩展性。

## 协程和虚拟线程的区别



## 进程和线程的区别

- 进程和线程是计算机中执行任务的两种不同方式，它们有以下几个主要区别：

  1. 资源分配和管理方式：
     - 进程：进程是操作系统分配资源和执行任务的基本单位。每个进程都有独立的内存空间和系统资源，如文件句柄、网络连接等。进程之间相互独立，彼此不可见也不可干扰。
     - 线程：线程是进程中的执行单元，共享相同的进程内存和资源。线程之间可以访问相同的数据和变量，通过共享内存进行通信和同步。

  2. 创建和切换开销：
     - 进程：进程的创建和切换开销较大，因为需要为每个进程分配独立的资源和环境。
     - 线程：线程的创建和切换开销相对较小，因为它们共享进程的资源和环境。

  3. 并发性和并行性：
     - 进程：多个进程可以同时执行，实现并发性。每个进程在操作系统中独立运行，有自己的执行顺序和资源。
     - 线程：多个线程可以在同一进程中并发执行，实现并行性。线程共享进程的资源，可以同时执行不同的代码块。

  4. 可靠性和稳定性：
     - 进程：由于进程间相互独立，一个进程的崩溃或错误不会影响其他进程的正常运行。因此，进程具有较高的可靠性和稳定性。
     - 线程：线程是进程内的执行单元，一个线程的错误可能会影响整个进程的稳定性。线程之间的错误和资源竞争问题需要谨慎处理。

  5. 编程与调试复杂度：
     - 进程：编写和调试进程间通信（IPC）的程序相对复杂，因为进程之间的通信需要使用特定的机制和API。
     - 线程：编写和调试多线程程序相对复杂，因为多个线程共享数据和资源，需要正确处理线程间的同步、互斥和共享资源的问题。

  总结起来，进程和线程在资源分配、创建开销、并发性、可靠性和编程复杂度等方面有所差异。正确选择和使用进程和线程可以根据具体的需求和场景来提高系统的性能和响应能力。

## 事件驱动模型

- 操作系统事件驱动的本质是基于异步的事件处理模型。传统的操作系统执行任务是按照顺序依次执行的，而事件驱动模型则是根据事件的发生和到达顺序来触发相应的操作，从而实现并发执行。

  在事件驱动的模型中，操作系统或应用程序通过事件监听器（event listener）和回调函数（callback function）注册对特定事件的监听。当某个事件发生时，操作系统会将该事件放入一个事件队列（event queue）中，并通过事件循环（event loop）来不断检查是否有待处理的事件。

  事件循环是事件驱动模型的核心概念。它负责从事件队列中取出事件，并将事件传递给相应的事件处理程序或回调函数。处理事件的过程是非阻塞的，即操作系统或应用程序可以同时处理多个事件，并在不同的事件之间切换执行。这样可以提高系统的响应速度和并发性。

  事件驱动模型的优点在于它能够充分利用计算资源，并提供高度灵活和可扩展的系统架构。操作系统可以根据需要动态注册和注销事件监听器，以响应不同类型的事件。同时，事件驱动模型也提供了良好的用户体验，用户操作的响应速度更快，同时系统也能够处理一些异步的任务，如网络请求、文件读写等。

  总而言之，操作系统事件驱动的本质是通过异步的事件处理模型，根据事件的发生和到达顺序来触发相应的操作，以实现并发执行和优化系统性能。

- 监听器通常是在一个单独的线程中执行的。这种线程通常被称为事件处理线程（event handling thread）。事件处理线程负责监听事件队列，并从中提取事件并调用相应的事件处理程序或回调函数。

  将监听器放在单独的线程中执行的好处是可以实现异步事件处理，同时避免阻塞主线程或其他任务的执行。当事件发生时，它首先被添加到事件队列中，然后事件处理线程从队列中取出事件并进行处理，这样系统可以立即返回到主线程继续执行其他任务。

  - 采用单独的线程执行事件监听器可以提高系统的响应速度和并发性。**由于事件处理线程和其他线程是独立的，所以它们可以并行执行，不会相互干扰或阻塞。这对于处理大量事件、高并发请求或需要长时间执行的事件处理非常有用。**

  需要注意的是，多线程编程涉及到资源共享和线程安全等问题，因此在设计和实现事件处理线程时需要考虑同步机制和线程安全性。正确地管理事件处理线程可以确保事件驱动模型的正确运行，并最大程度地发挥其优势。

- 过程

  - 注册监听   （事件、回调函数、监听器之间的绑定关系）
  - 事件存到时间队列
  - 监听器轮询队列，将事件发送给事件处理线程
    - 由于事件处理是异步的，故事件处理线程一般不是事件发生的线程
  - 事件处理线程根据回调函数中的内容进行处理

# ==linux==

## 软链接和硬链接

- 软链接（Symbolic Link 快捷方式

  - 是一种特殊类型的文件，它是一个指向其他文件或目录的快捷方式。软链接与硬链接不同，它不是直接指向文件的物理数据块，而是包含了目标文件的路径信息。

  - 创建软链接

    ```shell
    ln -s <目标文件或目录> <链接文件或目录>
    ```

  - 特点

    - 软链接可以链接文件和目录。
    - 它可以跨越不同的文件系统。
    - 如果原始文件或目录被删除或移动，软链接将失效。
    - 软链接相对于目标文件或目录来说，占用较少的磁盘空间

  - 优点

    - 文件或目录跨文件系统：软链接可以链接不同文件系统上的文件或目录。这意味着可以使用软链接将文件或目录链接起来，轻松实现跨分区或跨硬盘的文件访问和管理。
    - 灵活的文件重定向：通过创建软链接，可以将文件或目录重定向到另一个位置，而不需要复制或移动原始文件。这样做可以提供更灵活的文件访问和组织方式，而无需影响原始文件的实际位置。
    - 方便的文件共享：软链接可以指向任何文件或目录，使得多个用户或多个应用程序可以共享相同的文件资源。这种文件共享机制可以简化文件的共享和管理，同时确保所有链接都指向同一个目标。
    - 符号链接的修改和删除：与硬链接不同，修改或删除软链接不会影响目标文件或目录本身。软链接只是一个指向目标的路径信息，当目标文件或目录删除或更改位置时，软链接仍然存在但无效。这使得修改或删除软链接更加灵活和安全。
    - 软链接的跟踪和管理：通过检查软链接文件的属性，可以快速了解目标文件或目录的位置和状态。软链接可被用于构建复杂的文件系统结构、软件包管理和链接管理等任务。

- 硬链接（Hard Link 文件别名

  - 是一种特殊类型的链接，用于将一个文件与另一个文件节点链接在一起。与软链接不同，硬链接直接指向相同的物理数据块，多个硬链接与原始文件没有区别

  - 使用硬链接可以创建多个具有相同内容的文件，并使它们共享磁盘空间和inode。删除其中一个硬链接不会影响其他链接或原始文件的可用性。

  - 创建硬链接

    ```shell
    ln <原文件> <链接文件>
    ```

  - 特点

    - 硬链接只能链接文件，不能链接目录。
    - 不同的硬链接没有主次之分，它们与原始文件是完全相同的。
    - 硬链接与原始文件共享相同的inode和数据块，因此它们在文件系统中占用相同的空间。
    - 如果原始文件或任何一个硬链接被删除，其他链接仍然有效，因为它们都指向相同的数据块。
    - 通过硬链接能够直接改动原始文件，其他硬链接也能够同步

  - 优点

    - 共享文件内容：硬链接使得多个文件可以共享相同的物理文件内容。这意味着，即使有多个链接指向同一文件，它们实际上都是指向同一个物理数据块。这种共享机制可以节省磁盘空间，因为不同的链接不会占用额外的存储空间。
    - 文件备份：通过创建硬链接，可以在不复制整个文件的情况下创建文件的备份。当需要备份文件时，只需创建一个新的硬链接指向源文件，即可实现备份。因为硬链接与原始文件共享物理块，所以对于备份文件的修改也会反映在原始文件上。
    - 方便的文件管理：硬链接可以在不同的位置和目录中创建文件的别名。这使得文件系统中的文件组织更加灵活，可以通过不同的链接名称访问相同的文件内容。通过硬链接，我们可以创建文件的多个名称，以更直观和易于理解的方式组织和访问文件。
    - 链接的保留：即使删除了原始文件或其他链接，只要至少还存在一个硬链接，文件的内容仍然保留在文件系统中。这使得我们可以在不影响其他链接的情况下删除某个链接，而不会导致文件的永久删除。

## 软、硬连接的意义

- 软链接（Symbolic Link）和硬链接（Hard Link）都是用于创建文件或目录的链接，它们的意义和作用有所不同。

  软链接是创建一个指向目标文件或目录的特殊文件。软链接是一个特殊的文件，它包含了目标文件或目录的路径信息。当访问软链接时，系统会根据链接文件的路径找到目标文件或目录。软链接的特点是可以跨越文件系统和不同的存储设备，并且可以链接目录。

  软链接的主要意义有：

  1. 提供便捷的访问方式：软链接允许你在不同的目录之间创建链接，从而方便地访问目标文件或目录。

  2. 简化文件重命名和移动操作：如果你创建了一个软链接，当软链接文件移动或重命名时，链接仍然有效。这样就可以避免一些操作中断或出错的问题。

  3. 节约存储空间：软链接只是一个指向目标文件或目录的路径，它的大小很小。因此，通过软链接可以节约存储空间。

  硬链接是创建一个指向目标文件或目录的新的目录项（Inode）。硬链接实际上是使用了目标文件的 Inode 来创建新的链接，这意味着同一个 Inode 可以有多个链接指向它。硬链接的特点是不能跨越文件系统，也不能链接目录。

  硬链接的主要意义有：

  1. 共享文件内容：多个硬链接指向同一个 Inode，它们实际上是同一个文件，共享文件的内容。因此，修改其中一个硬链接，其他硬链接也会受到影响。

  2. 确保文件不被删除：如果一个文件有多个硬链接，只有当所有硬链接都被删除后，文件的内容才会真正被删除，从而提供了一种保护文件的机制。

  总结起来，软链接的意义在于提供便捷的访问方式和简化文件操作，而硬链接的意义在于共享文件内容和提供文件保护机制。根据具体的需求，可以选择适合的链接方式。

- 不同维度上的优化

## 权限管理

- 在Linux系统中，权限是用于控制对文件和目录的访问操作的机制。每个文件和目录都有相应的权限设置，其中包括三个主要的权限类别：用户、组和其他。
- 权限拥有者
  - 用户权限（User）：用户权限指定了文件或目录的所有者对其所拥有的文件的操作权限。有三种常见的用户权限
  - 组权限（Group）：组权限指定了文件或目录所属组的成员对其所属的文件的操作权限。它们与用户权限类似，可以设置为读、写和执行。通过将用户分组，可以更好地管理文件和目录的权限
  - 其他权限（Other）：其他权限指定了系统中的所有其他用户对文件或目录的访问权限。同样，其他权限也包括读、写和执行。
- 权限分类
  - 4-r
    - 读（read）：允许用户查看和读取文件内容。   
  - 2-w
    - 写（write）：允许用户修改或删除文件，以及创建新的文件或目录。
  - 1-x
    - 执行（execute）：对于文件，允许用户执行它作为可执行程序；对于目录，允许用户访问目录中的内容、进入目录、搜索目录或执行目录中的可执行程序
  - 0
    - 没有权限

- 使用

  ```shell
  chmod: 该命令用于修改文件或目录的权限。
  格式：chmod [权限设置] 文件或目录名称
  示例：chmod 755 filename、chmod 644 filename
  ```

- 例子分析

  - -rw-r--r-x
    - 第一个字符表示文件类型。`-` 表示这是一个普通文件，如果是目录，则显示为 `d`
    - 接下来的三个字符表示所有者的权限。`rw-` 表示所有者拥有读和写权限，但没有执行权限
    - 接下来的三个字符表示组的权限。`r--` 表示组成员只有读权限，没有写和执行权限。
    - 最后的三个字符表示其他用户的权限。`r-x` 表示其他用户具有读和执行权限，但没有写权限
    - 可用chmod 654 filename
  - drw-r--r-x
    - 目录

## netstat

- （network statistics）命令用于显示网络相关的统计信息，包括网络连接的状态、网络接口的信息、路由表等。它可以用于诊断网络问题、监视网络连接和查看网络资源的使用情况。

  ```shell
  netstat [选项]
  -a：显示所有的网络连接，包括监听和非监听状态的连接。
  -t：仅显示 TCP 连接。
  -u：仅显示 UDP 连接。
  -n：以数字形式显示地址和端口。
  -p：显示与网络连接关联的程序或进程的信息。
  -r：显示路由表。
  -s：显示网络统计信息，如接收和发送的数据包数量。
  -l：仅显示监听状态的网络连接。
  -i：显示网络接口信息。
  ```

## tractroute

- 用于跟踪网络数据包从源主机到目标主机之间的路径。它可用于诊断网络连接问题，确定数据包在网络中经过的路由器和节点。

- 该命令会发送一系列的网络数据包，并在数据包经过每个网络节点时记录下来。它显示每个节点的 IP 地址、节点的名称（如果可用）、往返时间（RTT）以及数据包丢失的情况

  ```shell
  traceroute [目标主机]
  
  traceroute google.com    #不要带如协议http://等信息
  
  traceroute to google.com (172.217.19.238), 30 hops max, 60 byte packets
   1  192.168.1.1 (192.168.1.1)  1.234 ms  1.123 ms  1.456 ms
   2  10.0.0.1 (10.0.0.1)  2.345 ms  1.987 ms  2.567 ms
   3  67.154.52.1 (67.154.52.1)  3.456 ms  3.567 ms  3.123 ms
   4  209.85.252.129 (209.85.252.129)  4.789 ms  4.678 ms  4.567 ms
   5  209.85.143.236 (209.85.143.236)  5.678 ms  5.789 ms  5.876 ms
   6  142.251.49.153 (142.251.49.153)  6.987 ms  6.876 ms  6.765 ms
   7  108.170.246.161 (108.170.246.161)  7.654 ms  7.543 ms  7.432 ms
   8  172.217.19.238 (172.217.19.238)  8.987 ms  8.876 ms  8.765 ms
  
  ```

## 查看磁盘io

- `iostat`: 

  - `iostat` 命令用于显示系统的磁盘实时 I/O 统计信息，包括每个磁盘的读写速率、平均等待时间和传输速率等。可以使用 `-x` 选项来获取更详细的信息。示例命令：

  ```shell
  iostat -x
  
  Linux 5.4.0-74-generic (hostname)  07/08/23  CPU  %usr  %nice  %sys  %iowait  %irq  %soft  %steal  %guest  %gnice  %idle   Device:  rrqm/s  wrqm/s    r/s    w/s   rkB/s   wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
    sda        0.18   0.32   0.71    0.56    19.36      5.85      47.26     0.00     58.25     15.69     105.56     3.93   0.46
    sdb        0.27   0.40   1.12    0.71    23.41      7.21      68.78     0.00     57.35     38.09     84.21      2.94   0.63
    
    
  输出的各列含义如下：
  
  %usr: 用户级别的 CPU 使用率
  
  %nice: 以“优先级”模式运行的进程的 CPU 使用率
  
  %sys: 系统级别的 CPU 使用率
  
  %iowait: 等待 I/O 的 CPU 使用率
  
  %irq: 在硬中断中使用的 CPU 时间百分比
  
  %soft: 在软中断中使用的 CPU 时间百分比
  
  %steal: 虚拟化环境中，由于运行其他虚拟机而阻塞的 CPU 时间百分比
  
  %guest: 运行虚拟机运行所使用的 CPU 时间百分比
  
  %gnice: 以“优先级”模式运行的虚拟机进程的 CPU 使用率
  
  %idle: 空闲 CPU 时间百分比
  
  Device: 每个磁盘设备的名称
  
  rrqm/s: 每秒钟的进行的读取请求合并次数
  
  wrqm/s: 每秒钟的进行的写入请求合并次数
  
  r/s: 每秒钟的读取次数
  
  w/s: 每秒钟的写入次数
  
  rkB/s: 每秒钟的读取数据量（以 kB 为单位）
  
  wkB/s: 每秒钟的写入数据量（以 kB 为单位）
  
  avgrq-sz: 平均请求队列长度（扇区数）
  
  avgqu-sz: 平均请求队列长度（请求数）
  
  await: 平均 I/O 请求等待时间（毫秒）
  
  r_await: 平均读取请求等待时间（毫秒）
  
  w_await: 平均写入请求等待时间（毫秒）
  
  svctm: 平均 I/O 请求服务时间（毫秒）
  
  %util: 被使用的设备容量百分比
  ```

- `iotop`: 

  - `iotop` 命令用于实时监视系统的磁盘 I/O。它会按照 I/O 的大小进行排序，并显示每个进程的磁盘活动情况。可以使用 `-a` 选项来显示所有进程的详细信息。示例命令：

  ```shell
  iotop -a
  ```

- `atop`: 

  - `atop` 是一个功能强大的性能监控工具，它可以提供关于磁盘 I/O、CPU 使用率、内存使用量等方面的详细信息。通过在 `atop` 命令中按 `d` 键来查看磁盘 I/O 统计信息。示例命令：

  ```shell
  atop
  ```

- `dstat`: 

  - `dstat` 是一个全能的系统资源统计工具，可以监视磁盘 I/O、CPU 使用率、内存使用量、网络流量等各种系统资源。可以使用 `--disk-util` 选项来显示磁盘 I/O 的统计信息。示例命令：

  ```shell
  dstat --disk-util
  ```

- `vmstat`: `vmstat` 命令用于报告系统的虚拟内存、进程、CPU 和块设备（包括磁盘）的统计信息。可以使用 `-d` 选项来显示磁盘 I/O 信息。示例命令：

  ```shell
  vmstat -d
  ```

## 内存问题排查

- 在Linux操作系统下，有许多常用的工具可用于处理和解决内存相关问题。以下是其中一些常见的工具
  - top：top 是一个综合性的系统监视工具，可以显示系统的整体状况，包括内存的使用情况、CPU 使用率、进程信息等。
  - free：free 命令可以显示系统内存的使用情况，包括总内存、已使用内存、空闲内存、缓冲区和缓存的使用情况等。
  - vmstat：vmstat 命令可以提供关于系统虚拟内存使用情况的统计数据，包括内存、CPU、磁盘和进程等方面的信息。
  - ps：ps 命令用于查看系统中正在运行的进程的信息，包括进程的内存使用情况、CPU 使用情况等。
  - pmap：pmap 命令可以提供进程的内存映射信息，包括进程使用的内存区域、共享库、堆栈等的大小和位置。
  - top、htop：top 和 htop 是交互式的系统监视工具，可以实时显示系统资源的使用情况，包括内存的使用情况和进程信息。
  - lsof：lsof 命令用于列出当前系统中打开的文件和进程的相关信息，包括文件打开使用的内存等。
  - strace：strace 命令可以跟踪进程的系统调用，可以帮助定位和分析在执行期间进行的系统操作，包括内存的分配和释放。
  - perf：perf 工具是一个强大的性能分析工具，可以用于收集系统性能数据，包括内存使用、CPU 使用、函数调用等。

# ==mysql==

## innoDB 行级锁的分类

- 行级锁的分类主要是为了增强并发性能和保证数据的一致性。通过使用不同类型的行级锁，InnoDB 能够提供高并发读写操作的能力，并确保数据的正确性和完整性
- 分类
  - 记录锁（Record Lock）：也称为行锁（Row Lock），它是针对数据表中的单个记录进行的锁定。当一个事务锁定了某一行的记录时，其他事务将无法修改或删除该行。
  - 间隙锁（Gap Lock）：它是针对数据表中两个记录之间的间隙进行的锁定。间隙指的是相邻记录之间的空隙，不包括当前记录。间隙锁的作用是防止其他事务在间隙中插入新的记录，从而保护了范围查询的完整性。
  - Next-Key 锁（Next-Key Lock）：它是记录锁和间隙锁的组合形式，用于保护范围查询操作的完整性。Next-Key 锁会锁定当前记录和其对应的间隙，以防止其他事务在范围内插入或修改数据。

## innodb共享锁和排它锁

- InnoDB 会根据需要自动添加适当的锁来实现并发控制，共享锁和排它锁的触发情况取决于事务对数据的读取或写入操作
- 如果需要显式地使用共享锁或排它锁，可以使用 `LOCK IN SHARE MODE`（获取共享锁）或 `FOR UPDATE`（获取排它锁）语句来指定锁模式
- 共享锁（Shared Lock）：
  - 当一个事务正在读取某个数据时，会为该数据加上共享锁。
  - 多个事务可以同时获取同一份数据的共享锁，允许并发地进行读取操作。
  - 共享锁之间是兼容的，不会相互阻塞。
  - 共享锁阻塞了其他事务的排它锁，但不阻塞其他事务的共享锁。
- 排它锁（Exclusive Lock）：
  - 当一个事务正在修改某个数据时，会为该数据加上排它锁。
  - 排它锁保证了数据的一致性，防止其他事务同时对同一份数据进行写入操作。
  - 排它锁之间、共享锁之间以及排它锁与共享锁之间都是互斥的，即它们会相互阻塞。
  - 一个事务持有排它锁时，其他事务无法获取该数据的共享锁或排它锁，直到持有排它锁的事务释放锁。

- select
  - SELECT 查询默认情况下不会自动加锁，而是会在默认的隔离级别下（REPEATABLE READ）加上共享锁
  - 加锁的是隔离级别
  - READ COMMITTED 和 REPEATABLE READ 隔离级别都是共享锁，SERIALIZABLE 隔离级别会持有锁范围更长的意向共享锁。

## join reorder

- Join Reorder（连接重新排序）是数据库查询优化的一项重要技术，它通过重新调整关系型数据库中多个表之间的连接顺序，以提高查询性能
- 常见设计思路
  - 收集统计信息：首先需要收集关于表的统计信息，包括表的大小、索引统计信息、选择性等。这些统计信息可以帮助评估连接操作的成本和选择最佳的连接顺序。
  - 枚举所有可能的连接顺序：通过枚举所有可能的连接顺序，可以遍历的方式产生不同的连接树。这可以通过使用递归算法、动态规划或图遍历等方法来实现。根据实际情况，可以限制搜索空间的大小以减少枚举的数量。
  - 估计连接成本：对于每个连接顺序，需要估计连接操作的成本。成本的评估可以根据统计信息和一些规则进行，包括扫描的数据量、索引的使用、连接的复杂度等。成本估计可以作为一个数值指标，用于比较不同连接顺序的性能。
  - 选择最优连接顺序：通过在可能的连接顺序中比较成本指标，可以选择成本最低的连接顺序作为最优解。这可以采用贪心算法、动态规划或其他优化方法来实现，并确保获得最佳的连接顺序。
- 总结
  - 需要注意的是，连接重新排序是一个复杂的优化问题，通常采用启发式算法或基于成本模型的优化方法。设计一个高效的Join Reorder算法需要综合考虑表的大小、索引的选择性、连接操作的成本等多个因素。实际实现中，还可以考虑并发查询、缓存机制等因素来进一步提高查询性能

## innodb的sql执行过程

- 执行过程中可能还涉及到缓存的使用、磁盘读写、日志记录和刷新等操作。每个步骤的具体细节和实现方式还会受到配置参数、硬件性能和数据库设计等因素的影响
  - 查询解析和语法分析（Parsing）：将SQL语句解析为内部数据结构，包括解析关键字、语法验证和创建解析树等操作。
  - 查询优化和执行计划生成（Query Optimization and Execution Plan Generation）：根据查询解析的结果，优化器会选择合适的执行计划。优化器会考虑到查询的表、索引、连接条件等因素，以确定最佳的索引使用、连接顺序和访问方法。
  - 锁定和并发控制（Locking and Concurrency Control）：根据事务隔离级别和锁定需求，InnoDB会对相关数据进行加锁。它使用多版本并发控制（MVCC）来提供高并发性和隔离级别。当需要读取或修改数据时，InnoDB会根据锁定级别获取合适的锁，以确保事务的一致性和隔离性。
  - 执行计划处理和索引查找（Execution Plan Processing and Index Lookup）：根据生成的执行计划，InnoDB会按照顺序执行相应的操作。对于涉及到数据行的查询，InnoDB会使用索引来定位满足条件的数据行。它通过B+树索引结构和自适应哈希索引来提高查询性能。
  - 缓存和缓冲池的使用（Cache and Buffer Pool Utilization）：InnoDB使用了多种缓存来提高性能。其中，缓冲池（Buffer Pool）是InnoDB最主要的数据缓存，用于存储数据页和索引页。缓冲池可以减少磁盘读写操作，提高查询效率。
  - 事务处理和日志记录（Transaction Handling and Logging）：如果存在事务操作，InnoDB将确保事务的一致性和持久性。它会将更新操作记录到事务日志（Transaction Log），并定期进行刷新（Flush）。事务的提交或回滚操作将根据日志完成。
  - 结果返回和连接关闭（Result Return and Connection Closure）：一旦查询执行完成，MySQL会将结果返回给客户端。同时，InnoDB会处理连接的关闭和相关资源的释放。

## rbo和cbo

- 数据库查询优化器中的策略
- rbo
  - RBO指的是"Rule-Based Optimization"（基于规则的优化）。在RBO中，优化器根据预定义的规则和启发式算法来选择最佳的执行计划。
  - 在RBO中，优化器通过使用硬编码的规则来分析和评估查询语句，以选择最佳的执行计划。这些规则通常基于统计信息、索引选择性、表大小等因素。一旦查询被解析和语法分析后，优化器会根据规则检查查询的结构、连接顺序、表的访问方法等，并选择相应的执行计划。
  - RBO的优点是简单且可预测，执行计划的选择由预定义的规则控制，对于简单查询和小规模数据库来说通常效果良好。
  - 然而，RBO也存在一些局限性和缺点：
    - 难以适应复杂查询：当查询涉及多个表、多个连接和多个条件时，RBO的规则可能无法全面考虑和评估所有可能的执行计划，导致性能下降。
    - 依赖固定的统计信息：RBO需要准确的统计信息用于计算成本和选择执行计划。如果统计信息过期或不准确，RBO的决策可能不准确。
    - 不灵活：由于RBO主要依赖静态规则，它难以适应动态变化和优化需求。随着数据库和查询复杂性的增加，RBO的性能可能逐渐变差。
  - 在现代数据库管理系统（DBMS）中，通常使用基于成本的优化器（例如使用代价估算模型的CBO）取代RBO。
- cbo
  - CBO指的是"Cost-Based Optimization"（基于成本的优化）。在CBO中，优化器根据代价估算模型和统计信息来选择最佳的执行计划。
  - 在CBO中，优化器通过对查询语句进行解析和语法分析后，收集相应的统计信息，如表的大小、索引选择性、数据分布等。这些统计信息用于计算执行计划中各个操作的成本，以及整体执行计划的成本估算。
  - 基于成本的优化器会遍历可能的执行计划空间，并为每个执行计划计算成本。然后，它选择成本最低的执行计划作为最佳解决方案。成本的计算可以基于多种因素，如磁盘IO代价、CPU消耗、内存使用等。
  - CBO的优点是能够更全面地分析和评估查询，考虑多种操作的成本和影响因素。相较于基于规则的优化（RBO），CBO能够更准确地估计执行计划的性能，并根据实际情况动态调整执行策略。
  - CBO也存在一些挑战和限制：
    - 统计信息的准确性：CBO依赖准确的统计信息来做出优化决策。如果统计信息过期、不准确或不完整，CBO的执行计划选择可能会出现误差。
    - 代价估算的复杂性：成本估算涉及多种因素和计算，包括IO代价、CPU消耗、网络延迟等。准确地建立和调整这些成本模型需要较高的技术和资源。
    - 查询优化时间：CBO的优化过程需要消耗一定的时间和计算资源。对于复杂的查询和大规模数据库，在优化过程中可能会导致较长的优化时间。
  - 尽管CBO在大多数情况下能够提供更好的查询性能，但在某些特定的场景下，RBO或其他优化策略可能仍然有优势

## b+tree的缺点

- 尽管B+树是一种常用的数据结构，用于索引和组织数据，但它也有一些缺点和限制
  - 高度的维护成本：B+树的维护需要频繁的插入、删除和更新操作。对于频繁更新的工作负载，需要重新平衡和调整树的结构，这可能导致额外的开销和性能下降。
  - 内存占用较高：B+树需要占用较多的内存空间来存储索引。对于较大的数据集和索引，需要更多的内存资源才能存储整棵树，这可能对内存限制较小的系统造成压力。
  - 对于随机访问和范围查询影响较大：B+树主要适用于范围查询和有序访问。在随机插入和随机查找的场景中，B+树的性能可能相对较差，因为需要通过多次磁盘读取来找到目标节点。（没有直接指向底层双向链表的引用
  - 不适合高并发写入场景：对于具有高并发写入的场景，B+树的性能可能受到限制。频繁的插入和删除操作可能导致数据页的分裂和合并操作，进而降低并发性能。（底层是数据页的链表，要保证有序性

## 索引下推

- 索引下推（Index Pushdown）是数据库查询优化技术中的一种策略，用于优化查询的性能。它基于索引的选择性和查询条件，将部分查询操作下推到索引层级，减少磁盘I/O和数据传输的开销
- 传统的查询执行流程中，首先会根据查询条件在索引层级上定位到相应的数据记录，然后再从存储层级获取对应的数据页进行进一步的操作和筛选。这种方式需要进行多次I/O操作，可能导致性能下降。
- 而索引下推的优化技术可以将一部分查询条件下推到索引层级，使得索引层级可以更早地过滤数据，并只返回满足条件的数据。这样可以减少需要检索的数据量和磁盘I/O操作的次数，提高查询性能。
- 索引下推常见的操作包括：
  - 谓词下推（Predicate Pushdown）：根据查询条件，将部分谓词下推至索引层级，减少从数据存储层级获取的数据量。例如，根据查询条件的范围条件，只从索引中选择满足条件的数据页进行读取，而不是整个表或索引的所有数据页。
  - 投影下推（Projection Pushdown）：将部分投影操作下推至索引层级，只返回查询所需的列，避免获取不必要的数据列。例如，在索引层级上只读取索引列而不是整个行的所有列。

## 谓词和投影

- 谓词（Predicate）

  - 谓词是用于筛选数据的条件，用于限制查询结果集中所返回的记录。谓词可以包括各种比较操作符（如等于、大于、小于等）、逻辑运算符（如AND、OR、NOT等）、范围操作符等。通过应用谓词，可以根据特定的条件筛选出满足条件的数据记录。

  - 例如，对于以下查询语句：
    SELECT * FROM Customers WHERE Age > 25 AND Country = ‘USA’;

    在这个查询中，谓词是Age > 25 AND Country = ‘USA’。谓词指定了两个条件，即年龄大于25并且国家是美国，用于筛选出满足这些条件的客户记录。

- 投影（Projection）

  - 投影是查询语句中指定要返回的列的过程。它用于指定查询结果中所包含的列，并排除不需要的列。通过投影，可以选择性地获取查询结果中感兴趣的列数据

  - 例如，对于以下查询语句：
    SELECT CustomerName, Email FROM Customers;

    在这个查询中，投影是CustomerName, Email。它指定了只返回顾客名称和电子邮件列的值，而不包括其他列。这种选择性地从查询结果中获取特定列的过程称为投影。

- 总结

  - 谓词和投影是构成数据库查询语句的重要组成部分，用于实现数据的筛选和选择性地获取列数据。它们在查询优化和查询执行阶段起着重要的作用，可以帮助有效地检索和操作数据库中的数据。

## 二级索引

- 二级索引是基于表中的某个列或多个列创建的，它包含了相应列的值以及指向相应记录的物理地址。在查询中，通过使用二级索引，数据库可以直接定位到符合查询条件的记录，而无需扫描整个数据表。
- 特点
  - 非聚集索引：InnoDB的二级索引是非聚集索引，即索引数据和实际数据存储在不同的位置。这意味着在通过二级索引查询时，数据库需要先通过索引找到相应的记录，并根据记录的物理地址再次访问实际数据。（聚簇和非聚簇只是索引底层的数据结构类型）
  - 辅助查询性能：二级索引能够显著提高查询性能，尤其是在涉及大量数据的表中。通过使用二级索引，可以快速定位到符合查询条件的记录，减少了全表扫描的开销。
  - 唯一性约束：除了普通的二级索引外，InnoDB还支持唯一索引和主键索引。唯一索引要求索引列的值唯一，而主键索引则是表中的主键约束所建立的索引。这些索引可以用于快速查找具有唯一性属性的记录。
- 二级索引也会对数据库的性能产生一定的影响。创建过多的二级索引可能会增加数据插入、更新和删除的开销，因为每次对表的数据进行修改时，都需要更新相应的索引。因此，在设计数据库时，需要权衡索引的数量和查询性能之间的平衡

## innodb和myisam的对比

- 对比
  - 事务支持：
    - InnoDB支持事务ACID（原子性、一致性、隔离性和持久性）属性。它使用行级锁定，并支持并发处理和高并发写入操作。
    - MyISAM不支持事务，它使用表级锁定，因此在并发写入操作时可能会遇到锁冲突和性能瓶颈。
  - 并发性能：
    - InnoDB具有更好的并发性能，因为它支持行级锁定并提供了更好的并发控制机制。这使得多个会话可以同时读取和写入不同的行。
    - MyISAM使用表级锁定，这意味着当一个会话对表进行写操作时，其他会话将无法读取或写入该表。
  - 索引：
    - InnoDB的二级索引是非聚簇索引，存储的是索引数据和对应的行指针。它支持外键约束和唯一性约束。
    - MyISAM的二级索引是聚簇索引，存储的是实际数据和对应的索引。它不支持外键约束和唯一性约束（只能通过程序或触发器实现）。
  - 容灾性：
    - InnoDB支持崩溃恢复和故障容错，并提供了自动故障恢复机制。它具有更好的数据完整性保护，可以在故障发生后自动恢复。
    - MyISAM不提供崩溃恢复和故障容错机制，如果在处理过程中发生故障，可能会导致数据损坏。
  - 数据一致性：
    - InnoDB通过使用事务和行级锁定来确保数据的一致性和完整性。
    - MyISAM在并发写入操作中不提供严格的数据一致性保证，可能会导致数据不一致。
- 总结
  - 一般来说，对于要求事务支持和高并发性能的应用，InnoDB是更好的选择。
  - 而对于主要进行读操作、不要求事务支持和对性能要求较高的应用，MyISAM可能会更适合

## myisam

- MyISAM是一种磁盘存储的非事务数据库引擎，而不是基于内存的数据库引擎
- 特点
  - 表锁定：MyISAM使用表级锁定机制。当一个会话对表进行写操作时（如插入、更新或删除），会对整个表进行锁定，这会导致其他会话无法同时对该表进行读写操作。这对于高并发写入操作可能会导致性能瓶颈。
  - 高读取性能：MyISAM对于读取操作有很好的性能表现，因为它的设计目标是进行高速的查询。它通过使用B树索引来加速数据的查找和排序操作。在只有读取操作的场景中，MyISAM通常能够提供较好的性能。不用回表查询
  - 全文检索：MyISAM支持全文检索功能，可以执行基于文本内容的高级搜索操作。通过使用全文索引，可以进行有效的文本匹配和搜索。
  - 不支持事务：与InnoDB等存储引擎不同，MyISAM不支持事务处理和回滚操作。这意味着在MyISAM中，无法使用事务来保证数据的一致性和完整性。
  - 不支持外键约束：MyISAM也不支持外键约束的实现。这意味着在MyISAM表中，无法通过定义外键关系来保证相关表之间的数据完整性。
  - 不支持崩溃恢复：MyISAM在发生崩溃或故障时不提供自动恢复机制。如果在处理过程中发生故障，可能会导致数据损坏。因此，备份和定期维护对于保证数据的可靠性非常重要。
- 注意
  - MyISAM存储引擎的索引底层是B树（B-tree），而不是B+树（B+ tree）。B树是一种平衡多路搜索树，用于对大量数据进行快速查找、插入和删除操作。它具有自平衡的特性，可以高效地支持范围查询。

## 索引没有命中原因

- 原因
  - 查询条件不在索引列上：索引的作用是加快查询速度，但如果查询条件不是索引列上的列，MySQL无法利用索引进行快速查找，而会进行全表扫描。因此，确保查询条件与索引列匹配是至关重要的。
  - 索引列顺序不匹配：当查询涉及多个列时，索引列的顺序非常重要。如果索引定义的列顺序与查询语句中使用的列顺序不匹配，MySQL可能无法正确利用索引，导致索引不命中。
  - 索引数据分布不均匀：如果索引列的数据分布不均匀，即某些索引值出现频率过高而其他值很少出现，查询优化器可能会认为全表扫描比使用索引更有效率，因为通过索引查找需要多次I/O操作。这种情况下，索引可能无法命中，导致效率下降。
  - 索引选择性低：索引的选择性是指索引列中唯一值的比例。如果索引列的选择性很低，即唯一值较少，那么使用索引查找数据的效率将降低。因此，在设计索引时，应尽量选择具有较高选择性的列作为索引列。（因为索引出来的还是一堆数据。。）
  - 查询使用了不可优化的操作：某些查询操作无法有效利用索引，例如对索引列进行函数操作、使用模糊匹配的LIKE '%value%'查询以及使用OR操作符等。这些操作可能会导致索引无法命中，需要注意查询语句的编写。
  - 统计信息不准确：MySQL使用统计信息来估计查询使用索引的成本，并根据成本选择最优的执行计划。如果统计信息不准确，优化器可能会做出错误的选择，导致索引无法命中。可以通过使用ANALYZE TABLE命令更新表的统计信息来解决这个问题。
  - 硬件或配置问题：索引的命中率也可能受到硬件或配置的影响。例如，磁盘性能不足、内存配置不合理、并发连接过多等都可能影响索引的效率。因此，对于硬件和配置方面的问题，需要进行适当的调优。
- 针对具体的索引命中问题，可以使用EXPLAIN语句来分析查询计划，确保查询优化器选择了正确的索引。此外，还可以使用索引提示来指导优化器的索引选择，以确保索引能够被正确利用。

## explain语句会执行sql？

- `EXPLAIN`语句本身并不会执行查询，它只是分析查询计划。通过执行`EXPLAIN`语句，MySQL会解析查询语句并生成一个查询计划，以估计执行该查询所需要的资源和时间。

- 当你执行`EXPLAIN`语句时，MySQL会解析查询语句，分析表结构和索引信息，然后生成一个查询计划，显示查询优化器将如何执行该查询。查询计划中包含了优化器的决策，如使用哪些索引，使用何种连接方式，以及表的访问顺序等。

- 通过查看`EXPLAIN`语句的输出结果，你可以获取有关查询执行计划的详细信息，包括表的访问顺序、使用的索引、扫描行数、连接方式、排序方式等。这些信息可以帮助你理解MySQL如何执行查询，并根据优化器的选择进行必要的调整和优化。

- 当执行`EXPLAIN`语句时，MySQL会解析查询语句并生成一个查询计划，以用于分析查询的执行过程。下面是使用`EXPLAIN`语句的一般步骤和输出结果的解读

  - 执行`EXPLAIN`语句：在MySQL命令行或任何支持MySQL语法的客户端中，输入`EXPLAIN`关键字，紧接着输入待分析的查询语句，并执行该语句。例如：

    ```sql
    EXPLAIN SELECT * FROM mytable WHERE column = 'value';
    ```

  - 解析查询语句：MySQL会先对查询语句进行解析，并分析表结构和索引的信息。

  - 输出查询计划：`EXPLAIN`语句的输出结果包含多列，每一列提供了不同的信息。以下是一些常见的列和它们的解释：

    - `id`: 查询计划树的节点ID，按照查询树的执行顺序递增。
    - `select_type`: 查询类型，表示执行查询的方式，如`SIMPLE（简单查询）`、`PRIMARY（主查询）`、`SUBQUERY（子查询）`、`UNION（联合查询）`等。
    - `table`: 正在访问的表名。
    - `type`: 查询方式，表示表的连接类型或访问方式，例如`ALL（全表扫描）`、`index（索引扫描）`、`range（范围扫描）`、`join（连接查询）`等。一般来说，更好的索引设计和优化会选择较狭窄的访问类型。
    - `possible_keys`和`key`: 可能使用的索引和实际使用的索引。
    - `key_len`: 使用的索引长度。
    - `ref`: 显示索引列的哪个值与查询的列进行匹配。
    - `rows`: 估计需要扫描的行数。
    - `Extra`: 额外的信息，如`Using index（使用了索引覆盖扫描）`、`Using where（使用了WHERE条件）`、`Using filesort（使用了文件排序）`等。

  - 分析查询计划：通过观察输出结果，你可以分析查询计划，判断是否使用了合适的索引以及是否进行了有效的优化。

    - 优先考虑`type`列：应该尽量避免全表扫描（`ALL`类型），而是选择使用索引扫描（`index`类型）或范围扫描（`range`类型），以获得更高效的查询。
    - 检查`possible_keys`和`key`列：确保查询使用了合适的索引。如果没有匹配的索引，可能需要创建新的索引或调整查询语句。
    - 注意`ref`列：它显示了与索引列进行匹配的值。如果`ref`列的值太大，可能需要重新考虑索引的选择或查询语句的优化。

  - 优化查询语句和索引：根据查询计划的输出结果，可以优化查询语句和索引设计，以提高查询性能。可能的优化措施包括创建缺失的索引、调整索引列的顺序、优化查询语句的写法等。

- 需要注意的是，尽管`EXPLAIN`语句不会实际执行查询，但它可能会访问表的统计信息。如果统计信息过期或不准确，可能会导致查询计划的估计不准确。因此，在执行`EXPLAIN`语句之前，最好先使用`ANALYZE TABLE`命令来更新表的统计信息。

## 两段提交-分布一致性

- MySQL的两段提交（Two-Phase Commit，简称2PC）是一种用于保证分布式事务的一致性的机制。它在涉及多个数据库节点的分布式事务中，通过协调各参与节点的准备阶段和提交阶段，来保证所有节点要么全部提交，要么全部回滚，以保持数据一致性。
- 过程
  - 准备阶段（Prepare Phase）：
    - 事务协调者（Coordinator）向参与者（Participants）发送准备请求。
    - 参与者执行事务操作，并将操作结果（提交或回滚）和事务日志记录到本地的事务日志中。
    - 参与者回复事务协调者，表示自己已经准备好。
  - 提交阶段（Commit Phase）：
    - 事务协调者收到来自所有参与者的准备回复后，决定是否进行提交或回滚。
    - 如果所有参与者都准备就绪，事务协调者发送提交请求。
    - 参与者接收到提交请求后，执行事务提交，并将提交回复发送给事务协调者。
    - 如果有任何一个参与者无法提交（如发生故障），事务协调者发送回滚请求。
      - 参与者接收到回滚请求后，执行事务回滚，并将回滚回复发送给事务协调者。
- 两段提交的关键在于保证在提交阶段中的所有参与者都能安全提交或回滚，以保持数据的一致性。事务协调者起到了协调和决策的角色，而参与者负责执行事务操作并向协调者报告准备状态。
- 存在问题
  - 协调者单点故障：如果事务协调者发生故障，会导致整个事务悬挂，无法继续进行。
  - 阻塞和性能问题：在准备阶段期间，参与者会锁定相关资源，这可能导致其他事务的等待和性能问题。
  - 数据不一致：如果参与者在准备阶段准备好并提交事务，但在提交阶段发生故障，那么一些参与者可能已经提交了事务，而另一些参与者则未提交，导致数据不一致。

## 事务commit后能rollback？

- 事务一旦提交，通常是不允许回滚的。这是根据传统的ACID（原子性、一致性、隔离性和持久性）事务模型的原则来决定的。
- ACID事务模型中的一致性（Consistency）原则要求事务操作必须将数据库从一个一致性状态转换到另一个一致性状态。因此，在事务提交后，数据库引擎会将事务的更改持久地保存到磁盘中，从而实现一致性和持久性。
- 然而，一些数据库系统提供了一些扩展机制，允许在特定条件下回滚已提交的事务。下面是一些常见的回滚已提交事务的机制：
  - 闪回（Flashback）技术：
    - Oracle数据库提供了多种形式的闪回技术，如闪回查询（Flashback Query）、闪回表（Flashback Table）、闪回数据库（Flashback Database）等。
    - 闪回查询允许用户查询早期的数据版本，而不需要显式地回滚事务。通过闪回表和闪回数据库，可以将表或整个数据库回滚到之前的状态。
  - 回滚段（Undo Log）：
    - MySQL数据库中的InnoDB引擎使用回滚段实现事务的隔离和回滚功能。
    - 每个事务在进行更改时，都会生成相应的回滚段记录，用于撤销事务或恢复到之前的状态。
    - 通过设置适当的参数，可以控制回滚段的大小和使用，以满足应用程序的需求
- 尽管存在这些扩展机制，但回滚已提交的事务仍然是一个复杂和有限的操作。在一般情况下，事务提交后是无法直接回滚的。因此，在设计和实施应用程序时，需要根据具体的需求和数据库系统的支持来选择合适的回滚方案，并加以测试和验证以确保数据的一致性和完整性。

## int(10)和varchar(10)

- 区别
  - 数据类型定义：
    - int(10)：int是整数类型，用于存储整数数据，括号中的数字仅表示显示宽度（即显示的字符数），不影响存储范围或数据类型。
    - varchar(10)：varchar是可变长度字符串类型，用于存储字符数据，括号中的数字表示字段的最大长度，即最多可以存储10个字符。
  - 存储空间：
    - int(10)：int数据类型在MySQL中始终占据4个字节的存储空间，不受括号中的数字限制。
    - varchar(10)：varchar字段在存储实际数据时，会根据实际内容的长度进行动态存储，占据的存储空间为实际数据长度加上额外的字节（通常为1或2字节）来存储长度信息。因此，varchar字段的存储空间是根据具体内容的长度决定的，并且最大长度受到括号中指定的限制。
  - 类型特性：
    - int(10)：int是整数类型，用于存储数值，可以进行数学运算和排序。括号中的数字不影响整数的存储范围，而是用于控制显示宽度。
    - varchar(10)：varchar是字符类型，用于存储字符串。由于是可变长度的，它可以存储不定长度的字符串，但最大长度受到括号中指定的限制。varchar字段可以存储各种字符，而非只限于数字。

## 优化慢查询

- 在进行任何调优前，应先进行性能评估和测试，以了解目前的瓶颈和性能瓶颈所在，并针对性地采取相应的解决方案。此外，不同数据库系统和具体场景下，可能需要使用不同的解决方案
  - 索引优化：通过在常用查询条件上创建合适的索引，可以加快查询速度。分析查询执行计划，确定哪些列或组合列可以建立索引，并确保索引的选择性高。
  - 优化查询语句：优化查询语句本身可以改善查询性能。可通过以下方式进行优化：
    - 避免全表扫描：尽可能避免不带索引的条件或使用不必要的通配符，以减少全表扫描的开销。
    - 减少查询范围：使用合适的条件限制查询范围，提高查询效率。
    - 避免子查询或使用连接优化：对于复杂查询，考虑使用连接（JOIN）优化，减少子查询的数量和嵌套。
    - 优化排序和分组：合理使用ORDER BY、GROUP BY语句，并使用索引覆盖来避免排序和分组操作对性能的影响。
  - 表结构优化：合理设计和规划表结构，包括避免过度规范化、冗余和重复数据的存储。优化表结构可以减少查询时的连接操作、提高查询效率。
  - 缓存查询结果：对于频繁且稳定的查询结果，可以考虑将结果缓存起来，以减少数据库的访问压力。
  - 数据库参数调整：根据数据库的类型和配置，适当调整数据库的参数，如查询缓存大小、并发连接数等。
  - 数据库分区：对于庞大的数据表，可以考虑进行分区，以减少查询的范围和提高查询性能。
  - 性能监控和调优：使用数据库监控工具来实时监控数据库的性能，并定期分析查询日志和执行计划来发现潜在的慢查询问题，并进行相应调优。#
  - 硬件升级：如果以上优化手段仍无法解决慢查询问题，可以考虑进行硬件升级，如增加内存、更换磁盘以提高数据库的整体性能

## 存储引擎

- 当涉及到选择合适的MySQL存储引擎时，以下是更详细的特点、优点和缺点的介绍：

  1. InnoDB引擎：
     - 特点：InnoDB是MySQL默认的事务性引擎，支持行级锁定和事务处理。它具有高并发性能、崩溃恢复和数据完整性保护的特点。
     - 优点：InnoDB适合对数据完整性要求较高的应用场景。它支持外键约束、崩溃恢复和并发控制，并可以处理大规模数据和高并发读写操作。
     - 缺点：相对于其他引擎，InnoDB消耗更多的CPU和内存资源。此外，如果应用主要是只读操作或批量插入，选择其他引擎可能更合适。

  2. MyISAM引擎：
     - 特点：MyISAM是一个非事务型引擎，以表级锁定的方式进行操作，适合于读密集型应用。它具有高插入速度和较小的存储空间开销。
     - 优点：MyISAM在存储大量静态或不经常更新的数据方面表现良好。它支持全文索引和压缩表，并且查询速度较快。
     - 缺点：MyISAM不支持事务处理和行级锁定，这意味着当有多个并发写入时，可能会发生锁定冲突。此外，MyISAM表容易发生数据损坏，崩溃恢复困难。

  3. Memory引擎：
     - 特点：Memory引擎将数据存储在内存中，提供了非常快速的读写性能。它适用于需要高速读写、但数据规模较小且不需要持久化存储的场景。
     - 优点：Memory引擎具有极高的性能，因为它避免了磁盘IO的开销。它适合用于缓存表、临时表和需要频繁读写的应用。
     - 缺点：Memory引擎的数据存储在内存中，因此在服务器重启或崩溃发生时，数据会丢失。此外，Memory引擎不支持事务处理和复杂查询操作。

  4. NDB Cluster引擎（MySQL Cluster）：
     - 特点：NDB Cluster引擎是一种分布式数据库引擎，具有高可用性和可伸缩性的特点。它适用于需要高可用性、容错性和大规模分布式存储的应用。
     - 优点：NDB Cluster通过数据的分布和冗余存储，实现了高可靠性和负载均衡。它支持事务处理、并发性能好，并可适用于大规模数据存储的场景。
     - 缺点：NDB Cluster引擎的配置和部署相对复杂，对硬件和网络的要求较高。因此，它主要被用于大型企业级应用，不适合小规模应用和单机部署。

  此外，MySQL还有其他一些存储引擎，如Archive、CSV、Blackhole等，它们各自有着特定的特点和适用场景。在选择存储引擎时，你应该根据具体应用需求和性能要求进行评估和权衡。

## mysql的sql执行过程

- MySQL 的 SQL 执行过程可以大致分为以下几个步骤：

  1. 语法分析：MySQL 接收到 SQL 查询语句后，首先进行语法分析，检查语法的正确性，确保语句符合 MySQL 的语法规则。

  2. 优化器处理：接下来，MySQL 会对查询语句进行优化处理，选择最优执行计划。优化器会根据索引、表的大小、统计信息等来估算执行计划的成本，并选择最合适的索引和连接方式。

  3. 查询执行计划生成：优化器选择好执行计划后，会生成查询执行计划，即确定具体的执行步骤和顺序。这个执行计划包括了表的读取顺序、连接方式、使用的索引等。

  4. 执行计划执行：MySQL 根据执行计划开始执行查询。根据查询计划，逐步执行各个步骤，如表的扫描、连接、过滤条件的应用等。对于复杂的查询，可能包含多个阶段的数据处理。

  5. 结果返回：执行查询后，MySQL 将查询结果返回给客户端。如果查询涉及到大量数据，则可能按需分块返回结果，以提高性能和减少内存占用。

  需要注意的是，MySQL 的查询执行过程还受到数据库配置、硬件性能以及数据量的影响。执行计划的选择也可能受到索引状态、统计信息的准确性等因素的影响。因此，合理的查询设计、索引的建立和统计信息的收集都有助于提升查询性能。

# ==docker==

## 用dockerfile构建镜像过程

- Dockerfile 中的指令具有顺序性，按照从上到下的顺序被执行。在构建镜像时，每个指令都会生成一个新的镜像层，并将其缓存以提高构建效率。如果在 Dockerfile 中有更改，将只会重新构建修改部分的镜像层及其后续层

- 流程

  - 创建一个空文件并命名为 Dockerfile。

  - 在 Dockerfile 中编写指令，以描述构建镜像所需的步骤。以下是一些常用的 Dockerfile 指令：

    - `FROM`：指定基础镜像，用于构建新镜像的基础。
    - `RUN`：在容器中执行命令，用来安装软件包、运行脚本等。
    - `COPY` 或 `ADD`：将文件从构建上下文复制到镜像中。
    - `WORKDIR`：设置工作目录。
    - `CMD` 或 `ENTRYPOINT`：设置容器启动时要执行的命令。

  - 在 Dockerfile 所在的目录中创建构建上下文。构建上下文是构建镜像过程中所需要的文件和目录，可以通过 `COPY` 或 `ADD` 指令将构建上下文中的文件复制到镜像中。

  - 打开终端，导航到 Dockerfile 所在的目录，执行以下命令来构建镜像：

    ```shell
    docker build -t <image_name>:<tag> .
    ```

    其中，`<image_name>` 是你要为镜像指定的名称，`<tag>` 是镜像的标签，`.` 表示使用当前目录作为构建上下文。

  - Docker 会按照 Dockerfile 中的指令逐步构建镜像。在构建过程中，Docker 会下载所需的基础镜像，并按照指令的顺序执行相应的操作。构建过程中生成的每个镜像层都会被缓存，以便下次构建时快速重用。

  - 构建完成后，可以使用以下命令查看构建的镜像列表：

    ```shell
    docker images
    ```

  - 构建的镜像现在可以通过标签 `<image_name>:<tag>` 来使用、部署或发布了。

- 模板

  ```dockerfile
  # 设置基础镜像
  FROM base_image:tag
  
  # 作者信息
  LABEL maintainer="Your Name <yourname@example.com>"
  
  # 设置工作目录
  WORKDIR /app
  
  # 复制应用程序及其依赖到容器中
  COPY . /app
  
  # 执行构建过程中需要的命令
  RUN command1 \
      && command2 \
      && command3
  
  # 设置环境变量
  ENV ENV_NAME value
  
  # 暴露容器的端口
  EXPOSE port_number
  
  # 容器启动时要执行的命令
  CMD ["command", "arg1", "arg2"]
  ```

  

# ==k8s==

## k8s简介

- 简介
  - Kubernetes（通常称为K8s）是一个开源的容器编排和集群管理平台，用于自动化部署、扩展和管理容器化的应用程序。它提供了一个强大的工具集，使得构建、部署和运行应用程序在跨多个主机上以容器的方式变得更加简单和可靠。
  - Kubernetes的设计目标是解决容器化应用程序在分布式环境下的复杂性和可靠性问题。它提供了一个可扩展的、高度可用的架构，具有智能的容器调度器和集群管理功能，以实现容器的自动化部署、弹性扩展和故障恢复。
  - Kubernetes的核心概念是Pod、Deployment、Service和Volume。Pod是Kubernetes的最小调度和管理单元，它由一个或多个容器组成，并共享网络和存储资源。Deployment用于管理容器的副本集，定义了应用程序的部署策略和更新方式。Service提供了一种抽象，为一组Pod提供统一的访问入口和负载均衡功能。Volume用于在Pod中存储和共享数据。
- 特点
  - 自动化容器部署和调度：Kubernetes可以自动将容器部署到集群中的节点上，并根据资源需求和约束条件等将Pod调度到合适的节点上。
  - 自动扩缩容：Kubernetes允许根据应用程序的负载情况进行自动扩容和缩容。它可以根据设定的规则自动增加或减少Pod副本的数量，以满足应用程序的需求。
  - 服务发现和负载均衡：Kubernetes提供了内建的服务发现机制，并通过负载均衡器将请求分发到后端的Pod实例，确保应用程序的可访问性和可用性。
  - 存储管理：Kubernetes支持多种存储选项，包括存储卷（Volume）、网络存储（NFS、GlusterFS等）和云存储（AWS EBS、Azure Disk等），用于持久化存储和共享数据。
  - 自动恢复和滚动更新：Kubernetes可以监控和管理Pod的健康状态，并在Pod失败或不可用时自动重启或替换它们。同时，它还支持滚动更新机制，从而确保应用程序的零停机时间更新。
  - 配置和密钥管理：Kubernetes允许将应用程序的配置信息和敏感数据（如凭据、密钥等）以ConfigMap和Secret的方式进行管理，并将其注入到Pod中。
  - 强大的声明式API：Kubernetes通过RESTful风格的API定义了一组资源对象，以声明式的方式描述应用程序的期望状态，并负责将集群的当前状态和期望状态进行调谐。
  - 批处理和定时任务：Kubernetes提供了Job和CronJob资源对象，用于管理一次性的批处理任务和定时计划任务，以方便地运行、监控和管理这些任务。
  - 多集群管理：Kubernetes支持多集群的管理和编排，可以在不同的云提供商、数据中心或地理位置上运行和管理多个独立的Kubernetes集群。
  - 插件和扩展性：Kubernetes提供了丰富的插件和扩展机制，允许用户根据特定的需求和场景，自定义和扩展Kubernetes的功能和行为。

## k8s组件

- Kubernetes（K8s）是一个分布式的、高可用的容器编排和集群管理平台

- Kubernetes的架构设计为容器化应用程序的部署、弹性扩展、自动恢复和管理提供了强大的功能和灵活性。这种分布式的架构使得Kubernetes能够处理大规模集群中复杂的容器编排和管理任务，并确保高可用性和可靠性。
- 组件
  - Master节点：
    - API Server：作为整个集群的入口，接收和处理API请求，并通过调用其他组件来完成操作。
    - Scheduler：负责根据容器的资源需求、亲和性和约束条件等，将Pod调度到合适的Worker节点上。
    - Controller Manager：管理集群中的控制器，例如副本控制器、服务控制器等。
    - etcd：分布式键值存储，用于保存集群的配置数据和状态信息。
  - Worker节点：
    - Kubelet：在节点上与Master节点通信，负责管理和监控该节点上的容器、Pod等资源。
    - Kubernetes Proxy：负责为Service提供负载均衡和代理功能，确保集群内的服务可访问性和可用性。
    - Container Runtime：负责运行容器的软件，例如Docker、Containerd等。
  - Pod：
    - Pod是Kubernetes的最小调度和管理单元，可以包含一个或多个紧密关联的容器。
    - 在同一个Pod中的容器共享相同的网络命名空间和存储卷，它们可以通过localhost进行通信。
  - Deployment：
    - Deployment是用于管理容器副本集的控制器，提供了应用程序的部署和更新方式。
    - Deployment定义了应用程序的副本数量、容器镜像、升级策略等配置信息。
  - Service：
    - Service是一种抽象，为一组Pod提供统一的访问入口，通过负载均衡将请求分发到后端的Pod实例。
    - Service可以通过标签选择器与Pod进行动态绑定，使得应用程序更具可伸缩性和可靠性。
  - Volume：
    - Volume是一种抽象，用于在Pod中持久化存储和共享数据。
    - Volume可以将外部存储、存储卷、空目录等挂载到容器中，确保数据的持久性和可访问性。

## pod的生命周期

- Kubernetes通过监控和管理每个Pod的生命周期，确保应用程序能够以可靠和可持续的方式运行，并在需要时自动进行处理和恢复
- 生命周期
  - Pending（等待阶段）：Pod被创建后，它会处于Pending状态，此时Kubernetes调度器正在为Pod选择合适的节点，并且等待节点上的容器运行时准备好相关资源。
  - Running（运行阶段）：一旦Pod被调度到一个节点上并且其中的容器已经创建并运行，它就处于Running状态。此时，Pod内的容器正在执行其定义的功能。
  - Succeeded（成功阶段）：如果Pod内的所有容器成功完成了它们的任务，那么Pod将进入Succeeded状态。这通常在批处理任务或定时任务完成后发生。
  - Failed（失败阶段）：如果Pod内的任何容器出现故障或意外终止，Pod将进入Failed状态。这可能是容器执行失败、运行时错误或资源不足等原因导致的。
  - Unknown（未知阶段）：如果Kubernetes无法获取到Pod的状态信息，Pod将处于Unknown状态。这可能是由于与Pod关联的节点与Master失去连接或通信故障等原因导致的。
- Running状态的Pod，还可能发生以下几种情况：
  - ContainerCreating（创建容器）：正在创建Pod中的容器。
  - ContainerRunning（容器运行中）：Pod中至少一个容器正在运行。
  - ContainerTerminated（容器终止）：Pod中的一个或多个容器已经终止。

# ==rabbitmq==

## 持久化

- RabbitMQ是一个开源的消息队列中间件，它提供了消息传递、发布/订阅、消息排队等功能。当涉及到持久化消息时，RabbitMQ使用持久化交换器（Durable Exchanges）、持久化队列（Durable Queues）、持久化消息（Durable Messages)来保证消息持久化

- 持久化交换器（Durable Exchanges）：

  - 当你声明一个交换器时，你可以将其标记为持久化，以确保在RabbitMQ服务器重启后仍然存在。持久化交换器的元数据（例如其类型、绑定关系等）会被存储在磁盘上，以便在服务器重启后恢复。你可以通过设置`durable=True`来声明持久化交换器。

    ```java
    channel.exchange_declare(exchange='my_exchange', exchange_type='direct', durable=True)
    ```

- 持久化队列（Durable Queues）：

  - 类似于交换器，你可以将队列标记为持久化，以确保在RabbitMQ服务器重启后仍然存在。持久化队列的元数据和队列中的消息都会存储在磁盘上。你可以通过设置`durable=True`来声明持久化队列。

    ```java
    channel.queue_declare(queue='my_queue', durable=True)
    ```

- 消息的持久化

  - 消息的持久化在交换器、队列以及发送消息时的所有环节都需要进行设置，否则消息可能不会被持久化
  - 持久化仅在消息传递的中间环节起作用，即消息发送到交换器或队列后。如果消费者未能及时处理消息、消息被确认之前，消息可能仍然存在丢失的风险。
  - 持久化会增加系统的开销，因为需要将消息写入磁盘。因此，需要根据系统的性能需求和容忍度进行权衡和配置。
  - 持久化不能完全保证消息的可靠性，仍有一定概率出现消息的丢失。为了确保更高的可靠性，可以使用消息确认机制（acknowledgement）和事务（transaction）。

## 死信队列

- 其实是普通队列
- 消费者可以通过以下参数指定死信消息的处理队列
  - x-dead-letter-exchange：消息被拒绝或过期后转发到的死信交换机名称。
  - x-dead-letter-routing-key：消息在死信交换机路由时使用的路由键。

## 队列参数

- 常见参数
  - durable：指定队列是否是持久化的。如果将此参数设置为true，则队列将在服务器重启后仍然存在。默认值为true。
  - auto-delete：指定队列在没有消费者连接时是否自动删除。如果将此参数设置为true，则当最后一个消费者断开连接时，队列将被自动删除。默认值为false。
  - exclusive：指定队列是否为独占队列。如果将此参数设置为true，则只允许当前连接创建的消费者使用该队列。其他连接无法访问该队列。默认值为false。
  - arguments：自定义的键值对参数，用于配置额外的队列行为。参数的具体含义和作用取决于使用的插件和配置。一些常见的参数包括：
    - x-message-ttl：设置队列中消息的过期时间，单位为毫秒。超过指定时间未被消费者接收，则消息会被视为过期消息并被丢弃。
    - x-max-length：设置队列中允许保存的消息的最大数量。超过指定数量时，新的消息会被丢弃或进入死信队列，具体取决于配置的策略。
    - x-expires：设置队列没有被使用时的自动删除时间，单位为毫秒。如果队列在指定时间内未被使用，则会自动删除。（不会成为死信）
    - x-dead-letter-exchange：消息被拒绝或过期后转发到的死信交换机名称。
    - x-dead-letter-routing-key：消息在死信交换机路由时使用的路由键。
    - x-max-length-bytes：设置队列中消息总字节的最大限制。超过指定字节限制时，新的消息会被丢弃或进入死信队列，具体取决于配置的策略。
    - x-max-priority：设置消息的优先级上限。可以为消息设置不同的优先级，数字越大表示优先级越高。
    - x-overflow：设置队列满时消息的处理方式。可选值包括"drop-head"（丢弃最早的消息）和"reject-publish"（拒绝发布新的消息）。
    - x-queue-mode：设置队列的模式。可选值包括"lazy"（懒惰模式，消息在需要时才写入磁盘而不是立即写入）和"default"（默认模式）。

## ack机制

- 过程
  - 消费者从队列中获取到一条消息。
  - 消费者开始处理消息，执行相应的业务逻辑。
  - 处理
    - 如果消息处理成功
      - 消费者发送ACK信号给RabbitMQ来确认消息已被处理。
      - RabbitMQ接收到ACK信号后，将消息标记为已确认，并从队列中删除该消息。
    - 如果消息处理失败或发生异常
      - 消费者可以选择拒绝消息（reject）或抛出异常。
      - 如果消息被拒绝，RabbitMQ可以将其重新排队等待其他消费者处理，或选择将其发送到死信队列（Dead Letter Queue）。
    - 如果消费者断开连接或发生故障
      - RabbitMQ会将未被确认的消息重新分发给其他可用的消费者。
- 没有ACK确认的消息会一直保留在队列中，直到被确认或被重新分发

## 高可用

- RabbitMQ使用的是主备复制模式，通过复制消息队列和元数据来提供高可用性和数据冗余
  - 主备节点选举：在RabbitMQ集群中，主节点负责接收和处理客户端的消息，并将其复制到备份节点。主备节点之间通过心跳机制进行通信。如果主节点失去与集群的联系，备份节点将接替成为新的主节点。
  - 队列同步：主节点将消息发送给备份节点以实现消息的复制。RabbitMQ使用了基于AMQP协议的多个复制策略，例如"all"、"exactly"和"nodes"等。这些策略指定了将消息复制到所有备份节点、精确复制到特定数量的备份节点，或将消息复制到指定节点的范围内。
  - 元数据复制：除了消息的复制，RabbitMQ还复制了队列、交换机、绑定和其他元数据。这确保了当主节点失败时，备份节点能够接管整个系统的状态，包括队列和交换机的配置。
  - 消息确认和持久化：复制过程中，备份节点将确认消息的接收，并将其持久化到磁盘上。这样即使发生故障，备份节点也能重新启动且不会丢失已接收的消息

- 过该主备复制机制，RabbitMQ能够实现高可用性和数据冗余，确保消息队列的持久性和可靠性。即使在主节点故障或不可用的情况下，备份节点也能接管处理消息，并保证消息的传递和处理

# ==nginx==

## 正向代理和反向代理

- 反向代理和正向代理是两种不同的代理服务器架构，它们在代理方向、部署位置和应用场景等方面存在差异
- 无论是正向代理还是反向代理，它们都可以在网络通信中起到重要作用，提供更好的性能、可用性和安全性。具体使用哪种代理方式取决于需求和场景的不同。
- 正向代理：
  - 正向代理是客户端与目标服务器之间的代理服务器。客户端向代理服务器发送请求，代理服务器再将请求转发给目标服务器，并将从目标服务器得到的响应返回给客户端。
  - 客户端通常对目标服务器是不可见的，它们将所有请求发送给代理服务器，然后由代理服务器代表客户端与目标服务器通信。
  - 主要用于客户端无法直接访问目标服务器的情况，例如访问被封锁的网站或隐藏真实IP地址。
- 反向代理：
  - 反向代理是位于目标服务器和客户端之间的代理服务器。客户端发送请求到反向代理服务器，然后反向代理服务器将请求转发给一个或多个后端服务器，最后将后端服务器的响应返回给客户端。
  - 客户端对后端服务器是不可见的，它们将请求发送给反向代理服务器，而不是直接发送给后端服务器。
  - 反向代理通常用于负载均衡、缓存、SSL终端和安全代理等场景，能够提供高可用性、性能和安全性。
- 区别
  - 代理方向：正向代理是代理服务器代表客户端发送请求到目标服务器，而反向代理是代理服务器代表后端服务器接收客户端的请求。
  - 部署位置：正向代理位于客户端和目标服务器之间，而反向代理位于目标服务器和客户端之间。
  - 可见性：正向代理对目标服务器可见，而对客户端是透明的；而反向代理对客户端可见，而对后端服务器是透明的。

## nginx反向代理

- Nginx充当了客户端和后端服务器之间的中间层。它隐藏了后端服务器的细节，使客户端认为它们直接与Nginx通信。Nginx可以根据配置的规则将请求转发到一个或多个后端服务器，并提供负载均衡、高可用性、缓存和安全功能等。
- 流程。。
  - 客户端发送请求：
    客户端向Nginx服务器发送请求，请求包含目标URL以及其他相关信息。
  - Nginx接收请求：
    Nginx服务器接收到客户端的请求，并进行处理。
  - 选择后端服务器：
    Nginx根据预先配置的代理规则（如负载均衡算法、路由规则等），选择一个目标后端服务器来处理该请求。后端服务器可以是单个服务器，也可以是一个服务器池。
  - 转发请求：
    Nginx将客户端的请求转发给选择的后端服务器。它将请求的相关信息（如请求方法、头部、路径等）一并转发给后端服务器。
  - 后端服务器处理请求：
    后端服务器接收到Nginx转发的请求，并根据请求进行处理。后端服务器处理请求的方式取决于具体的应用程序或服务。处理完成后，后端服务器将生成的响应返回给Nginx。
  - Nginx接收响应：
    Nginx服务器接收到后端服务器返回的响应。
  - 响应发送至客户端：
    Nginx将后端服务器返回的响应发送给原始的客户端。响应可能包括状态码、响应头部以及实际的响应数据。

## nginx负载均衡

- 算法
  - 轮询（Round Robin）：Nginx按照请求的顺序依次将请求分发给后端服务器。每个后端服务器按照轮询顺序接收请求，实现请求的均衡分发。（默认）
  - IP哈希（IP Hash）：Nginx根据客户端的IP地址计算哈希值，并将同一IP地址的请求分发给同一个后端服务器。这样可以使得同一客户端的请求始终由同一个后端服务器处理，适用于需要保持会话/状态的场景。
  - 最少连接数（Least Connections）：Nginx根据当前后端服务器的连接数选择连接最少的服务器来处理请求。这种算法可以实现动态负载均衡，将请求分发给连接数最少的服务器，以提高整体性能。
  - 加权轮询（Weighted Round Robin）：Nginx根据后端服务器的配置权重，按照权重比例分配请求。权重越高的服务器会接收到更多的请求，适用于后端服务器性能不均衡的场景。
  - 加权最少连接数（Weighted Least Connections）：Nginx根据后端服务器的配置权重和连接数，选择连接数最少且权重最高的服务器来处理请求。通过结合连接数和权重的信息，实现更加智能的负载均衡。
  - 最少响应时间（Least Time）：Nginx根据后端服务器的平均响应时间选择处理请求的服务器。通过监控每个服务器的响应时间，将请求发送到平均响应时间最短的服务器，以提供更快的响应。
  - 随机（Random）：Nginx随机选择一个后端服务器来处理请求。每次请求都是随机分配给后端服务器，适用于无需考虑后端服务器负载情况的场景。
  - 哈希（Hash）：Nginx根据请求的某个特定标识（比如URL、HTTP头部等）计算哈希值，并将具有相同哈希值的请求分发给同一个后端服务器。这样可以实现特定请求始终由同一台服务器处理，适用于需要保持一致性的场景。
  - 基于响应时间的负载均衡（Least Response Time）：Nginx根据后端服务器的响应时间选择处理请求的服务器。这种算法会根据服务器的响应时间，先选择具有最快响应的服务器，以提供更好的用户体验。

# ==redis==

## hyperloglog

- HyperLogLog是一种基数估算算法，用于估计一个集合中不重复元素的个数。它能够以极高的概率提供接近精确计数的结果，同时占用很小的内存空间
- 底层结构
  - 使用了一种称为HyperLogLog++的算法，它是在HyperLogLog的基础上改进和优化而来
  - HyperLogLog++的底层结构是一个稀疏数组（Sparse Array），它由一个固定长度的数组和一个稀疏精简的稀疏位图（Sparse Bitmap）组成
    - 稀疏数组中的每个位置都可以存储一个8位的桶（Bucket），每个桶记录了一个哈希函数计算得到的最高连续0的位数。
    - 通过记录每个桶中最高连续0的位数，HyperLogLog++可以对多个哈希函数的结果进行聚合，从而推断集合的基数（不重复元素的数量）。
  - HyperLogLog++的预估误差率通常在0.81%左右，在大多数情况下可以提供准确的基数估算。在低基数的情况下误差会大
  - HyperLogLog++只支持计数操作，不支持删除或者查询个别元素。
  - 总结
    - Redis的HyperLogLog底层使用了稀疏数组和稀疏位图的结构，通过多个独立的哈希函数计算找到对应的桶，并记录每个桶中最高连续0的位数，从而实现了高效且占用较小内存的基数估算功能。
- 记录最高连续0位数
  - 位数相当于知道个数
  - HyperLogLog算法中只存储最高连续0的位数的原因在于统计基数（集合元素的数量）并不需要精确地知道每个元素的具体值，而只需要知道元素的哈希值
  - 而哈希函数的输出是均匀分布的，因此最高连续0的位数的数量可以用来估算基数。
  - 具体来说，哈希函数将元素映射为一个二进制位串，在最高连续0的位数较大的情况下，该元素的哈希值在位串中的前缀0较长。这个最高连续0的位数越大，表示对应的哈希值的前缀0越长，说明这个元素在集合中出现的概率越低。因此，通过统计最高连续0的位数的数量，可以对元素的分布情况进行推断，从而估算出基数
- 空间对比
  - 默认情况下，HyperLogLog 使用 12KB 的内存空间，可以估算超过 10^9 个不同元素的基数，误差率约为 0.81%
  - HashSet
    - Java的Integer对象（每个对象占用约16字节）
    - 负载因子：假设负载因子仍然为0.75。
    - 总内存占用大小为：(10^9 * 16字节) / 0.75 ≈ 2.14 * 10^10 字节 = 21.4 GB

## bitmap

- 底层
  - Bitmap的底层结构是一个动态字符串（String），实际上是一个由二进制位组成的串
  - 建一个Bitmap时，它是一个空的、没有任何位被设置的数据结构。它的大小会根据你设置的位的位置而动态地进行扩展，以适应你实际存储的数据
- 个位都有一个对应的索引位置。位的索引号从0开始递增，依次对应于Bitmap字符串的每个二进制位。位0对应的是Bitmap字符串的最低有效位，位1对应次低有效位，以此类推
- 内存
  - 内存占用= (最大索引值 / 8) 字节
  - 13个用户：最大索引值为12，内存占用 = 12 / 8 字节

## 布隆过滤器优缺点

- 优点
  - 快速查询：布隆过滤器的查询时间复杂度是常数级别的，不受集合大小的影响。只需进行一系列哈希函数计算和位操作即可判断元素是否存在于集合中，速度非常快。
  - 空间效率高：相比于其他数据结构（如哈希表、红黑树等），布隆过滤器所占用的空间相对较小。布隆过滤器利用位数组和哈希函数的特性，可以用较少的存储空间来表示大规模的数据集合。
  - 可扩展性：布隆过滤器可以通过调整哈希函数的数量和位数组的大小来进行扩展。通过增加哈希函数的数量，可以减小冲突的概率，提高准确性；通过增大位数组的大小，可以降低误判率。
- 缺点
  - 误判率存在：布隆过滤器在判断一个元素是否存在时，可能会产生一定的误判率。这是因为哈希函数可能会导致不同的元素映射到相同的位上，从而造成误判。误判率随着布隆过滤器的填充程度增加而增加。
  - 不支持删除操作：布隆过滤器的设计初衷是支持快速的查询操作，而不支持删除操作。删除一个元素比较困难，因为删除一个元素可能会影响到其他元素的判断结果。如果需要支持删除操作，可能需要使用其他数据结构来辅助处理。（可用计数法来表示，但是这样性能会下降）

## aof文件过大

- 方案
  - AOF重写：Redis提供了AOF重写机制，通过执行BgRewtiteAOF命令手动重写或设置配置项`aof_rewrite_incremental_fsync`为"yes"，可以启动AOF自动重写过程。Redis会读取旧的AOF文件，并且只选择那些能表示当前数据状态的命令，去除了多余的命令。当重写完成后，Redis会将新的AOF文件替换原来的AOF文件
  - AOF压缩：如果对数据的实时性要求不高，可以使用`BGREWRITEAOF`命令开启AOF重写后，再进行压缩操作。可以使用`redis-cli`工具的`--rdb-output`参数将重写后的AOF文件转换为RDB文件，然后再使用`redis-cli`工具的`--aof-rewrite`参数将RDB文件转换为AOF文件，从而减小AOF文件的大小。
  - AOF自动重写配置：通过设置`auto-aof-rewrite-percentage`和`auto-aof-rewrite-min-size`配置项，可以定期触发AOF重写操作，限制AOF文件大小。
  - 压缩和归档：可以通过压缩AOF文件或将历史的AOF文件归档到其他存储介质中，以释放磁盘空间。可以使用压缩工具（如gzip）对AOF文件进行压缩，并在需要时进行解压缩。
  - 修改AOF策略：可以根据实际需求，调整AOF持久化策略。可以选择每秒保存一次或每个写命令保存一次等策略，以控制AOF文件的增长速度。
- 总结
  - 需要注意的是，在执行AOF重写或压缩操作时，Redis服务器可能会消耗大量的CPU和磁盘IO资源，可能会对系统性能产生一定影响。因此，在生产环境中进行这些操作时，应事先评估其对系统性能的影响，并选择合适的执行时间。
  - 另外，为了保证数据的安全，建议在进行上述操作之前，先备份当前的AOF文件，以防意外情况发生。

## 和memcached对比

- 功能
  - 数据类型支持：
    - Memcached：支持简单的键值对存储。数据可以是任何类型的二进制数据。
    - Redis：支持多种数据类型，如字符串、哈希表、列表、集合和有序集合。
  - 数据持久化：
    - Memcached：不支持数据持久化。在重启后，所有数据都会丢失。
    - Redis：支持数据持久化，可将数据保存到磁盘，以便在重启后恢复数据。
  - 数据查询：
    - Memcached：仅支持简单的键值对查找和存储操作。
    - Redis：支持丰富的数据查询操作，如范围查询、排序、分页等。
  - 数据复制和高可用性：
    - Memcached：不提供内置的数据复制和高可用性机制。
    - Redis：支持主从复制和Sentinel模式，提供数据的冗余备份和故障转移。
  - 扩展性：
    - Memcached：通过增加更多的缓存节点来扩展容量和吞吐量。
    - Redis：除了支持水平扩展，还提供了集群模式，可以自动分片数据并在多个节点上进行存储。
  - 性能：
    - Memcached：以极高的性能著称，适用于对读写操作速度要求非常高的场景。
    - Redis：性能也非常出色，特别适合在数据结构和功能多样性之间进行权衡。
- 选择使用Memcached还是Redis取决于应用程序的具体需求。
  - 如果简单的键值存储足够，并且对于性能和吞吐量有极高的要求，Memcached可能是理想的选择。
  - 如果需要更丰富的数据结构和功能，以及数据持久化、高可用性和分布式支持，Redis可能更适合。

# ==distributed==

## 集群下原子性

- 在集群环境下，保证原子性是一个复杂的问题，需要考虑到并发访问、数据一致性以及分布式事务等方面。以下是一些常见的方法来保证在集群环境下的原子性操作
  - 分布式锁：使用分布式锁可以确保在一个时间点只有一个节点能够执行某个关键操作。常见的分布式锁实现包括基于数据库的锁、基于缓存的锁（如Redis等）以及基于ZooKeeper等的锁。当一个节点获取到锁后，其他节点将被阻塞，直到锁被释放。这样可以保证在集群环境下的原子性操作。
  - 乐观锁：乐观锁是通过在数据中引入一个版本号或时间戳，对数据更新进行标记的方式来解决并发冲突。在更新数据之前，检查当前版本号或时间戳与操作之前获取的版本号或时间戳是否一致，如果一致则允许更新，否则拒绝更新。这种方式适用于多读少写的情况，可以减少锁的竞争。
  - 分布式事务：对于涉及多个分布式系统组件之间的原子性操作，需要使用分布式事务来确保数据的一致性。分布式事务管理器（如基于Two-Phase Commit协议的XA协议）可以协调各个参与方的事务并进行提交或回滚，保证所有操作要么全部成功，要么全部失败。
  - 幂等性设计：在设计操作时，考虑将其设计为幂等操作，即多次执行相同的操作得到的结果与执行一次相同的操作结果相同。这样，即使在发生网络异常或操作失败时，重复执行操作也不会引起数据的不一致。通过幂等性设计，即使操作在集群环境下被多次执行，仍能保证最终结果的一致性。
  - 数据分区设计：将数据分成不同的区域，每个区域由不同的节点或服务器管理，可以减少并发冲突的概率。不同的数据分区应该尽可能避免直接冲突，从而降低对分布式锁的需求，提高整个系统的性能。

- 需要注意的是，并发访问和原子性保证是一个复杂的问题，没有一种通用的解决方案适用于所有情况。在实际应用中，需要综合考虑业务需求、数据访问模式和系统架构等因素，选择适合的方法来保证在集群环境下的原子性操作。

## 分布式锁发生异常catch要怎么处理

- 在分布式锁的代码逻辑中，如果发生异常，可以根据具体的情况来决定应该采取何种处理方式。以下是一些常见的异常处理策略：
  - 日志记录：在catch块中可以使用日志记录工具，将异常信息记录下来，以便于排查问题和进行故障诊断。
  - 重试操作：对于某些可恢复的异常，可以选择进行重试操作。在catch块中可以进行一定次数的重试，以期待下次操作成功。这种策略适用于那些由于临时的网络故障或资源竞争导致的异常情况。
  - 释放锁资源：如果捕获到异常，可能会导致原本需要释放的锁资源没有得到释放。在catch块中，应该确保释放之前获得的锁资源，以避免出现锁资源泄露的情况。例如，在finally块中添加释放锁的逻辑，确保无论是否发生异常，都能够执行到释放锁的操作。
  - 报告异常：在异常发生时，可以通过一些方式将异常信息及时通知相关的开发人员或运维人员。例如，发送邮件或推送到消息队列，以便及时处理异常情况。
- 需要根据具体的业务场景和异常类型来决定如何处理异常。有些异常是可以恢复的，可以进行重试；而有些异常可能是无法恢复的，需要进行相应的告警和处理。
- 无论选择哪种处理方式，重要的是要记录日志并及时响应异常，以便及时检测和修复问题，从而保证分布式锁的正确性和可靠性。

## 如何保证幂等性

- 保证幂等性是在分布式系统中非常重要的一个概念，它确保相同的请求在多次执行时产生相同的结果，而不会对系统状态产生额外的副作用。以下是一些常见的方法来保证幂等性：

  1. 唯一标识符（Unique Identifier）：为每个请求生成一个唯一的标识符，并将该标识符与请求一起传递到服务端。服务端可以在处理请求之前，检查该唯一标识符是否已被处理过，如果已处理，则直接返回相同的结果，而不执行重复操作。
  2. 幂等性检查与记录：服务端在处理请求之前，可以先检查请求是否已经被处理过，并将请求的唯一标识符记录下来。如果已处理，则直接返回相同的结果。这可以通过缓存、数据库或分布式锁等机制来实现。
  3. 乐观锁或版本控制：在处理请求之前，使用乐观锁或版本控制来确保数据的一致性。通过在请求中传递版本号或时间戳，并在更新操作时比较版本号或时间戳，可以避免并发操作引起的数据冲突和重复更新。
  4. 普通幂等方法设计：在设计服务端的具体方法时，可以通过遵循幂等性的设计原则来确保幂等性。例如，使用幂等性的数据库操作（如使用唯一索引或合适的关系约束）、幂等性的数据更新方式（如使用原子操作）、幂等性的消息处理等。
  5. 重试机制：在客户端发生请求失败或超时的情况下，可以使用重试机制来确保请求的幂等性。通过在客户端进行幂等性检查，并根据需要进行重发，确保请求最终能够得到处理。

  需要注意的是，不同的业务场景和系统设计可能需要采用不同的方法来保证幂等性。在实际应用中，需要根据具体情况选择合适的方法，并在系统设计和实现中严格遵循幂等性原则。同时，在测试中对幂等性进行充分的验证和验证也是非常重要的。

## 分布式锁

- 分布式锁主要用于解决多个进程（或多个节点）之间的并发访问问题，在分布式系统中，由于多个节点同时访问共享资源，可能会导致数据不一致或系统异常。因此，引入了分布式锁来保证并发环境下的数据一致性和系统的可靠性。
- 分布式锁可以提供在分布式系统中对共享资源的安全访问和协调操作的手段，保证数据一致性、系统的可靠性和性能的提升。
- 使用分布式锁的常见场景和原因：
  1. 数据库操作：在分布式系统中，多个进程可能需要同时对同一个数据库进行读写操作，通过使用分布式锁，可以确保同一时间只有一个进程能够对数据库进行写操作，避免数据冲突和脏数据的问题。
  2. 缓存同步：分布式系统中经常使用缓存来提高性能，但在某些场景下，多个进程可能需要同时更新同一个缓存项。通过使用分布式锁，可以保证只有一个进程能够获取到锁并更新缓存项，避免缓存数据的不一致。
  3. 分布式任务调度：在分布式系统中，多个节点可能同时执行相同的任务，通过使用分布式锁，可以保证同一时间只有一个节点在执行某个任务，避免重复执行或者任务冲突的问题。
  4. 防止重复操作：某些操作只需要执行一次，但分布式系统中存在多个节点同时执行的可能。通过使用分布式锁，可以保证同一时间只有一个节点能够获取到锁并执行该操作，避免重复操作。
  5. 限制资源访问：对于一些有限资源（比如许可证、连接池等），通过使用分布式锁，可以限制并发访问数量，确保资源的稳定使用。

- 实现分布式锁的方式有多种
  - 基于数据库：
    - 使用数据库的事务和锁机制来实现分布式锁，比如在MySQL中使用行级锁或悲观锁。
    - 通过在数据库中创建一个特定的表或记录来表示锁的状态，获取锁时插入记录，释放锁时删除记录。
  - 基于缓存服务：
    - 使用分布式缓存服务如Redis、Memcached等来实现分布式锁。
    - 通过调用缓存服务提供的原子操作（如setnx、setex等）来实现获取锁和释放锁的操作，利用缓存的原子性和过期时间实现分布式锁的控制。
  - 基于ZooKeeper：
    - 使用ZooKeeper这种分布式协调服务来实现分布式锁。
    - 可以利用ZooKeeper的有序临时节点（Sequential/Ephemeral）特性，在ZooKeeper中创建唯一有序节点作为锁的标识。
    - 进程通过比较自己创建的节点是否是最小的节点，来判断是否获取到锁。
  - 基于分布式协议：
    - 使用分布式协议如Raft或Paxos等实现分布式锁。
    - 借助分布式协议的一致性特性，在多个节点之间协调来确定锁的控制权。
    - 对于不同的协议，需要根据其具体实现方式和规则来实现分布式锁。
- 无论采用哪种方式，实现分布式锁都需要考虑如下关键点：
  - 正确性：要确保同一时间只有一个进程获取到锁。
  - 高可用：要处理锁的竞争和故障恢复，避免单点故障。
  - 超时处理：避免死锁和长时间等待锁的情况，可设置超时时间。
  - 互斥性：要保证锁是独占的，其他进程不能修改或删除已被获取的锁。
  - 性能：要考虑分布式锁的性能开销，尽量减少网络通信和资源占用。

## 两段提交协议-分布式事务

- 两段提交协议（Two-Phase Commit Protocol，简称2PC）是一种用于分布式系统中实现原子性事务的协议。它通过协调多个参与者（一般是数据库或分布式服务）的状态，确保在分布式环境下的事务操作的一致性和可靠性。

- 包含以下两个阶段：

  1. 准备阶段（Prepare Phase）：
     - 协调者（Coordinator）向所有参与者（Participants）发出准备请求（Prepare Request）。
     - 参与者接收到准备请求后，会执行本地事务的准备操作，并将准备好的状态（Ready）通过预备响应（Prepare Response）返回给协调者。
  2. 提交阶段（Commit Phase）：
     - 协调者根据所有参与者的准备响应情况决定是否提交事务。
     - 如果所有参与者都准备好了，协调者向所有参与者发出提交请求（Commit Request）。
     - 参与者接收到提交请求后，会执行本地事务的提交操作，并将提交结果通过提交响应（Commit Response）返回给协调者。

- 基本原理：

  - 在准备阶段，协调者向所有参与者询问是否准备好提交事务。参与者每次收到准备请求，都会将本地事务的准备状态存储起来，表示已准备好。
  - 在提交阶段，协调者先确认所有参与者的准备状态，如果有任何一个参与者未准备好，则中断提交过程，并向所有参与者发送中断请求（Abort Request）。否则，协调者向所有参与者发送提交请求。
  - 参与者在收到提交请求后，执行事务的提交操作，并将提交结果返回给协调者。协调者收到所有参与者的提交结果后，根据结果决定是否完成或中断事务。

- 总结

  - 两段提交协议的优点是可以确保分布式环境下的事务操作的一致性，但同时也存在一些缺点，比如在协调者单点故障或网络延迟的情况下，可能会导致整个系统的阻塞。为了解决这些问题，后续也出现了一些改进的协议，如三段提交协议（3PC）和Paxos等

- 提交时发生异常怎么办

  - 如果在提交阶段发生异常，可能会导致系统处于不一致的状态
    - 超时机制：可以在两段提交协议中引入超时机制，限定参与者和协调者在特定时间内完成操作。如果超时时间内没有得到响应，可以认为操作失败，进而中断事务并进行相应的恢复操作。
    - 事务回滚（Rollback）：如果在提交阶段发生异常，可以选择回滚事务。协调者可以向所有参与者发送中断请求（Abort Request），要求参与者执行事务的回滚操作，并将回滚结果返回给协调者。协调者收到所有参与者的回滚结果后，进行相应的处理。
    - 恢复与重试：如果在提交阶段发生异常，可以进行恢复操作并重试提交。该方式需要将系统设计为具有幂等性，即多次执行同一操作所产生的结果是一样的。协调者可以向所有参与者发送重试请求，参与者可以检查之前操作的情况，并执行相应的恢复操作。
    - 异常通知与手动干预：如果发生异常并且无法通过自动处理解决，可以采取手动干预的方式进行处理。例如，协调者可以通知相关人员或系统管理员，并协调处理异常情况，进行人工干预来修复系统并恢复一致性。
  - 以上处理方式可根据实际场景和需求进行选择和调整。重要的是，在设计和实施分布式事务时要考虑到异常情况，并确保对异常进行适当的处理，以保障数据的一致性和系统的可靠性。

- 优点和缺点：

  优点：

  1. 原子性：通过两段提交协议，可以保证分布式系统中事务的原子性。所有参与者在准备阶段确认事务的准备情况后，协调者在提交阶段决定是否执行事务的最终提交操作，从而确保所有参与者的状态保持一致。
  2. 一致性：两段提交协议能够保证分布式系统中操作的一致性。在协调者的指导下，所有参与者在准备阶段执行本地事务的准备操作，协调者根据参与者的准备状态决定是否提交事务，从而保证了数据的一致性。
  3. 可靠性：通过两段提交协议，可以在分布式系统中处理各种故障和异常情况。协议中包含参与者和协调者之间的消息交换和确认机制，可以对网络故障、节点故障等进行容错处理，保证系统的可靠性。

  缺点：

  1. 阻塞问题：在两段提交协议中，如果协调者或参与者发生故障，可能会导致整个系统阻塞。因为在提交阶段，所有参与者都需要等待协调者的指令，如果协调者无法响应，参与者无法确定下一步的操作，从而可能导致无法继续进行事务处理。
  2. 单点故障：两段提交协议下的协调者是一个中心化的实体，如果协调者发生故障，可能会导致整个系统无法正常工作。为了避免单点故障，需要采取冗余化的措施，例如使用备份协调者，但引入备份协调者又增加了系统的复杂性。
  3. 不适合高并发场景：由于两段提交协议在提交阶段需要等待所有参与者的响应，在高并发的情况下，可能会导致大量的等待和通信开销，影响系统的性能和吞吐量。
  4. 数据不一致的风险：尽管两段提交协议可以保证事务在分布式系统中的一致性，但在异常情况下，仍然存在数据不一致的风险。例如，如果在提交阶段发生故障，已经提交的参与者无法回滚，可能会导致部分数据的不一致。

- A事务提交了，B发生了异常进行回滚。但是由于A已经提交了，无法进行回滚怎么办

  - 可以考虑以下几种解决方案：

    1. 补偿操作（Compensating Action）：针对B事务的回滚需求，可以定义相应的补偿操作，用于撤销或修正A事务已经提交的操作。通过执行补偿操作，尽量将系统恢复到原始状态或满足一致性要求。

    2. 版本控制（Versioning）：在系统设计时引入版本控制机制，通过记录每个事务的版本号或状态，可以在后续发生异常需要回滚的情况下，根据事务的版本信息执行相应的回滚操作。这样可以避免无法回滚已经提交的事务的问题。 （备忘录模式）

    3. 可靠消息队列（Reliable Message Queue）：使用可靠消息队列来处理事务操作，将A和B的操作封装成消息，按顺序发布到消息队列中。当发生异常需要回滚时，可以消费消息队列中的消息并执行相应的回滚操作。这样可以保证A事务和B事务的执行顺序和一致性。

    4. 设计避免数据依赖性（Design for No Data Dependency）：在系统设计时，可以尽量避免设计上的数据依赖性，使得每个事务的执行结果相对独立。这样一旦发生回滚需求，可以通过撤销或修正对应的操作，而不需要回滚已经提交的事务。

       这个好

# ==spring==

## bean的初始化

- 过程
  - 实例化Bean对象：当Spring容器启动时，它会根据配置文件或注解来确定需要创建的Bean。Spring根据Bean的作用域（Singleton、Prototype等）和创建方式（构造函数、工厂方法等）来实例化Bean对象。实例化后的Bean对象处于未初始化状态。
  - 设置Bean的属性：
    - 在创建Bean实例后，Spring通过依赖注入（Dependency Injection）来设置Bean的属性。依赖注入可以通过XML配置、注解或Java代码来完成。Spring根据配置中的依赖关系，将对应属性的值注入到Bean中。
    - 如果Bean实现了InitializingBean接口，Spring会在属性注入完成后调用其afterPropertiesSet()方法，开发者可以在该方法中进行其他的属性验证或初始化逻辑。
  - 实现Aware接口
    - Spring在属性填充后，会检查Bean是否实现了一些特定的Aware接口，并回调相应的方法。
    - 例如，对于Bean实现了ApplicationContextAware接口，Spring将调用setApplicationContext()方法，从而允许Bean获取对应用程序上下文的引用。
  - Bean的后置处理（Post Processors）：在Bean的实例化和属性设置完成后，Spring提供了一些回调接口（如BeanPostProcessor和BeanFactoryPostProcessor）来对Bean进行进一步的处理。开发者可以自定义实现这些接口，通过覆盖对应的方法来对Bean做自定义的操作，如修改属性值、扩展Bean等。这些后置处理器可以在Bean的生命周期的不同阶段进行操作。
  - 初始化方法调用：如果在Bean的配置中指定了初始化方法，Spring会在Bean的属性设置完成后调用该方法。可以通过XML配置（init-method属性）或注解（@PostConstruct注解）来指定初始化方法的名称。该方法可以包含自定义的初始化逻辑，例如数据加载、资源初始化等。
  - 返回完全初始化的Bean：在初始化方法调用完毕后，Bean被认为是完全初始化的状态，可以供其他Bean或应用程序使用。
- Spring框架管理Bean的生命周期，它负责创建、初始化和销毁Bean对象。开发人员可以通过配置和接口实现来自定义Bean的初始化过程，以适应具体的业务需求。

## bean的生命周期

- 在bean的初始化过程后再添加两个
  - Bean的使用（In Use）：
    - 在Bean被完全初始化后，可以在应用程序中使用该Bean。
    - Bean可以被注入到其他Bean中，或者直接通过Spring容器获取并使用。
  - Bean的销毁（Destruction）：
    - 当Spring容器关闭或者不再需要Bean时，Bean会被销毁。
    - 如果Bean实现了DisposableBean接口，Spring会在容器关闭前调用其destroy()方法。
    - 开发人员也可以通过配置`destroy-method`属性（XML配置）或`@PreDestroy`注解（注解配置）来指定自定义的销毁方法。

## Aware接口

- 特定的Aware接口在Spring中用于提供一种机制，允许Bean获取对特定资源或环境的引用。通过实现这些接口，Bean可以与Spring容器和其他相关资源进行交互，实现更高级别的功能和定制。下面是几个常见的Aware接口及其用途：
  - ApplicationContextAware：
    - 实现该接口可以获取对ApplicationContext的引用。
    - 可以使用ApplicationContext提供的功能，如获取其他Bean、发布事件、获取国际化文本等。
  - BeanFactoryAware：
    - 实现该接口可以获取对BeanFactory的引用。
    - 可以使用BeanFactory实例来请求其他Bean的获取、配置元数据获取等。
  - EnvironmentAware：
    - 实现该接口可以获取对Environment的引用。
    - 可以使用Environment来获取配置属性、配置文件信息等。
  - ResourceLoaderAware：
    - 实现该接口可以获取对ResourceLoader的引用。
    - 可以使用ResourceLoader加载外部资源，如文件、URL等。
  - ServletContextAware：
    - 当在Web应用程序中使用Spring时，实现该接口可以获取对ServletContext的引用。
    - 可以使用ServletContext访问Web应用程序的上下文信息，如请求参数、属性等。

## aop切面位置

- 位置
  - 类方法（Method）级别：可以在切面中定义切点表达式，选择特定类的某个或某些方法进行代理增强。在切面中定义增强逻辑，如前置通知（@Before）、后置通知（@After）、返回通知（@AfterReturning）、异常通知（@AfterThrowing）等，实现对目标方法的增强操作。
  - 类（Class）级别：通过在切面中定义切点表达式，选择特定的类进行代理增强。可以通过@Before、@After、@Around等注解在切面中添加相应的增强逻辑。类级别的代理增强会在该类的所有方法执行时生效。
  - 接口（Interface）级别：可以通过在切面中定义切点表达式，选择实现了特定接口的类进行代理增强。在切面中添加增强逻辑，对实现了该接口的类的方法进行增强操作。
  - 包（Package）级别：通过在切面中定义切点表达式，选择特定的包及其子包下的所有类进行代理增强。在切面中添加增强逻辑，对该包下的所有类的方法进行增强操作。
- 注意
  - 我们可以使用注解方式定义切面和切点表达式，也可以使用XML配置文件来定义切面和切点。在切面中，可以定义增强逻辑，如日志记录、事务处理、权限校验等等，来实现对目标方法的特定操作。
  - Spring AOP是基于代理的动态代理方式，可以通过JDK动态代理或CGLIB动态代理实现。代理增强只作用于通过Spring容器管理的Bean，因此非Spring容器管理的对象无法进行代理增强

## @RequestParam和@RequestBody区别

- @RequestParam
  - 用于从请求的URL中获取参数值。它可以指定参数的名称、是否必需以及默认值。当使用@RequestParam注解时，参数值将从请求的查询字符串或表单数据中提取。
- @RequestBody
  - 用于从请求的主体中获取参数值。它适用于POST、PUT等请求方法，其中参数值以JSON、XML等格式发送到服务器。在使用@RequestBody注解时，Spring会尝试将请求主体中的数据反序列化为指定的参数类型。

# ==tomcat==

## tomcat启动

- 从操作系统的角度来看，Tomcat启动涉及到资源分配、库加载、JVM启动、传递启动参数、加载启动脚本、初始化服务器、监听端口等一系列操作
  - 分配资源：
    - 操作系统会为Tomcat进程分配内存，用于存储Tomcat的代码、数据和堆空间。
    - 操作系统还会为Tomcat进程分配文件描述符（File Descriptors），用于处理文件和网络I/O操作。
    - 操作系统会为Tomcat进程分配线程资源，用于处理并发请求。
  - 加载共享库和依赖项：
    - Tomcat可能依赖于一些共享库（SO文件）和其他依赖项，如数据库驱动程序等。
    - 操作系统负责加载这些共享库和依赖项，以便Tomcat能够正确链接和使用它们。
  - 启动Java虚拟机（JVM）：
    - 操作系统会为Tomcat启动一个Java虚拟机，即JVM。JVM是运行Java应用程序的虚拟机。
    - 操作系统会为JVM分配内存（堆空间），并加载JVM所需的类库和运行时环境。
  - 指定Tomcat的启动参数：
    - 操作系统会将一些启动参数传递给Tomcat，例如JVM的参数。这些参数在启动脚本中指定，如堆大小、GC策略等。
    - 操作系统还会传递Tomcat本身的配置参数，例如端口号、存储路径、日志级别等。
  - 加载并执行Tomcat的启动脚本：
    - 操作系统会加载Tomcat的启动脚本，如`catalina.sh`（Linux/Unix）或`catalina.bat`（Windows）。
    - 启动脚本负责设置环境变量、定义类路径、配置系统属性，并调用Java虚拟机来启动Tomcat。
  - 初始化Tomcat服务器：
    - 当Tomcat启动后，它会执行一系列初始化操作。
    - 这包括加载和解析配置文件，如`server.xml`和`web.xml`。这些配置文件定义Tomcat服务器和Web应用程序的各种设置和参数。
    - Tomcat还会创建线程池、初始化连接池、加载类库等。
  - 监听端口：
    - Tomcat服务器启动后，会开始监听指定的端口，如HTTP端口号（默认为80）和HTTPS端口号（默认为443）。
    - 操作系统负责确保这些端口可用，并将传入的请求转发给Tomcat服务器进行处理。
  - 处理请求：
    - 一旦Tomcat服务器启动并监听端口，它将开始处理传入的请求。
    - 这包括接收和解析HTTP请求，根据请求路径调用相应的Servlet、JSP或处理程序来处理请求。
    - Tomcat会生成响应，并将其返回给客户端，以完成请求-响应的交互。
  - 运行Tomcat服务器：
    - Tomcat服务器将在操作系统上持续运行，处理传入的请求，并提供Web应用程序的服务。
    - 它会在后台维护线程池、连接池和其他资源，以及处理并发请求、管理会话等。
- 这些步骤完成后，Tomcat将成为一个基于Java的Web服务器，可以接收和处理来自客户端的请求，并提供相应的Web服务。

# ==设计模式==

## 单例模式的好处

- 2
  - 全局访问：单例模式可以保证一个类只有一个实例存在，这样可以方便其他对象在任何时候访问该实例，而不需要创建多个实例。
  - 资源共享：由于单例模式只有一个实例，可以避免多个实例同时访问某些共享资源出现竞争的问题，确保资源的安全访问。
  - 避免重复创建：对于频繁使用的对象，使用单例模式可以减少对象创建的开销，提高性能。
  - 控制实例数量：通过单例模式，可以控制一个类的实例数量，避免系统中出现过多的相似对象，节省系统资源。
  - 管理状态和操作：单例模式可以提供一个中心化的位置来管理状态和操作，方便进行协调和控制。

# ==其他==

## 平衡二叉树和完全二叉树

- 区别
  - 定义和特性：平衡二叉树是一种满足平衡条件的二叉树。它的定义是指对于任意节点，它的左子树和右子树的高度差不超过1。而完全二叉树是一种二叉树，除了最后一层的叶子节点可以不满外，其他层的节点都要达到最大数量，并且叶子节点都集中在左侧。
  - 高度差限制：平衡二叉树要求左子树和右子树的高度差不超过1，因此在平衡二叉树中，任意节点的左右子树高度差都要小于等于1。而完全二叉树并没有要求左右子树的高度差具有特定的限制，只要满足层级结构即可。
  - 节点的插入和删除操作：平衡二叉树在执行节点的插入和删除操作时，需要通过旋转等调整来保持树的平衡。这样的调整可能会导致树结构的改变。而完全二叉树的节点插入和删除操作相对简单，只需保持树的完全性，即保持叶子节点集中在左侧。
  - 存储空间：由于平衡二叉树要维持平衡条件，可能需要进行结构调整，并且会存储节点的平衡因子等额外信息。因此，平衡二叉树的存储空间通常比完全二叉树要多。完全二叉树则只需要存储节点值和指向子节点的指针即可。
- 完全二叉树是一种特殊的平衡二叉树

## 云原生

- 云原生（Cloud Native）是一种设计和构建应用程序的方法论，旨在充分利用云计算平台的优势，实现高效、弹性和可扩展的应用部署和管理。
- 云原生的设计理念和原则如下：
  - 微服务架构：云原生应用程序采用微服务架构，将应用程序拆分为一组更小、更独立的服务。每个微服务都专注于一个具体的业务功能，可以独立开发、部署和扩展。微服务之间通常使用轻量级的通信机制（如 REST API）进行交互，并且可以使用不同的技术栈和编程语言来实现。
  - 容器化：云原生应用程序通常使用容器技术进行部署和管理。容器提供了一种隔离、轻量级和可移植的运行环境，使得应用程序可以在不同的主机和云环境中无缝运行。常用的容器平台是 Docker，它可以让开发人员将应用程序及其所有依赖项（如库、运行时环境）打包到一个独立的容器镜像中，并在任何支持 Docker 的环境中部署和运行。
  - 自动化运维和管理：云原生应用程序倡导使用自动化工具和流程来管理应用程序的生命周期。这包括自动化的构建、测试、部署和监控，以及自动化的弹性伸缩和故障恢复。通过自动化，开发人员和运维团队可以更快速、可靠地交付新的功能和修复问题，同时提高应用程序的稳定性和可靠性。
  - 声明式 API：云原生应用程序使用声明式 API 来描述应用程序的期望状态，而不是直接指定执行步骤。这样可以帮助运行平台自动化地管理应用程序的状态，减少运维的工作量。例如，使用 Kubernetes 这样的容器编排平台，可以使用 YAML 文件声明应用程序的部署、服务发现、负载均衡等配置，Kubernetes 会负责根据这些声明来自动管理应用程序的运行状态。
  - 持续交付和持续集成：云原生应用程序鼓励使用持续交付和持续集成的实践，通过自动化的构建、测试和部署流程来频繁地交付新的版本。这有助于加快应用程序的迭代速度和响应能力，同时确保每个版本的质量和稳定性。持续交付和持续集成通常依赖于自动化的构建工具、代码仓库和测试框架。
- 总的来说，云原生是一种面向云计算环境设计和构建应用程序的方法论。它通过微服务架构、容器化、自动化运维和持续交付等实践，使得应用程序更加灵活、可扩展和可靠。云原生的设计目标是快速响应需求变化、自动适应大规模负载，并提供高效的开发和运维体验。同时，云原生也倡导使用云计算平台提供的服务，如弹性伸缩、负载均衡和监控，来进一步优化应用程序的性能和可用性。

## DDD和mvc对比的优劣势

- 领域模型驱动（Domain-Driven Design）和 MVC（Model-View-Controller）是软件开发中常用的两种方法。下面是它们的一些优劣势对比：

  优势：

  领域模型驱动（DDD）：

  1. 专注于业务领域：DDD注重建立一个准确和完整的领域模型，使开发人员更加关注解决业务问题。
  2. 灵活性和可维护性：DDD强调模型的灵活性和可维护性，使得在业务需求变化时更容易进行修改和调整。
  3. 沟通和理解：通过领域模型作为通用语言，开发团队和业务专家之间的沟通和理解更加容易。

  MVC：

  1. 分离关注点：MVC将应用程序分为模型、视图和控制器，有助于分离不同的关注点，并提高代码的可维护性和可测试性。
  2. 可扩展性：MVC将应用程序分层，有助于增加新的功能或更改现有功能而不影响其他部分。
  3. 可重用性：通过模型和控制器的分离，可以更容易地重用这些组件，提高开发效率。

  劣势：

  领域模型驱动（DDD）：

  1. 学习成本高：DDD需要开发人员具备深入的业务领域知识，并掌握建模技巧，这可能需要一定的学习成本。
  2. 可能增加复杂性：一些复杂的业务领域可能需要复杂的领域模型，这可能会增加系统的复杂性。

  MVC：

  1. 需要额外的设计：MVC需要进行模型、视图和控制器之间的合理设计和交互，这可能增加了设计的复杂性。
  2. 可能导致代码分散：如果控制器逻辑变得复杂，可能会导致控制器中的代码逻辑分散和难以维护。

  需要注意的是，领域模型驱动和MVC并不是相互排斥的方法，它们可以结合使用。可以在MVC架构中采用领域模型驱动的思想，将领域模型作为关键的部分，以实现更好的业务模型和系统设计。

## 排查sql执行慢

- 当一条 SQL 执行变慢时，可以按照以下步骤进行排查：
  1. 检查索引：确保相关表上的索引是否正确创建。可以使用 `EXPLAIN` 或 `SHOW INDEX` 命令来查看查询计划和索引信息。
  2. 检查统计信息：确保统计信息是最新的。可以通过运行 `ANALYZE TABLE` 命令来更新统计信息。
  3. 检查执行计划：使用 `EXPLAIN` 命令分析查询的执行计划，查看是否存在全表扫描、排序或连接操作等耗时的操作步骤。
  4. 监控系统资源：检查数据库服务器的 CPU 使用率、内存占用和磁盘 I/O 等系统资源，看是否存在资源瓶颈。
  5. 检查锁和阻塞：查看是否存在其他会话持有锁或导致阻塞的情况，可以通过 `SHOW PROCESSLIST` 或 `INFORMATION_SCHEMA.INNODB_LOCKS` 来查询锁和阻塞信息。
  6. 优化查询语句：如果前面的步骤没有找到问题，可以考虑对慢查询进行优化。尝试使用更合适的索引、重写查询，或者调整查询参数等方式来提升性能。
  7. 监控和日志：当问题仍然存在时，可以通过监控工具和数据库日志来进一步分析慢查询的原因，如执行时间、响应时间、磁盘访问等方面。

## sql为什么变慢

- 一开始是快的

- 当一开始执行较快但后来变慢时，可能存在以下原因：

  1. 数据量增加：如果数据量逐渐增长，原本快速执行的查询可能因为数据量过大而变得慢。在这种情况下，可以考虑优化查询、增加索引或进行分区等操作来提升性能。
  2. 索引失效：可能由于数据的变动或者查询模式的改变，原本有效的索引可能变得无效或不适用。检查查询是否仍能充分利用已有的索引，如果不是，可以尝试调整索引或添加新的索引。
  3. 查询优化器决策：数据库查询优化器可能在查询执行计划优化过程中做出了不理想的决策，导致性能下降。可以尝试使用 `FORCE INDEX` 或者调整数据库的优化器相关参数来影响查询执行计划。
  4. 数据库参数调整：数据库配置的一些参数可能影响查询性能。检查数据库配置参数，例如缓冲区大小、连接池设置等，根据实际情况进行调整。
  5. 硬件资源瓶颈：随着时间的推移，硬件设备的性能可能下降或资源使用不均衡，例如 CPU 或磁盘 I/O。检查系统资源的利用率和性能，以确定是否存在硬件瓶颈。
  6. 锁和并发问题：可能有其他并发查询导致锁冲突或者阻塞，在查询执行时产生延迟。检查是否有长时间运行的事务或者其他会话引起的阻塞。

  解决问题的方法可以参考前面提到的排查步骤，根据具体情况逐步排查，找到导致慢查询的原因并进行相应的优化或调整

## 调用链路上的问题怎么排查

- 当调用链路中的某个环节无法正常通信时，可以按照以下步骤来排查和解决问题：

  1. 确定问题的范围：首先确定问题是出现在哪个环节，是 A 调用 B 时出现问题，还是 B 调用其他组件时出现问题。
  2. 检查网络连接：确认 A 和 B 之间的网络连接是否正常，可以通过 ping 命令、telnet 命令等来测试网络连接是否可达。
  3. 检查配置信息：确保 A 和 B 的配置信息是否正确，例如 IP 地址、端口号、认证信息等。确保 A 和 B 之间的通信配置是一致的。
  4. 日志和错误信息：查看 A 和 B 的日志文件或错误信息，尽可能收集更多的信息来定位问题。日志中可能会显示网络连接错误、超时等相关的错误信息。
  5. 确认服务状态：检查 B 服务是否正常运行，可以通过访问 B 的健康检查接口或者查看进程状态来确认服务的可用性。
  6. 并发问题和负载：如果 B 服务正常运行，但在高并发或负载较大的情况下无法响应，可能是因为负载过载或资源不足。可以通过监控系统资源、性能测试等方式来检查并发问题和负载情况。
  7. 防火墙和安全组规则：检查防火墙和安全组规则，确保网络配置允许 A 和 B 之间的通信。需要确保相关的网络端口被正确打开，并且没有被防火墙或安全组规则限制。
  8. 依赖组件和版本问题：检查 A 和 B 依赖的其他组件或服务是否正常，并且版本兼容。有时候由于组件或服务的版本不兼容，会导致通信异常。
  9. 协议和格式问题：确保 A 和 B 之间的通信协议和数据格式是一致的。如果 A 和 B 之间使用了不同的协议或数据格式，可能会导致通信中断或解析错误。
  10. 重启和重启顺序：尝试重启 A 和 B，以确保应用程序和服务处于正常状态。有时候，服务或组件可能出现异常，重启可以清除可能的问题。
  11. 监控和追踪工具：使用监控和追踪工具来捕捉调用链路中的请求和响应，以及潜在的错误信息。这些工具可以提供可视化的图形界面来帮助分析和排查问题。
  12. 调试和日志级别：增加调试和日志级别，以获取更详细的日志信息。这些信息可以用于进一步分析和定位问题。
  13. 代码审查和单元测试：仔细检查 A 和 B 的代码，特别是调用接口和网络通信部分的代码。使用单元测试来验证调用链路中各个模块的功能和正确性。
  14. 与供应商或开发团队联系：如果以上步骤仍未解决问题，可以与相关的供应商或开发团队联系，向他们提供详细的问题描述和日志信息，以获得进一步的支持和解决方案。

  总之，在排查调用链路中的问题时，需要有系统性地进行逐步排查和分析。最好从基础的网络连接、配置和日志开始，逐渐深入排查，同时利用工具和资源来帮助定位和解决问题。

## token容易被捕获，怎么解决安全性问题

- 确保令牌（Token）的安全性对于应用程序和系统是非常重要的。以下是一些常见的方法来提高令牌的安全性和减少被捕获的风险：
  1. 使用安全传输协议：确保令牌在传输过程中使用安全的加密通道，如 HTTPS，以防止被中间人攻击或窃听。使用加密的传输协议可以保护令牌的机密性和完整性。
  2. 限制令牌的寿命：设置令牌的有效期限，使其在一定时间内有效，之后需要重新获取新的令牌。短期令牌可以减少被捕获后被滥用的风险。
  3. 令牌刷新和续期：使用令牌刷新机制，让应用程序可以周期性地刷新或续期令牌，而无需重新进行完整的用户身份验证。这可以减少令牌在传输和存储过程中的风险。
  4. 使用安全令牌存储：令牌在客户端或服务器端的存储需要采取适当的安全性措施。在客户端，可以使用安全的本地存储机制，如使用安全的密钥串、操作系统提供的安全存储等。在服务器端，对令牌的存储和管理也需要使用安全的方式，如加密存储、访问控制等。
  5. 令牌权限和范围控制：为每个令牌分配适当的权限和范围，以确保令牌只能访问其所需的资源和数据。不要将过多或不必要的权限分配给令牌，以减少被滥用的机会。
  6. 使用多因素身份验证：结合令牌验证和其他身份验证因素（如密码、生物识别等）来增强安全性。多因素身份验证可以提供额外的层级保护，即使令牌被捕获，攻击者也难以获得其他身份验证因素。
  7. 定期审查和更新令牌机制：定期审查和更新令牌机制，确保使用最新的安全标准

## 去重sql

- 分组、编号、排序

  ```sql
  SELECT *
  FROM (
    SELECT *, 
    ROW_NUMBER() OVER (PARTITION BY redis_key ORDER BY timestamp DESC) as row_num
    FROM your_table
  ) AS subquery
  WHERE row_num = 1;
  ```

  

## String比较

- 3

  ```java
          String s1="abc";
          String s2="abc";
          String s3=new String("abc");
          String s4=new String("abc");
  
          System.out.println(s1==s2);// true
          System.out.println(s1==s3);// false
          System.out.println(s3==s4);// false
  ```

## Integer比较

- 3

  ```java
          Integer n1=123;
          Integer n2=123;
          Integer n3=new Integer(123);
          Integer n4=new Integer(123);
  
          System.out.println(n1==n2); // true
          System.out.println(n1==n3); // false
          System.out.println(n3==n4); // false
  
          Integer s1=321;
          Integer s2=321;
          Integer s3=new Integer(321);
          Integer s4=new Integer(321);
  
          System.out.println(s1==s2); // false
          System.out.println(s1==s3); // false
          System.out.println(s3==s4); // false
  ```

## 实现线程的方式

Runnable,Callable

Thread,FutureTask

线程池

## 对象什么时候回收

GC遇到以下情况

没有被引用

只被软引用 引用且内存不足

弱引用引用

虚引用：跟踪回收顺序等信息